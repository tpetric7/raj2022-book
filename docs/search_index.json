[["index.html", "Computergestützte Textanalyse mit R Kapitel 1 Einführung", " Computergestützte Textanalyse mit R Teodor Petri 2022-03-20 Kapitel 1 Einführung In diesem Einführungskurs machen wir Sie mit quantitativen Methoden zur Erfassung von linguistischen Merkmalen in deutschen, englischen und slowenischen Texten bekannt. Unseren Kurs beginnen wir mit wichtigen Grundlagen für die Verwendung der Benutzeroberfläche RStudio und die Manipulation von Dateien und Datensätzen. Neben einigen Base-R-Funktionen lernen wir vor allem, mit den Funktionen des Programmbündels tidyverse umzugehen. In mehreren Kurseinheiten beschäftigen wir uns mit kleineren Datensätzen aus dem Bereich Sprachverwendung und einigen grundlegenden statistischen Testmethoden. In einer Kurseinheit vergleichen wir Ermittlungsfunktionen in R und Microsoft Excel, einem beliebten Tabellenprogramm. Das Thema mehrerer Kurseinheiten ist die Zerlegung von Texten in kleinere Einheiten und ihre Bearbeitung auf verschiedenen linguistischen Ebenen: auf phonetischer, graphematischer, morphologischer und syntaktischer Ebene.1 "],["installation.html", "Kapitel 2 Installation 2.1 Grundlegende Programme 2.2 Zusätzliche Programme 2.3 Hilfe 2.4 Andere Werkzeuge 2.5 Cloud-Dienste", " Kapitel 2 Installation 2.1 Grundlegende Programme Für die computergestützte Arbeit mit Sprachmaterial verwenden wir freie Softwaretools, d.h. die Programmiersprache R, die grafische Oberfläche RStudio für komfortableres Programmieren und RTools für die Installation von Entwicklungsversionen in einer Windows-Umgebung (letztere ist optional, aber für fortgeschrittene Benutzer empfohlen). Die Programme R (4.1.2 oder höher), RStudio (2022.02.0+443 oder höher) und RTools (4.0.0 oder höher) können von den folgenden Webadressen heruntergeladen werden (wir führen nur die Installationspakete für die Operationssysteme Windows und MacOS an, für Linux bitte selber nachschlagen): für Windows https://cloud.r-project.org/bin/windows/base/, für MacOS https://cloud.r-project.org/bin/macosx/ https://www.rstudio.com/products/rstudio/download/ (Windows oder MacOS) für fortgeschrittene Benutzer, die auf die Entwicklungsversionen der Programme auf github und einige andere Dienste zugreifen möchten https://cran.r-project.org/bin/windows/Rtools/. Auf neueren Computern laufen meist 64-Bit-Betriebssysteme (Windows, macOS, Linux). Es ist daher ratsam, 64-Bit-Versionen der R-Programme (package, library) zu wählen. Auf einem 32-Bit-Betriebssystem (wenn Sie noch einen älteren Computer verwenden) benötigen Sie 32-Bit-Versionen der Software. 2.2 Zusätzliche Programme Nach der Installation von R und RStudio sind viele Funktionen bereits verfügbar. Zusätzliche Softwarefunktionen können über die CRAN-Server bezogen werden. Alle diese Angebote sind kostenlos. Wenn Sie Programmfunktionen oder Bibliotheken (library) hinzufügen wollen, installieren Sie diese zuerst von den CRAN Programmiersprachenservern. In der Programmierumgebung RStudio kann dies auf mehrere Arten erreicht werden: Für Anfänger ist es am einfachsten, in RStudio auf die Registerkarte (Reiter) Packages (Pakete) zu klicken, dann auf Installieren, und dann den Namen des gewünschten Programms oder Pakets in das neue Dialogfeld einzugeben. Beachten Sie, dass in der Programmiersprache R zwischen Groß- und Kleinschreibung unterschieden wird (im Gegensatz zu Excel). Ein schnellerer Weg, neue Programmfunktionen zu installieren, ist die Eingabe des Befehls install.packages(\"package-name\") in der Console (Konsole). Der Paketname muss in Anführungszeichen angegeben werden. Programmfunktionen, die nicht Teil des Basis-Softwarepakets (Base R) sind, müssen vom Benutzer in den Speicher des Computers geladen werden. Dies geschieht, indem man library(Paketname) (Anführungszeichen werden hier nicht benötigt) in das Programmskript oder in die Konsole einträgt. Im Programmskript speichert der Benutzer die Programmbefehle für seine Datenanalyse, insbesondere solche, die wiederverwendet und/oder automatisch ausgeführt werden sollen. Programmskripte kann der Benutzer entweder als R-Skript (Standard-Dateierweiterung: R) oder als Rmarkdown-Dokument (Standard-Dateierweiterung: Rmd) speichern. Man kann Programme bzw. Bibliotheken (library) ausführen, indem man sie lediglich in der Konsole eingibt, aber dies wird nicht im Programmskript gespeichert. Für die computergestützte Arbeit mit Sprachmaterial und die statistische Analyse werden wir in den folgenden Kapiteln die folgenden zusätzlichen Bibliotheken (Pakete, Programme, Sammlungen von Softwarefunktionen) verwenden (für weitere spezielle Analysen und Darstellungen aber noch eine ganze Reihe weiterer Programme): Für Anfänger sind sicher auch Software-Add-ons interessant, die die Auswahl von Befehlen mit der Maus und mehrere vordefinierte Arbeitsabläufe ermöglichen, wie z. B. die folgenden Add-ons für statistische Analysen: - RCommander: library(Rcmdr) - Rattle: library(Rattle). Solche Add-Ons erfordern nicht, dass man sich schon in den ersten Wochen oder Monaten die Namen von Bibliotheken oder Befehlen merken muss, die für die Analyse in der Programmiersprache R benötigt werden. RCommander druckt auch die Reihenfolge der Befehle für Sie aus, so dass es einfacher ist zu lernen, wie man ein Programmskript oder ein Rmarkdown-Dokument erstellt. 2.3 Hilfe In RStudio ist auch eine sorgfältig ausgearbeitete Hilfe eingebaut, die auf verschiedene Weise erreicht werden kann: Wählen Sie die Registerkarte Help (Hilfe) und geben Sie in das Feld Search (Suchen) einen Suchbegriff ein (z.B. den Namen des Befehls oder der Funktion, über die Sie Einzelheiten erfahren möchten). Klicken Sie sich dann zur gewünschten Hilfeseite durch. Wenn Sie bereits eine Bibliothek (Paket) in den Speicher Ihres Computers geladen haben, ist es aber meist bequemer, (a) die Maus über eine Programmfunktion im Skript zu halten, um einen Hinweis zu erhalten oder (b) mit der Maus den Befehlsnamen anzuklicken und die Funktionstaste F1 (in Windows) für kontextsensitive Hilfe zu drücken. Auf diese Weise landen wir meist auf der entsprechenden Hilfeseite. 2.4 Andere Werkzeuge Zusätzlich zu Tabellenkalkulations- und Diagrammsoftware (z.B. Microsoft Excel oder kostenfreie Programmbündel wie OpenOffice oder LibreOffice) kann ein freies statistisches Analyseprogramm nützlich sein, insbesonderer wenn man noch nicht in der Lage ist, ein analytisches Verfahren in der Programmiersprache R durchzuführen: z.B. Jamovi (https://www.jamovi.org/download.html), das unter der Haube mit der Programmiersprache R arbeitet und ansprechende statistische Berichte und tabellarische oder graphische Darstellungen erstellen kann; Jasp (https://jasp-stats.org/download/), das wie Jamovi mehrere Assistenten für die Erstellung von statistischen Berichten und Darstellungen bietet, aber in Bezug auf die durchführbaren Programmierfunktionen hinter dem ersten zurückbleibt. Für eine schnelle und unkomplizierte Analyse eines Textes (oder einer kleinen Anzahl von Texten) ist das webbasierte Tool Voyant Tools (https://voyant-tools.org/) sehr praktisch. Laden Sie den Text von Ihrem Computer (oder kopieren Sie ihn in ein Dialogfeld), klicken Sie auf Anzeigen und nach einigen Sekunden erzeugt das Programm eine ganze Reihe von Diagrammen und Tabellen zu den Eigenschaften der Wortformen im Text. Wenn Sie mit einer großen Anzahl von Texten zu tun haben, die sogar separat analysiert werden sollen, wird es unbequem, mit diesem Werkzeug zu arbeiten, und es ist einfacher, in der RStudio-Umgebung mit der Programmiersprache R zu arbeiten (oder mit anderen Programmiersprachen, z.B. mit dem ebenfalls beliebten Python). Ein weiterer Vorteil der Programmiersprache R und Rmarkdown-Dokumenten ist die Möglichkeit, einen Bericht, Artikel, Blog oder ein Buch zu erstellen, in dem sowohl der Datensatz, das komplette Analyseverfahren, die Ergebnisse der Analyse in tabellarischer und graphischer Form als auch der begleitende Text enthalten ist. Ein Beispiel dafür ist dieses Buch, das Sie im Augenblick verwenden. 2.5 Cloud-Dienste Für fortgeschrittene Benutzer ist das Portal github (https://github.com/) vorläufig noch kostenlos (obwohl es von Microsoft aufgekauft wurde). Es ermöglicht die Speicherung von Analyseverfahren, Softwarebibliotheken, die Veröffentlichung von Arbeitsmaterial und die Zusammenarbeit zwischen Programmierern und Benutzern. Das Buch, das Sie gerade lesen, hat auch ein Zuhause auf github. Es ist erwähnenswert, dass die Programmiersprache R und die Programmierumgebung RStudio auch in Cloud-Diensten verwendet werden können, ohne dass R und RStudio auf dem eigenen Computer installiert werden müssen, z. B. RStudio cloud: https://rstudio.cloud/ Google Collaboratory (https://colab.research.google.com) Kaggle (https://www.kaggle.com/). Für diese Dienste müssen Sie ein Konto anlegen und sich beim entsprechenden Webportal anmelden. Die Nutzung dieser Dienste ist kostenlos. Der Vorteil ist, dass Ihre Analyse auf einem leistungsstarken Cloud-Server ausgeführt wird, womit potenzielle Softwarekonflikte und unerwünschte Änderungen am Betriebssystem sowie Belastungen für Ihren eigenen Computer vermieden werden. Der Nachteil der kostenlosen Nutzung von RStudio.cloud ist, dass der kostenfreie Speicherplatz bei großen Textsammlungen schnell erschöpft ist, so dass man nicht weiterarbeiten kann und die bereits erzielten Ergebnisse verloren gehen. Google Colab ist in erster Linie für Programmierer gedacht, die in der Programmiersprache Python arbeiten. Wenn Sie mit der Programmiersprache R arbeiten, sollten Sie damit rechnen, dass bestimmte Programmierfunktionen nicht zur Verfügung stehen (z.B. die readtext-Bibliothek zum unkomplizierten Öffnen einer Textsammlung). "],["showcase.html", "Kapitel 3 Showcase 3.1 ggplot with emojis 3.2 Data PreparationWere using data from Chart2000.com:Music Charts 2000 - 2020 3.3 First Boxplot: Some tweaks to a basic ggplot2 chartusing ggthemes by Jeffrey B. Arnold 3.4 Show Ns Visuallyby specifying varwidth = TRUE 3.5 Show Ns Numerically using EnvStats::stat_n_text() by Steven Millard and Alexander Kowarik 3.6 Show Summary Statistics on Hover (Mouse-Over)using plotly by Carson Sievert 3.7 Label Outliersvia a user-defined function and geom_text() 3.8 Display Individual Data Pointsusing geom_jitter() 3.9 Use Aesthetics to Include Information on Another VariableCombining shape and color, using RColorBrewer by Erich Neuwirth 3.10 Add Meansfor comparison to medians 3.11 Calculate and Display Statistical Tests for Group Differencesusing ggstatsplot by Indrajeet Patil 3.12 Group Differences:Example for a significant test 3.13 Use Images as Labelsvia ggtext by Claus Wilke", " Kapitel 3 Showcase Einige Beispiele, was man mit den Funktionen in der Programmiersprache R machen kann. Graphische Darstellungen von Wolf Riepl (https://github.com/fjodor). https://github.com/fjodor/dataviz_ideas 3.1 ggplot with emojis # https://github.com/dill/emoGG # devtools::install_github(&quot;dill/emoGG&quot;) library(ggplot2) library(emoGG) emoji_search(&quot;tulip&quot;) ## emoji code keyword ## 1929 tulip 1f337 flowers ## 1930 tulip 1f337 plant ## 1931 tulip 1f337 nature ## 1932 tulip 1f337 summer ## 1933 tulip 1f337 spring ## 4595 copyright a9 ip ## 5538 liechtenstein 1f1f1\\\\U0001f1ee li # tulips ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_emoji(emoji=&quot;1f337&quot;) # cars ggplot(mtcars, aes(wt, mpg))+ geom_emoji(emoji=&quot;1f697&quot;) # random posx &lt;- runif(50, 0, 10) posy &lt;- runif(50, 0, 10) ggplot(data.frame(x = posx, y = posy), aes(x, y)) + geom_emoji(emoji=&quot;1f63b&quot;) # big emoji in background qplot(x=Sepal.Length, y=Sepal.Width, data=iris, geom=&quot;point&quot;) + add_emoji(emoji=&quot;1f337&quot;) 3.2 Data PreparationWere using data from Chart2000.com:Music Charts 2000 - 2020 library(knitr) library(kableExtra) library(flexdashboard) library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- ## v tibble 3.1.6 v dplyr 1.0.8 ## v tidyr 1.2.0 v stringr 1.4.0 ## v readr 2.1.2 v forcats 0.5.1 ## v purrr 0.3.4 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::group_rows() masks kableExtra::group_rows() ## x dplyr::lag() masks stats::lag() library(ggthemes) library(DT) library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout library(EnvStats) ## ## Attaching package: &#39;EnvStats&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## predict, predict.lm ## The following object is masked from &#39;package:base&#39;: ## ## print.default library(ggstatsplot) ## You can cite this package as: ## Patil, I. (2021). Visualizations with statistical details: The &#39;ggstatsplot&#39; approach. ## Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167 library(ggtext) knitr::opts_chunk$set(echo = FALSE) all_songs &lt;- read_csv(file = &quot;data/chart2000-songyear-0-3-0062.csv&quot;, na = c(&quot;&quot;, &quot;-&quot;)) ## Rows: 2100 Columns: 11 ## -- Column specification -------------------------------------------------------- ## Delimiter: &quot;,&quot; ## chr (2): artist, song ## dbl (9): year, position, indicativerevenue, us, uk, de, fr, ca, au ## ## i Use `spec()` to retrieve the full column specification for this data. ## i Specify the column types or set `show_col_types = FALSE` to quiet this message. attr(all_songs, &quot;spec&quot;) &lt;- NULL From the site https://chart2000.com/about.htm#fairuse we downloaded the file chart2000-songyear-0-3-0062, which you can also find on my github profile at https://github.com/fjodor/dataviz_ideas. It contains the top 100 songs for each year from 2000 to 2020. library(knitr) library(kableExtra) library(flexdashboard) library(tidyverse) library(ggthemes) library(DT) library(plotly) library(EnvStats) library(ggstatsplot) library(ggtext) knitr::opts_chunk$set(echo = FALSE) all_songs &lt;- read_csv(file = &quot;data/chart2000-songyear-0-3-0062.csv&quot;, na = c(&quot;&quot;, &quot;-&quot;)) attr(all_songs, &quot;spec&quot;) &lt;- NULL First we filter out the most successful artists: The Top 5 in terms of total score, i. e. the sum of Indicative Revenue (IR). According to https://chart2000.com/about.htm, IR is an attempt to measure the complete revenue generated by a song or album over a certain period. It does take inflation and currency conversion into account and can approximately be related to total revenue generated across the whole music chain in thousands of dollars. top_artists &lt;- all_songs %&gt;% group_by(artist) %&gt;% summarise(total_score = sum(indicativerevenue)) %&gt;% arrange(desc(total_score)) %&gt;% head(n = 5) %&gt;% pull(artist) songs &lt;- all_songs %&gt;% filter(artist %in% top_artists) %&gt;% mutate(artist = fct_infreq(artist), indicativerevenue = round(indicativerevenue)) datatable(songs, filter = &quot;top&quot;) 3.3 First Boxplot: Some tweaks to a basic ggplot2 chartusing ggthemes by Jeffrey B. Arnold This is our first attempt at summarizing indicative revenue of the Top 10 artists. What we have already done: Chosen a new theme: ggthemes::theme_solarized(). Thanks to package author Jeffrey B. Arnold and contributors Increased font size Ordered artists by total number of songs in top 100 for each year 2000 - 2020 Rotated x axis labels Removed x axis tickmarks Removed vertical grid lines theme_set(theme_solarized(base_size = 15)) theme_update(axis.text.x = element_text(angle = 90), axis.ticks.x = element_blank(), panel.grid.major.x = element_blank()) ggplot(songs, aes(x = artist, y = indicativerevenue)) + geom_boxplot() + labs(x = &quot;&quot;, y = &quot;Indicative Revenue&quot;, title = &quot;Indicative Revenue by Artist&quot;, subtitle = &quot;Artists sorted by number of songs in Top 100 per year&quot;, caption = &quot;Source: Chart2000.com, Songs of the year, Version 0-3-0062&quot;) 3.4 Show Ns Visuallyby specifying varwidth = TRUE The trick here is simply the varwidth = TRUE argument in the geom_boxplot() call. ggplot(songs, aes(x = artist, y = indicativerevenue)) + geom_boxplot(varwidth = TRUE) + labs(x = &quot;&quot;, y = &quot;Indicative Revenue&quot;, title = &quot;Indicative Revenue by Artist&quot;, subtitle = &quot;Artists sorted by number of songs in Top 100 per year&quot;, caption = &quot;Source: Chart2000.com, Songs of the year, Version 0-3-0062&quot;) 3.5 Show Ns Numerically using EnvStats::stat_n_text() by Steven Millard and Alexander Kowarik We could, of course, calculate Ns and display them using geom_text() or geom_label(), but well use the convencience function stat_n_text() from the EnvStats package by Steven Millard and Alexander Kowarik instead. ggplot(songs, aes(x = artist, y = indicativerevenue)) + geom_boxplot(varwidth = TRUE) + labs(x = &quot;&quot;, y = &quot;Indicative Revenue&quot;, title = &quot;Indicative Revenue by Artist&quot;, subtitle = &quot;Artists sorted by number of songs in Top 100 per year&quot;, caption = &quot;Source: Chart2000.com, Songs of the year, Version 0-3-0062&quot;) + stat_n_text(y.pos = 900) 3.6 Show Summary Statistics on Hover (Mouse-Over)using plotly by Carson Sievert Note that plotly uses a different calculation method, e. g. a different definition of outliers. songs %&gt;% plot_ly(x = ~artist, y = ~indicativerevenue, type = &quot;box&quot;) 3.7 Label Outliersvia a user-defined function and geom_text() A user-defined function (found on Stackoverflow, answer by JasonAizkalns) returns songs of outliers, NA otherwise. This newly calculated variable is passed to the geom_text() function. Introducing color for a clearer group distinction. In this case we suppress the color legend, as the groups should be clear from the x axis labels. is_outlier &lt;- function(x) { return(x &lt; quantile(x, 0.25) - 1.5 * IQR(x) | x &gt; quantile(x, 0.75) + 1.5 * IQR(x)) } songs %&gt;% group_by(artist) %&gt;% mutate(outlier = ifelse(is_outlier(indicativerevenue), song, NA)) %&gt;% ggplot(aes(x = artist, y = indicativerevenue, color = artist)) + geom_boxplot(varwidth = TRUE) + geom_text(aes(label = outlier), na.rm = TRUE, nudge_y = 1500) + labs(x = &quot;&quot;, y = &quot;Indicative Revenue&quot;, title = &quot;Indicative Revenue by Artist&quot;, subtitle = &quot;Artists sorted by number of songs in Top 100 per year&quot;, caption = &quot;Source: Chart2000.com, Songs of the year, Version 0-3-0062&quot;) + scale_color_discrete(guide = NULL) 3.8 Display Individual Data Pointsusing geom_jitter() You can get the same boxplot for different distributions, e. g. a normal distribution (most data points around the mean) vs. a u-shaped distribution (two peaks: one below and one above the mean, with very few data points near the mean). So displaying all data points gives a better sense of the underlying distributions. It combines macro and micro levels. Inspired by Edward Tufte, see his great book Envisioning Information. Note the use of alpha (opacity), width and height in geom_jitter(), which reduces overplotting. ggplot(songs, aes(x = artist, y = indicativerevenue, color = artist)) + geom_boxplot(varwidth = TRUE, outlier.color = NA) + geom_jitter(alpha = 0.6, width = 0.2, height = 0) + labs(x = &quot;&quot;, y = &quot;Indicative Revenue&quot;, title = &quot;Indicative Revenue by Artist&quot;, subtitle = &quot;Artists sorted by number of songs in Top 100 per year&quot;, caption = &quot;Source: Chart2000.com, Songs of the year, Version 0-3-0062&quot;) + scale_color_discrete(guide = NULL) + stat_n_text(y.pos = 900) You could use plotly here again to display information about songs on hover. At the moment, a hack is needed to remove outliers from the boxplot - maybe support for that will improve in a future version of plotly. Plotlys way seems to be to display the data points side-by-side to the boxplot, not overlay it. 3.9 Use Aesthetics to Include Information on Another VariableCombining shape and color, using RColorBrewer by Erich Neuwirth Aesthetics for color and shape defined inside geom_jitter() (otherwise, there would be separate boxplots for the groups) Two aesthetics represented by one legend, as they refer to the same variable and are named the same Using RColorBrewer by Erich Neuwirth songs %&gt;% rowwise() %&gt;% mutate(no1 = any(c_across(us:au) == 1, na.rm = TRUE)) %&gt;% ggplot(aes(x = artist, y = indicativerevenue)) + geom_boxplot(varwidth = TRUE, outlier.color = NA) + geom_jitter(alpha = 0.6, width = 0.2, height = 0, aes(shape = no1, color = no1)) + labs(x = &quot;&quot;, y = &quot;Indicative Revenue&quot;, title = &quot;Indicative Revenue by Artist&quot;, subtitle = &quot;Artists sorted by number of songs in Top 100 per year&quot;, caption = &quot;Source: Chart2000.com, Songs of the year, Version 0-3-0062&quot;) + scale_color_brewer(palette = &quot;Dark2&quot;, name = &quot;No. 1\\n(Any Country)?&quot;) + scale_shape_discrete(name = &quot;No. 1\\n(Any Country)?&quot;) + stat_n_text(y.pos = 900) 3.10 Add Meansfor comparison to medians ## Warning: Removed 5 rows containing missing values (geom_segment). Interesting to see how in some cases (The Black Eyed Peas) median and mean seem to overlap, while in other cases (Ed Sheeran) there is a notable gap between median and mean. As is often the case, the mean is considerably higher, as it is influenced by outliers at the high end of the range of values. Here Id rather not show the individual data points to avoid information overflow. This chart focuses on summary statistics. ggplot(songs, aes(x = artist, y = indicativerevenue, color = artist)) + geom_boxplot(varwidth = TRUE) + # geom_jitter(alpha = 0.6, width = 0.2, height = 0) + stat_summary(fun = &quot;mean&quot;, color = &quot;black&quot;, shape = 8) + labs(x = &quot;&quot;, y = &quot;Indicative Revenue&quot;, title = &quot;Indicative Revenue by Artist&quot;, subtitle = &quot;Artists sorted by number of songs in Top 100 per year&quot;, caption = &quot;* Mean\\n\\nSource: Chart2000.com, Songs of the year, Version 0-3-0062&quot;) + scale_color_discrete(guide = NULL) + stat_n_text(y.pos = 900) 3.11 Calculate and Display Statistical Tests for Group Differencesusing ggstatsplot by Indrajeet Patil The ggstatsplot package by Indrajeet Patil is very powerful and flexible, well integrated with ggplot2 and well documented, see help(package = ggstatsplot). Note that this package adds a large number of dependencies. There are several tests to choose from, including non-parametric, robust and Bayesian. See ?ggbetweenstats() and Website Documentation. ggstatsplot::ggbetweenstats( data = songs, x = artist, xlab = &quot;&quot;, y = indicativerevenue, ylab = &quot;Indicative Revenue&quot;, plot.type = &quot;box&quot;, type = &quot;p&quot;, conf.level = 0.95, title = &quot;Indicative Revenue by Artist&quot; ) 3.12 Group Differences:Example for a significant test While Ed Sheeran reached the highest average indicative revenue, he also recorded a much higher standard deviation. Therefore, only Justin Timberlake averaged significantly higher than Miley Cyrus in this comparison. Note the p value correction for multiple comparisons (corrected Holm method). songs2 &lt;- all_songs %&gt;% filter(artist %in% c(&quot;Ed Sheeran&quot;, &quot;Justin Timberlake&quot;, &quot;Miley Cyrus&quot;)) ggstatsplot::ggbetweenstats( data = songs2, x = artist, xlab = &quot;&quot;, y = indicativerevenue, ylab = &quot;Indicative Revenue&quot;, plot.type = &quot;box&quot;, type = &quot;p&quot;, conf.level = 0.95, title = &quot;Indicative Revenue by Artist&quot; ) 3.13 Use Images as Labelsvia ggtext by Claus Wilke This is made possible by Claus Wilkes excellent ggtext package. Downloaded the files beforehand and stored them in the report / dashboard folder. A named vector of labels is passed to scale_x_discrete(); besides, theme(axis.text.x) needs an element_markdown() function. # Download and rename files labels &lt;- c(Rihanna = &quot;&lt;img src=&#39;pictures/Rihanna.jpg&#39; width = &#39;100&#39; /&gt;&lt;br&gt;*Rihanna*&quot;, Pink = &quot;&lt;img src=&#39;pictures/Pink.jpg&#39; width = &#39;100&#39; /&gt;&lt;br&gt;*Pink*&quot;, &#39;Maroon 5&#39; = &quot;&lt;img src=&#39;pictures/Maroon_5.jpg&#39; width = &#39;100&#39; /&gt;&lt;br&gt;*Maroon 5*&quot;, &#39;The Black Eyed Peas&#39; = &quot;&lt;img src=&#39;pictures/Black_Eyed_Peas.jpeg&#39; width = &#39;100&#39; /&gt;&lt;br&gt;*Black Eyed Peas*&quot;, &#39;Ed Sheeran&#39; = &quot;&lt;img src=&#39;pictures/Ed_Sheeran.jpg&#39; width = &#39;100&#39; /&gt;&lt;br&gt;*Ed Sheeran*&quot; ) ggplot(songs, aes(x = artist, y = indicativerevenue, color = artist)) + geom_boxplot(varwidth = TRUE) + labs(x = &quot;&quot;, y = &quot;Indicative Revenue&quot;, title = &quot;Indicative Revenue by Artist&quot;, subtitle = &quot;Artists sorted by number of songs in Top 100 per year&quot;, caption = &quot;Source: Chart2000.com, Songs of the year, Version 0-3-0062&quot;) + scale_color_discrete(guide = NULL) + scale_x_discrete(name = NULL, labels = labels) + stat_n_text(y.pos = 900) + theme_solarized(base_size = 14) + theme(axis.text.x = element_markdown(color = &quot;black&quot;, angle = 0)) "],["r-rstudio.html", "Kapitel 4 R &amp; RStudio 4.1 Programme 4.2 Textdatei öffnen 4.3 Öffnen mehrerer Texte 4.4 Öffnen von Tabellen 4.5 Öffnen einer Excel-Tabelle 4.6 Datei speichern 4.7 Download von Zip-Dateien 4.8 Verzeichnisse 4.9 Zip-Dateien entpacken und löschen 4.10 Dateilisten anzeigen 4.11 Mehrere Dateien öffnen 4.12 Zeichensatz konvertieren 4.13 Datensatz vorbereiten 4.14 Umwandlung eines R-Skripts", " Kapitel 4 R &amp; RStudio Eine der ersten Fragen nach der Installation von R und RStudio betrifft das Öffnen, Speichern und Bearbeiten von Dateien, die sich auf Ihrer Festplatte befinden oder im Internet zugänglich sind. 4.1 Programme In diesem Abschnitt verwenden wir die folgenden Programme (libraries) und Programmsammlungen: library(tidyverse) # zbirka knjinic za delo s tabelami library(readtext) # branje besedil library(readxl) # branje Excelovih preglednic library(writexl) # pisanje &amp; shranjevanje Excelovih tabel library(rmarkdown) # tu: za lepi izgled tabel library(kableExtra) # za lepi izgled tabel Es gibt viele Möglichkeiten, Dateien zu öffnen und zu speichern. Im Folgenden stelle ich Ihnen einige der häufigsten vor: 4.2 Textdatei öffnen Wir wollen einen Text öffnen und einlesen und ihn anschließend analysieren. Eine Möglichkeit, einen Text in den Arbeitsspeicher zu laden, ist die Kombination der beiden folgenden Befehle oder Funktionen: - read_lines() zum Öffnen und Lesen einer Datei; - file.choose(), um eine Datei mit der Maus auszuwählen. Es öffnet sich ein Dialogfenster und wie in einem Textverarbeitungsprogramm (z.B. Word, Open Office, ) oder einem Tabellenkalkulationsprogramm (Excel, Calc) müssen Sie den Ordner ausfindig machen, der die gewünschte Datei enthält. In diesem Fall ist dies die Textdatei tom.txt im Ordner /data/books). Der Inhalt der Textdatei wird in der (von uns bestimmten) Variablen text gespeichert. Normalerweise wollen wir diesen Prozess automatisieren. Daher tragen wir den Pfad zur Datei (d.h. in welchem Ordner sich die Datei befindet) und den Namen der Datei, an der wir interessiert sind, in ein Programmskript. Benutzer des Betriebssystems Windows sollten sich bei der Pfadangabe daran gewöhnen, den Schrägstrich / anstelle des Schrägstrichs \" zu verwenden (wie das in den Operationssystem Unix oder MacOS üblich ist). Wenn der Text nicht innerhalb eines RStudio-Projekts geöffnet wird, muss der vollständige Pfad zur Zieldatei angegeben werden. Wenn ein Projekt in RStudio erstellt wurde (New Project im Menü File), dann ist es nicht notwendig, den vollständigen Pfad zur Datei anzugeben. Es reicht aus, die Ordner anzugeben, die dem Arbeitsverzeichnis (working directory) untergeordnet sind. Wie kann ich den Pfad zum Arbeitsverzeichnis herausfinden? - Schreiben Sie den Befehl getwd() in die Konsole (Console) oder ins Programmskript. Der Pfad kann kopiert oder als Konstante (oder Variable) gespeichert werden. - Im Dateimanager oder Explorer, der sich standardmäßig auf der rechten Seite von RStudio befindet, können wir mit der Maus auf Mehr (More) klicken und dann die Option Pfad des Arbeitsverzeichnisses in die Zwischenablage kopieren wählen (Copy Folder Path to Clipboard). Im Menü Mehr (More) finden wir auch eine Option zum Ändern des Arbeitsverzeichnisses Als Arbeitsverzeichnis festlegen (Set as Working Directory), was aber in einem RStudio-Projekt normalerweise nicht notwendig ist. Das Setzen des Arbeitsverzeichnisses wird ebenfalls mit der Funktion setwd() durchgeführt. getwd() ## [1] &quot;D:/Users/teodo/Documents/R/raj2022-book&quot; moja_delovna_mapa &lt;- getwd() mein_arbeitsverzeichnis &lt;- getwd() my_working_directory &lt;- getwd() In unserem Beispiel lautet der Pfad zum Arbeitsordner D:/Users/teodo/Documents/R/raj2022-book. Die gewünschte Textdatei tom.txt befindet sich im Unterordner /data/books). Wenn wir vorher einRStudio-project erstellt haben, genügt die Angabe des Unterordners und die Hinzufügung des Zieldateinamens  alles in Anführungszeichen. library(tidyverse) # in RStudio project besedilo = read_lines(&quot;data/books/tom.txt&quot;) Der gesamte Pfad zur Zieldatei kann kopiert und eingefügt werden. Um Schreibfehler zu vermeiden und um sich wiederholtes Schreiben zu ersparen, ist es besser, Programmskripte im Rahmen von RStudio- Projekten anzulegen und dann entweder als R- oder Rmarkdown-Datei zu speichern. library(tidyverse) # full path besedilo &lt;- read_lines(&quot;D:/Users/teodo/Documents/R/raj2022-book/data/books/tom.txt&quot;) Eine weitere Möglichkeit ist das Einfügen von Adressen oder Pfaden in eine Datei mit paste() oder paste0(). Die erste Funktion fügt ein Leerzeichen zwischen jedem eingefügten Teil des Pfades ein, während die zweite Funktion Pfadteile (oder beliebige Textteile) ohne Leerzeichen zusammenklebt. In den beiden folgenden Beispielen werden drei Komponenten bzw. zwei Bestandteile eines Dateipfades zusammengeklebt. library(tidyverse) # full path with paste0() besedilo &lt;- read_lines(paste0(moja_delovna_mapa, &quot;/&quot;, &quot;data/books/tom.txt&quot;)) besedilo &lt;- read_lines(paste0(moja_delovna_mapa, &quot;/data/books/tom.txt&quot;)) Zusätzlich zu paste() und paste0() verwenden wir andere Bibliotheken zum Einfügen, z.B. glue und here. Die Bibliothek (library) here ist sehr beliebt für das Einfügen von Dateipfadkomponenten. Die Funktion here() selbst bestimmt das aktuelle Arbeitsverzeichnis. Pfadkomponenten werden auf die gleiche Weise wie bei der Funktion paste() oder einfach mit Kommas verkettet. Der Vorteil der Funktion here() besteht also darin, dass Schrägstriche im Dateipfad vermieden werden. Benutzer des Betriebssystems Windows werden sich über diese Funktion besonders freuen, da sie anfangs oft übersehen, dass sie im Dateipfad die falschen Schrägstriche (\" statt/) eingefügt haben. library(tidyverse) library(here) # full path with here() besedilo &lt;- read_lines(here(&quot;data&quot;, &quot;books&quot;, &quot;tom.txt&quot;)) here(&quot;data&quot;, &quot;books&quot;, &quot;tom.txt&quot;) ## [1] &quot;D:/Users/teodo/Documents/R/raj2022-book/data/books/tom.txt&quot; Das Öffnen eines Textes von einer Webadresse (url) ist ebenfalls möglich, aber meist ist es notwendig, den HTML-Code zu entfernen. url &lt;- &quot;http://teachsam.de/deutsch/d_literatur/d_aut/bor/bor_kuech_txt.htm&quot; teachsam_kuechenuhr &lt;- read_lines(url) head(teachsam_kuechenuhr) ## [1] &quot;&lt;html&gt;&quot; ## [2] &quot;&lt;head&gt;&quot; ## [3] &quot;&lt;meta NAME=\\&quot;author\\&quot; CONTENT=\\&quot;Gert Egle\\&quot;&gt;&quot; ## [4] &quot;&lt;meta NAME=\\&quot;copyright\\&quot; CONTENT=\\&quot;Gert Egle/www.teachsam.de lizensiert unter Creative Commons Lizenz: Namensnennung und Weitergabe unter gleichen Bedingungen, CC-BY-SA 4.0 International licenxe, OER\\&quot;&gt;&quot; ## [5] &quot;&lt;meta NAME=\\&quot;keywords\\&quot; CONTENT=\\&quot;Borchert,K&lt;fc&gt;chenuhr, Die K&lt;fc&gt;chenuhr,\\&quot;&gt;&quot; ## [6] &quot;&lt;meta NAME=\\&quot;description\\&quot; CONTENT=\\&quot;In diesem teachSam-Arbeitsbereich k&lt;f6&gt;nnen Sie sich mit Die K&lt;fc&gt;chenuhr von Wolfgang Borchert befassen.\\&quot;&gt;&quot; Es ist wesentlich einfacher, eine Webseite mit Programmen wie z.B. der rvest-Bibliothek den HTML-Code zu entfernen und auf diese Weise einen sauberen Text von einer Webseite herunterzuladen. Das Extrahieren von Text oder Tabellen aus Webseiten (Webscraping) ist eine Technik für fortgeschrittene Benutzer. library(rvest) ## ## Attaching package: &#39;rvest&#39; ## The following object is masked from &#39;package:readr&#39;: ## ## guess_encoding html_document &lt;- read_html(url) # detected with Selector Gadget in in Chrome browser path &lt;- &quot;blockquote blockquote&quot; text &lt;- html_document %&gt;% html_node(path) %&gt;% html_text(trim = T) In dem Text, der von der Bibliothek rvest von der Webseite abgerufen wurde, entfernt die Funktion str_squish() unerwünschte Leerstellen. Der Text wird mit der Funktion str_sub() angezeigt. Im folgenden Beispiel werden nur die ersten 200 Zeichen des Textes angezeigt. Es wird nicht empfohlen, den gesamten Text auf dem Bildschirm ausgeben zu lassen, da dies sehr viel Zeit in Anspruch nehmen kann und den für die Analyse benötigten Arbeitsspeicher belegt. teachsam_kuechenuhr_rvest &lt;- text %&gt;% str_squish() str_sub(teachsam_kuechenuhr_rvest, start = 1, end = 200) ## [1] &quot;Wolfgang Borchert, Die Küchenuhr Sie sahen ihn schon von weitem auf sich zukommen, denn er fiel auf. Er hatte ein ganz altes Gesicht, aber wie er ging, daran sah man, dass er erst zwanzig war. Er setz&quot; Solange es sich nur um den Inhalt einer einzigen Webseite handelt, ist eine Programmierung in der Programmiersprache R (oder irgend einer anderen Programmiersprache) nicht unbedingt erforderlich. Der Text kann natürlich von der Webseite in einen Notizblock (Notepad) oder ein Textverarbeitungsprogramm (z.B. Word, Libre Office) kopiert und dann in einer Datei gespeichert werden. Das Webscraping mit der rvest-Bibliothek ist jedoch eine viel effizientere Technik, da es möglich ist, Tausende von Texten (z.B. Zeitungsartikel) von Webseiten in sehr kurzer Zeit herunterzuladen und auf einer Festplatte zu speichern. 4.3 Öffnen mehrerer Texte Mit der Funktion readtext(), die mit dem Programm library(readtext) geladen wird, lassen sich leicht mehrere Texte von der Festplatte öffnen. Wenn Sie statt des Dateinamens nur ein Sternchen und die Dateierweiterung (z.B. *.txt) im ausgewählten Verzeichnis (z.B. data/books/) angeben, dann öffnet das Programm alle Textdateien mit dieser Erweiterung und speichert diese Sammlung in einer vom Benutzer bestimmten Variablen (z.B. texts). Das Programm erstellt eine Tabelle mit den gelesenen Texten und Metadaten dieser Texte. Die Funktion readtext() öffnet Texte mit verschiedenen Dateisuffixen: txt, csv, docx, pdf, xml, . Die Option encoding (Kodierung) ermöglicht die Angabe des Zeichensatzes (code page), in dem die Texte gespeichert wurden. Vorgegeben ist in der Regel die Codepage *UTF-8*, die die meisten Sonderzeichen von verschiedenen Sprachen (auch Deutsch und Slowenisch) enthält. Sie können auch irgend einen anderen Zeichensatz angeben, und zwar immer eingerahmt in Anführungszeichen, z. B. latin1 für einen westeuropäischen Zeichensatz oder latin2 für einen mittel- und osteuropäischen Zeichensatz. library(readtext) besedila = readtext(&quot;data/books/*.txt&quot;, encoding = &quot;UTF-8&quot;) besedila ## readtext object consisting of 2 documents and 0 docvars. ## # Description: df [2 x 2] ## doc_id text ## &lt;chr&gt; &lt;chr&gt; ## 1 prozess.txt &quot;\\&quot;Der Prozes\\&quot;...&quot; ## 2 tom.txt &quot;\\&quot;Tom Sawyer\\&quot;...&quot; Auch das Öffnen eines Textes von einer Webadresse (url) ist möglich, z.B. PDF-Dateien. Der Text wird mitsamt Metadaten in einer Tabelle gespeichert, die sowohl Eigenschaften eines Datensatzes (data.frame) als auch die einer Liste (list) aufweist. library(readtext) url &lt;- &quot;https://www.moutard.de/wordpress/archiv/sek2dt/muster/kuechenuhr_i.pdf&quot; pdf_readtext = readtext(url) pdf_readtext ## readtext object consisting of 1 document and 0 docvars. ## # Description: df [1 x 2] ## doc_id text ## &lt;chr&gt; &lt;chr&gt; ## 1 kuechenuhr_i.pdf &quot;\\&quot;Wolfgang B\\&quot;...&quot; 4.4 Öffnen von Tabellen Es gibt eine Vielzahl von Programmfunktionen, mit denen man Tabellen in verschiedenen Formaten öffnen kann. Die Funktionen read_csv() oder read_csv2() sind nur zwei der Funktionen zum Öffnen einer Tabelle mit der Endung csv (diese lassen sich übrigens auch mit Excel oder Calc öffnen). Für Anfänger ist es sicher attraktiv, eine Tabelle über das Menü und mit Maus zu importieren File &gt; Import Dataset &gt;  (Datei &gt; Datensatz importieren &gt; ). Dann öffnet sich  ähnlich wie beim Statistikprogramm IBM SPSS  ein Dialogfeld mit einem Assistenten zum Importieren einer Tabelle oder eines Datensatzes, so dass das entsprechende Format festgelegt werden kann. Außerdem zeigt RStudio an, welcher Programmierbefehl zum Öffnen der Tabelle in das Programmskript eingefügt werden kann. Auf diese Weise können Sie die notwendigen Programmierschritte zum Öffnen von Tabellen erlernen. Für die automatische Ausführung des Programmskripts ist es jedoch am besten, eine Befehlszeile zu verfassen, um die Tabelle einzulesen und (optional) anzuzeigen: library(tidyverse) # read data frame tabela = read_csv2(&quot;data/plural_Subj_sum.csv&quot;) # show first rows head(tabela) %&gt;% rmarkdown::paged_table() # prettier RStudio ermöglicht auch den Import von Datensätzen aus einigen anderen Statistikprogrammen (z.B. SPSS, STATA) und Tabellenkalkulationsprogrammen (Excel). Zusätzlich installierbare Softwarefunktionen bieten noch mehr Importmöglichkeiten (z.B. die readxl-Bibliothek für den Import von Excel-Tabellen). Das Öffnen einer Tabelle von einer Webadresse (url) ist ebenfalls möglich, z.B. entweder mit den Funktionen read_csv() oder read_csv2() oder mit der Funktion read_delim(). url &lt;- &quot;https://perso.telecom-paristech.fr/eagan/class/igr204/data/cars.csv&quot; car_dataset &lt;- read_delim(url, delim = &quot;;&quot;, # c = character, n = number col_types = &quot;cnnnnnnnn&quot;) ## Warning: One or more parsing issues, see `problems()` for details car_dataset &lt;- read_csv2(url, # c = character, n = number col_types = &quot;cnnnnnnnn&quot;) ## i Using &quot;&#39;,&#39;&quot; as decimal and &quot;&#39;.&#39;&quot; as grouping mark. Use `read_delim()` for more control. ## Warning: One or more parsing issues, see `problems()` for details car_dataset &lt;- car_dataset[-1,] # remove first row after column name head(car_dataset) ## # A tibble: 6 x 9 ## Car MPG Cylinders Displacement Horsepower Weight Acceleration Model Origin ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Chev~ 180 8 3070 1300 3504 120 70 NA ## 2 Buic~ 150 8 3500 1650 3693 115 70 NA ## 3 Plym~ 180 8 3180 1500 3436 110 70 NA ## 4 AMC ~ 160 8 3040 1500 3433 120 70 NA ## 5 Ford~ 170 8 3020 1400 3449 105 70 NA ## 6 Ford~ 150 8 4290 1980 4341 100 70 NA Die Option col_types stellt sicher, dass beim Aufrufen der Datei ein angemessenes Datenformat (alphanumerische Zeichenkette, Dezimalzahl usw.) verwendet wird. 4.5 Öffnen einer Excel-Tabelle Excel von Microsoft ist ein beliebtes Tabellenprogramm. Daher gibt es auch in der Programmiersprache R mehrere Programme, die das Öffnen von Excel-Dateien (Suffix xlsx) erlauben, z.B. die Funktion read_xlsx() oder read_excel() des Programms readxl. library(readxl) excel = read_xlsx(&quot;data/S03_Vokalformanten_Diagramme.xlsx&quot;, sheet = &quot;A1-4_alle&quot;) head(excel) %&gt;% rmarkdown::paged_table() 4.6 Datei speichern Standardmäßig speichern die meisten Softwarefunktionen Datensätze gemäß der heutzutage bevorzugten Unicode-Zeichensatztabelle, die auch sprachspezifische und andere spezielle Symbole zulässt. Bei bestimmten Programmfunktionen muss dies eventuell angegeben werden, z.B. encoding=\"UTF-8\" bzw. fileEncoding=\"UTF-8\". Im Falle der nachfolgenden tidyverse-Funktionen gilt jedoch, dass standardmäßig im UTF-8-Format gespeichert wird, falls keine gegenteilige Angabe gemacht wird. library(tidyverse) # shranjevanje posaminega besedila write_lines(besedilo, &quot;moje_besedilo.txt&quot;) # shranjevanje tabele, v kateri je zbirka besedil write_csv2(besedila, &quot;moja_tabela_z_besedili.csv&quot;) library(writexl) # shranjevanje preglednice write_xlsx(tabela, &quot;moja_tabela.xlsx&quot;) # shranjevanje tabele, v kateri je zibrka besedil # Excel dovoljuje do 32767 znakov. # ta zbirko presega to mejo, zato je ne moremo shraniti v Excelovi preglednici # write_xlsx(besedila, &quot;moja_tabela_z_besedili.xlsx&quot;) Nun folgen weitere grundlegende Datei-Operationen in R: z.B. - Download von Zip-Dateien (zip files); - Extrahieren von komprimierten Dateien in ein Verzeichnis (extract compressed files to a folder); - Überprüfen und Anlegen eines oder mehrer Verzeichnisse (check &amp; create a folder or subfolders); - Durchsuchen und Lesen von Dateien in einem Verzeichnis (list &amp; read files in a folder). 4.7 Download von Zip-Dateien Eine Softwarefunktion zum Herunterladen einer Datei aus dem Internet, unter Verwendung der Funktion download.file(), die die Angabe der Webadresse (url) und den Pfad (location, pot) zum Speichern der Datei benötigt. Im folgenden Beispiel wird die komprimierte Datei im Arbeitsverzeichnis dieses R-Projekts gespeichert. Um uns etwas Schreibarbeit zu ersparen, verwenden wir wieder die Funktion here(). url &lt;- &quot;https://github.com/tpetric7/tpetric7.github.io/archive/refs/heads/main.zip&quot; # location &lt;- &quot;d:/Users/teodo/Downloads/tpetric7-master.zip&quot; location &lt;- here(&quot;tpetric7-master.zip&quot;) download.file(url, location) 4.8 Verzeichnisse 4.8.1 Existenz eines Verzeichnisses prüfen Ist ein Verzeichnis bereits vorhanden? Wir überprüfen dies mit der Funktion dir.exists(), die nach dem Dateipfad fragt. Auch hier verwenden wir die Funktion here(), um den Dateipfad anzugeben. Eine TRUE-Ausgabe bedeutet, dass der Ordner existiert, eine FALSE-Ausgabe bedeutet, dass er noch nicht angelegt ist. # pot &lt;- &quot;d:/Users/teodo/Downloads/tpetric7-master&quot; pot &lt;- here(&quot;tpetric7-master&quot;) exist &lt;- dir.exists(pot) exist ## [1] FALSE 4.8.2 Verzeichnis anlegen Wenn ein Verzeichnis noch nicht existiert, können wir es erstellen. Ist es bereits angelegt ist, können wir Auskunft darüber erhalten. Das Erstellen eines neuen Verzeichnisses erfolgt mit der Funktion dir.create(), die nach dem Pfadnamen des neuen Ordners fragt. Dieser Programmierbefehl wird oft mit der bedingten Anweisung ifelse() kombiniert. Der Bedingungssatz beginnt mit einer Bedingung. Im folgenden Beispiel ist dies exist == FALSE. Wenn die Bedingung erfüllt ist (d.h. dass der Ordner noch nicht existiert), dann gibt das Programm TRUE aus und erstellt einen Ordner mit einem Namen nach unserer Wahl. Wenn die Bedingung nicht erfüllt ist (d.h. dass der Ordner mit dem angegebenen Namen bereits existiert ), dann wird directory already exists (Verzeichnis existiert bereits) ausgegeben. ifelse(exist == FALSE, dir.create(pot, showWarnings = TRUE, recursive = TRUE), &quot;directory already exists&quot;) ## [1] TRUE 4.8.3 Unterverzeichnisse anlegen Wir wollen mehrere Verzeichnisse anlegen. - Definieren wir zunächst einen Vektor mit Namen der Verzeichnisse. In der Programmschleife benötigen wir den Namen des Vektors subfolder_names. - Wir verwenden die Programmschleife for(){}, die so lange läuft, bis die Programmfunktion jedem Verzeichnis einen Namen zugewiesen hat. - Wie oft die Schleife ausgeführt wird, wird durch eine Zahl angegeben. Wir könnten die Zahl 4 angeben, da wir zuvor vier Namen für die neuen Ordner definiert haben. Eine elegantere Methode ist die Verwendung der Funktion length() (Länge des Vektors), die dem Programm das letzte Glied der Sequenz mitteilt, d.h. das Ende der Schleife. Der Zähler i ist anfangs gleich eins und wird nach jedem Schleifendurchlauf um eins erhöht. Da der Vektor vier Namen enthält, endet die Ausführung der Programmschleife nach dem vierten Durchlauf. - In der Schleife führen wir am Schluss die Funktion dir.create() aus. Das Ergebnis speichern wir in einer Variable namens folder (Ordner). Im folgenden Beispiel erstellt das Programm Unterordner in dem Ordner (die Variable heißt pot, dt. Pfad), den wir oben erstellt haben. subfolder_names &lt;- c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;) for (i in 1:length(subfolder_names)){ folder &lt;- dir.create(paste0(pot, &quot;/&quot;, subfolder_names[i])) } 4.8.4 Verzeichnis entfernen Das Programm kann die Verknüpfung mit Ordnern mit der Funktion unlink() aufheben. Im folgenden Beispiel werden wir vier (leere) Verzeichnisse entfernen, die wir kurz zuvor erstellt haben (siehe oben die dafür verwendeten Ordnernamen!). Wir entfernen zuerst nur einen Ordner. unlink(here(pot, &quot;a&quot;), recursive = TRUE) Sie können auch mehrere Verzeichnis mit einem Befehl entfernen. In diesem Fall brauchen wir eine Programmschleife. Im folgenden Beispiel werden die verbleibenden drei Unterordner entfernt: der Zähler i beginnt also mit 2 und endet mit 4, was durch length() angegeben wird. Die Option recursive = TRUE bedeutet, dass das Programm auch alle Unterverzeichnisse in diesen Ordnern löscht. for (i in 2:length(subfolder_names)){ unlink(here(pot, subfolder_names[i]), recursive = TRUE) } 4.9 Zip-Dateien entpacken und löschen Die komprimierte Datei wird mit der Funktion unzip() entpackt, und der Pfad oder Ort der entpackten Datei wird mit der Option exdir angegeben. Im folgenden Beispiel wird davon ausgegangen, dass Sie die gezippte Datei bereits auf die Festplatte heruntergeladen haben, und zwar in das Arbeitsverzeichnis. # unzip(&quot;d:/Users/teodo/Downloads/tpetric7-master.zip&quot;, exdir = pot) unzip(&quot;tpetric7-master.zip&quot;, exdir = pot) Das Verzeichnis und die komprimierte Datei werden nicht mehr benötigt. In den nächsten beiden Schritten werden wir beide und alles, was sich darin befindet, wieder entfernen. unlink(pot, recursive = TRUE) unlink(&quot;tpetric7-master.zip&quot;, recursive = TRUE) 4.10 Dateilisten anzeigen Die Liste der Dateien in einem oder mehreren Verzeichnissen auf der Festplatte eines Computers wird mit der Funktion list.files() angezeigt. seznam &lt;- list.files(&quot;data/books&quot;, pattern = &quot;\\\\.txt$&quot;, recursive = TRUE, full.names = TRUE) seznam ## [1] &quot;data/books/hesse/Hermann Hesse Der Steppenwolf.txt&quot; ## [2] &quot;data/books/kleist/Kleist_Kohlhaas_Projekt_Gutenberg.txt&quot; ## [3] &quot;data/books/kleist/Michael_Kohlhaas_english.txt&quot; ## [4] &quot;data/books/prozess.txt&quot; ## [5] &quot;data/books/tom.txt&quot; ## [6] &quot;data/books/translations/metamorph/metamorphosis.txt&quot; ## [7] &quot;data/books/translations/metamorph/verwandlung.txt&quot; ## [8] &quot;data/books/translations/prozess/prozess.txt&quot; ## [9] &quot;data/books/translations/prozess/trial.txt&quot; ## [10] &quot;data/books/translations/sawyer/tom_de.txt&quot; ## [11] &quot;data/books/translations/sawyer/tom_en.txt&quot; ## [12] &quot;data/books/verwandlung/verwandlung.txt&quot; 4.11 Mehrere Dateien öffnen 4.11.1 quanteda: Die oben bereits besprochene Funktion readtext() kann mehrere Dateien in einem oder mehreren Verzeichnissen öffnen. Wenn diese Funktion mal nicht verwendet werden kann, gibt es mehrere andere Möglichkeiten. 4.11.2 tidyverse: Im tidyverse-Programmpaket ist das Programm purrr mit der Funktion map() verfügbar, die die klassische for(){}-Schleife effizient zu ersetzen vermag. Im folgenden Beispiel verwenden wir die Funktion read_lines(), um den Text aus einer Liste zu öffnen, die wir zuvor mit der Funktion list.files() erstellt haben. Die Funktion map() hingegen hat den gleichen Zweck wie die for()-Schleife, ist aber kürzer. Auf diese Weise wird der gesamte Text der Dateiliste in den Arbeitsspeicher des Computers geladen und in einer beliebig benannten Variable gespeichert. Die Funktion substr() hingegen ermöglicht es uns, den gesamten Text oder normalerweise nur einen Teil des Textes anzuzeigen. Wir haben oben eine ähnliche Funktion verwendet, str_sub(). Im folgenden Beispiel wollen wir den Inhalt der ersten Datei [1] und der zweiten Datei [2] in der Liste sehen. Wir wollen nicht den gesamten Text drucken, sondern nur die ersten 50 oder 70 Zeichen. library(tidyverse) alltxt &lt;- seznam %&gt;% map(read_lines) substr(alltxt[1], 1, 50) ## [1] &quot;c(\\&quot;Hermann Hesse \\&quot;, \\&quot;Der Steppenwolf \\&quot;, \\&quot;\\&quot;, \\&quot;Erzäh&quot; substr(alltxt[2], 1, 70) ## [1] &quot;c(\\&quot;Heinrich von Kleist\\&quot;, \\&quot;Michael Kohlhaas\\&quot;, \\&quot;Aus einer alten Chronik\\&quot;&quot; 4.11.3 Base R: Im Grundfunktionssatz der Programmiersprache R hat der lapply()-Befehl eine ähnliche Funktion wie der map()-Befehl oder die for()-Schleife. Die Funktion map() verfügt jedoch über weitere fortgeschrittene Optionen, die wir hier nicht ausschöpfen. alltxt &lt;- lapply(seznam, readLines) substr(alltxt[1], 1, 50) ## [1] &quot;c(\\&quot;Hermann Hesse \\&quot;, \\&quot;Der Steppenwolf \\&quot;, \\&quot;\\&quot;, \\&quot;Erz¤&quot; substr(alltxt[2], 1, 70) ## [1] &quot;c(\\&quot;Heinrich von Kleist\\&quot;, \\&quot;Michael Kohlhaas\\&quot;, \\&quot;Aus einer alten Chronik\\&quot;&quot; 4.12 Zeichensatz konvertieren Die Zeichensatztabelle (code page) kann durch die Funktion iconv() ersetzt werden. Die unten folgende Programmzeile wandelt den Text tom.txt (siehe oben) von der Codepage UTF-8 in die Codepage latin1 um. Die Funktion Encoding() ermittelt die Zeichensatztabelle (Code page) eines Textes. Die Funktion str_sub(), die zur Anzeige eines Textausschnitts dient, wurde bereits oben eingeführt. alltxt1_converted &lt;- iconv(besedilo, from = &quot;UTF-8&quot;, to = &quot;latin1&quot;) Encoding(alltxt1_converted) ## [1] &quot;latin1&quot; str_sub(alltxt1_converted, 4800, 4900) ## [1] &quot;u führen . Wenn ich ihn sich selbst überlasse , drückt mich mein Gewissen , und so oft ich ihn s&quot; 4.13 Datensatz vorbereiten Oft müssen wir einen Datensatz oder eine Tabelle umwandeln, um die Antwort auf eine Forschungsfrage zu finden. Die Datenumwandlung wird oft im Englischen als Data Wrangling (Datenringen, Datenraufen) bezeichnet. Im Folgenden werden wir uns einige der gebräuchlichsten Tidyverse-Funktionen zur Vorbereitung eines Datensatzes oder einer Tabelle ansehen: - select() - group_by() - filter() - mutate() - count() - summarise() - arrange() - separate() - unite() - pivot_longer() - pivot_wider() - pull() - Date(). Die Funktionsweise der einzelnen Programmfunktionen wird anhand des Datensatzes tweets_rollingstones.rds veranschaulicht, den wir zunächst mit read_rds() öffnen. tweets_rollingstones &lt;- read_rds(&quot;data/tweets_rollingstones.rds&quot;) Wenn wir dem Namen des Datensatzes das Zeichen $ hinzufügen, werden die Variablen (Spalten der Tabelle) angezeigt: z.B. tweets_rollingstones\\(user_id*, *tweets_rollingstones\\)text usw. Der Operator %&gt;%% (Tastenkürzel: Crtl+Shift+M) verbindet die Programmschritte bzw. Zeilen miteinander: (a) die Schritte werden in zeitlicher Reihenfolge ausgeführt, und (b) die folgenden Schritte übernehmen das Objekt aus dem ersten Programmschritt (d.h. den Datensatz). Dies verkürzt die einzelnen Programmschritte. In vielen Fällen kann man stattdessen auch den Operator |&gt; (Base-R-Funktion) einsetzen. Die Funktion head() begrenzt die Anzahl der auszugebenden Datenzeilen. tweets_rollingstones$created_at %&gt;% head(3) ## [1] &quot;2021-08-29 17:35:52 UTC&quot; &quot;2021-08-29 11:19:40 UTC&quot; ## [3] &quot;2021-08-25 16:39:18 UTC&quot; Andere Möglichkeiten, die Namen von Variablen (Spalten) in einer Tabelle anzuzeigen, sind z.B. names() oder colnames(). Sie können auch die Funktionen glimpse() oder skim() des Programms skim verwenden, die ausführlichere Auskunft über die Struktur und Variablennamen im Datensatz geben. Eine ähnliche Funktion hat auch die summary()-Funktion, die zu den Grundfunktionen der R-Programmiersprache gehört. Die folgende Datenausgabe ist auf 10 Namen begrenzt. Ermöglicht wird dies durch die Funktion head(). names(tweets_rollingstones) %&gt;% head(10) ## [1] &quot;user_id&quot; &quot;status_id&quot; &quot;created_at&quot; ## [4] &quot;screen_name&quot; &quot;text&quot; &quot;source&quot; ## [7] &quot;display_text_width&quot; &quot;reply_to_status_id&quot; &quot;reply_to_user_id&quot; ## [10] &quot;reply_to_screen_name&quot; # colnames(tweets_rollingstones) # glimpse(tweets_rollingstones) # skimr::skim(tweets_rollingstones) 4.13.1 select() Mit der Funktion select() können Sie Spalten in einer Tabelle auswählen. Es gibt mehrere Auswahlmöglichkeiten. Im folgenden Beispiel wird nur eine Spalte (Variable) ausgewählt und die drei ersten Zeilen davon angezeigt. tweets_rollingstones %&gt;% select(screen_name) %&gt;% head(3) ## # A tibble: 3 x 1 ## screen_name ## &lt;chr&gt; ## 1 aliceinglen ## 2 MatjazDerin ## 3 MarliMarkez Sie können mehrere aufeinanderfolgende Spalten auswählen, indem Sie einen Doppelpunkt zwischen die Namen der äußeren Variablen (Spalten in der Tabelle) setzen. Ein Komma wird zwischen zwei Variablen gesetzt, die im Datensatz nicht unmittelbar nacheinander auftreten. Im folgenden Beispiel werden vier Spalten ausgewählt, von denen die letzten drei nacheinander erscheinen, daher wird ein Doppelpunkt zwischen den Variablen created_at und text verwendet. Auf die erste Spalte user_id folgt ein Komma, weil zwischen ihr und created_at eine weitere Spalte liegt, die wir nicht ausgewählt haben. tweets_rollingstones %&gt;% select(user_id, created_at:text) %&gt;% # which columns? head(3) %&gt;% # 3 rows only rmarkdown::paged_table() # prettier table Das Minus vor dem Spaltennamen bedeutet, dass Sie die Spalte ausschließen möchten. Mehrere Spalten können gleichzeitig eliminiert werden, indem das Zeichen c() mit vorangestelltem Minus verwendet wird. tweets_rollingstones %&gt;% select(lang, location, user_id:status_id, source, created_at:text) %&gt;% select(-lang) %&gt;% # remove one column select(-c(user_id, status_id)) %&gt;% # remove two columns select(-c(created_at, screen_name:text)) %&gt;% # multiple columns head(3) %&gt;% rmarkdown::paged_table() 4.13.2 filter() Während die Funktion select() Spalten (Variablen) auswählt oder eliminiert, können wir mit der Funktion filter() die Anzahl der Zeilen begrenzen, die wir im Verlauf der Analyse verwenden möchten. Es muss zumindest eine Bedingung angegeben werden, es können jedoch auch mehrere (komplexe) Bedingungen sein. Vorsicht ist geboten bei der Verwendung logischer Operatoren (z. B. Doppelgleichheitszeichen: == statt einfachem Gleichheitszeichen: =). Das folgende Beispiel zeigt eine einfache Filterung eines Datensatzes. In späteren Kapiteln werden wir oft komplexere Filtermuster benötigen, die Funktionen wie str_detect() und Kenntnisse über reguläre Ausdrücke (regular expressions) verlangen. Die Filterbedingung ist die Auswahl von slowenischen Tweets, d.h. die Spalte lang (language, Sprache) muss die Zeichenkette sl enthalten. Zeilen, die diese Bedingung nicht erfüllen, werden in der gefilterten Tabelle nicht angezeigt. tweets_rollingstones %&gt;% select(lang, user_id, created_at:text) %&gt;% # which columns? filter(lang == &quot;sl&quot;) %&gt;% # Slovenian tweets only head(3) %&gt;% # 3 rows only rmarkdown::paged_table() # prettier table Solange dieses Ergebnis nicht gespeichert wird (z. B. als tweets_rollingstones_slovenski), bleibt der Datensatz unverändert. Das Gleiche gilt für alle anderen erwähnten Funktionen zur Anpassung von Tabellen. Das Format oder Ergebnis, das wir behalten wollen, muss unter einem (neuen) Namen gespeichert werden. Es können mehrere Bedingungen verwendet werden. Um Bedingungen miteinander zu verknüpfen, verwenden wir einen logischen Operator: Anstelle des Plus (+)-Zeichens fügen wir &amp; (d.h. ampersand) ein, und um eine alternative Bedingung aufzurufen, verwenden wir den logischen Operator | (d.h. oder). Der logische Operator &amp; bedeutet, dass beide Bedingungen erfüllt sein müssen. Wenn wir den logischen Operator | einfügen würden, wäre es ausreichend, wenn mindestens eine dieser Bedingungen erfüllt wäre, und es wäre zulässig, wenn beide erfüllt wären. Im folgenden Beispiel werden zwei Bedingungen verwendet: Der Tweet muss auf Slowenisch sein (Spalte lang == sl) und die Spalte location (Ort) darf nicht leer sein (location != ). Die erste Bedingung enthält den logischen Operator == (zwei Gleichheitszeichen!), und die zweite Bedingung enthält den logischen Operator != (Ausrufezeichen + Gleichheitszeichen). Mit dem Ausrufezeichen wird das Gleichheitszeichen negiert. tweets_rollingstones %&gt;% select(lang, user_id, location, created_at:text) %&gt;% # which columns? filter(lang == &quot;sl&quot; &amp; location != &quot;&quot;) %&gt;% # Slovenian tweets only head(3) %&gt;% # 3 rows only rmarkdown::paged_table() # prettier table 4.13.3 group_by() Die group_by()-Funktion ermöglicht das Gruppieren aller Datensatzspalten nach einer Spalte oder nach mehreren Spalten. Die Gruppierung der Spalte(n) ist oft erwünscht im Zusammenhang mit anderen Funktionen, die den Datensatz zusammenfassen (aggregieren), wie z.B. bei Verwendung der summarise()-Funktion. tweets_rollingstones %&gt;% select(lang, user_id, location, created_at:text) %&gt;% # which columns? group_by(lang) %&gt;% tail(3) %&gt;% # 3 last rows only rmarkdown::paged_table() # prettier table 4.13.4 mutate() Mit der mutate()-Funktion bilden wir neue Tabellenspalten (Variablen) oder verändern bereits bestehende. Im folgenden Beispiel bilden wir die Spalte tweet_length, die mit Hilfe der Funktion nchar() die Anzahl der Zeichen in der Tabellenspalte text auszählt. tweets_rollingstones %&gt;% select(lang, text) %&gt;% # which columns? # how many characters does each tweet have? - nchar() mutate(tweet_length = nchar(text)) %&gt;% # new column select(lang, tweet_length, text) %&gt;% # re-ordering of columns head(5) %&gt;% # show only first five rows rmarkdown::paged_table() # prettier table 4.13.5 count() Mit der count()-Funktion kann man die Anzahl von Kategorienwerten ermitteln. Beim Auszählen ist der group_by()-Befehl nicht notwendig, da der count()-Befehl die Daten vor dem Auszählen gruppiert. Im folgenden Beispiel wird ausgezählt, wie viele Tweets in jeder der drei Sprachen (englisch, deutsch, slowenisch) im Datensatz vorkommen. Es ist ersichtlich, dass dieser Datensatz vor allem englische Tweets enthält. tweets_rollingstones %&gt;% select(lang, user_id, location, created_at:text) %&gt;% # which columns? count(lang) %&gt;% rmarkdown::paged_table() # prettier table 4.13.6 summarise() Die summarise()- oder summarize()-Funktion dient der Zusammenfassung (Aggregation) von Datenwerten. Im folgenden Beispiel wird die Auswahl des Datensatzes nach den der Spalte für die Sprachen (lang) gruppiert. Daher berechnet der folgende summarise()-Befehl die durchschnittliche Länge der Tweets (mean(nchar(text))) pro Sprache (de, en, sl). Ohne den group_by()-Befehl erhielte man mit dem summarise()-Befehl den Durchschnittswert für alle Tweets (ohne Sprachen zu unterscheiden). tweets_rollingstones %&gt;% group_by(lang) %&gt;% summarise(tweet_length = mean(nchar(text))) %&gt;% rmarkdown::paged_table() # prettier table 4.13.7 round() Mit der round()-Funktion kann man Dezimalzahlen auf- oder abrunden. Im folgenden Beispiel machen wir das in einem speziellen Schritt, damit die Syntax der Funktion besser zu sehen ist. tweets_rollingstones %&gt;% group_by(lang) %&gt;% summarise(tweet_length = mean(nchar(text))) %&gt;% mutate(tweet_length = round(tweet_length, 0)) %&gt;% # no decimal values rmarkdown::paged_table() # prettier table 4.13.8 arrange() Der arrange()-Befehl ist eine Sortierfunktion. Im folgenden Beispiel wird das Ergebnis nach Länge der Tweets aufsteigend sortiert. tweets_rollingstones %&gt;% select(lang, user_id, location, created_at:text) %&gt;% # which columns? group_by(lang) %&gt;% summarise(tweet_length = mean(nchar(text))) %&gt;% arrange(tweet_length) %&gt;% rmarkdown::paged_table() # prettier table Will man die Reihenfolge der Sortierung umkehren, muss man vor dem Spaltennamen ein Minuszeichen setzen (z.B. arrange(-tweet_length) oder desc für descending (absteigend, z.B. arrange(desc(tweet_length))). 4.13.9 distinct() Die distinct()-Funktion entfernt doppelt oder mehrfach auftretende Werte aus einer Tabellenspalte (oder mehreren Spalten). Eine ähnliche Funktion ist unique(). Zu beachten ist, dass R im Gegensatz zu Excel Groß- und Kleinschreibung unterscheidet. Deshalb sind z.B. Slovenija und slovenija keine verdoppelten Datenzeilen. tweets_rollingstones %&gt;% distinct(location) %&gt;% head(10) %&gt;% rmarkdown::paged_table() # prettier table 4.13.10 str_to_lower() Groß- und Kleinschreibung von Buchstaben kann man mit einer Reihe von Funktionen vereinheitlichen, z.B. mit str_to_lower() (nur Kleinbuchstaben), str_to_upper() (nur Großbuchstaben), str_to_sentence() (erstes Wort mit großem Anfangsbuchstaben - wie im Satz), str_to_title() (alle Wörter mit großen Anfangsbuchstaben - wie in englischen Titeln üblich). tweets_rollingstones %&gt;% mutate(location = str_to_title(location)) %&gt;% distinct(location) %&gt;% head(10) %&gt;% rmarkdown::paged_table() # prettier table 4.13.11 str_replace() Die Funktion str_replace() ermöglicht es, einzelne oder mehrere Zeichen in einer Spalte durch ein anderes (andere) zu ersetzen. Zum mehrmaligen Zeichenaustausch in einer Tabellenspalte dient str_replace_all(). Beide Funktionen wirken nur in alphabetisch gekennzeichneten Tabellenspalten (chr - character). Für den Austausch von Zahlenwerten in numerisch gekennzeichneten Spalten (dbl / int - double / integer) verwendet man replace() bzw. replace_all(). Der gsub()-Befehl funktioniert in beiden Fällen, erwartet aber, dass der Name der Tabellenspalte zuletzt angegeben wird. In Tabellen werden die angeführten Ersatz-Befehle gewöhnlich mit dem mutate()-Befehl verknüpft. tweets_rollingstones %&gt;% mutate(location = str_replace(location, # which column? pattern = &quot;Istra,&quot;, # search for &quot;&quot;)) %&gt;% # replace with empty string distinct(location) %&gt;% head(10) %&gt;% rmarkdown::paged_table() # prettier table 4.13.12 separate() und unite() Mit der separate()-Funktion kann man eine bestehende Tabellenspalte in zwei oder mehrere neue Tabellenspalten zerlegen. Der unite()-Befehl bewirkt das Gegenteil. Im Beispiel wollen wir die Spalte location in die Spalten city und country zerlegen. In vielen Tabellenspalten sehen wir, dass zwischen angegebenem Ort und Land ein Komma steht. Wir nutzen das Komma als Trennzeichen, um zwei neue Spalten zu bilden. Wie gut das funktioniert, hängt natürlich davon ab, wie konsequent die Angaben in der Spalte sind. tweets_rollingstones %&gt;% separate(location, into = c(&quot;city&quot;, &quot;country&quot;), sep = &quot;,&quot;, remove = FALSE) %&gt;% # keep the location column distinct(city, country) %&gt;% head(10) %&gt;% rmarkdown::paged_table() # prettier table Ein Beispiel mit der unite()-Funktion: tweets_rollingstones %&gt;% unite(user, c(screen_name, user_id), sep = &quot;_&quot;) %&gt;% # keep the location column distinct(user) %&gt;% head() %&gt;% rmarkdown::paged_table() # prettier table 4.13.13 pivot_wider() Mit der pivot_wider()-Funktion können wir eine Tabelle, die im langen Datenformat vorliegt, in eine weite Tabelle umwandeln. tweet_length_wide &lt;- tweets_rollingstones %&gt;% group_by(lang) %&gt;% summarise(tweet_length = mean(nchar(text), na.rm = TRUE)) %&gt;% pivot_wider(names_from = lang, values_from = tweet_length) tweet_length_wide %&gt;% rmarkdown::paged_table() # prettier table 4.13.14 pivot_longer() In vielen Fällen benötigen wir eine Tabelle im langen Datenformat (z.B. um Diagramme mit ggplot() darzustellen). Das lässt sich mit der pivot_longer()-Funktion bewerkstelligen. tweet_length_wide %&gt;% # data in wide format pivot_longer(de:sl, # which columns? names_to = &quot;Sprachen&quot;, # new categorical column values_to = &quot;Tweetlänge&quot;) %&gt;% # new numerical column rmarkdown::paged_table() # prettier table 4.13.15 Datumsfunktionen Es gibt eine ganze Reihe von Datums- und Zeitfunktionen. Im folgenden Beispiel zerlegen wir die Datums- und Zeitangabe, die im speziellen POSIXct-Format vorliegt, in mehrere Tabellenspalten (Jahr, Monat, Tag, Uhrzeit). Die Datumsspalte kann man entweder mit der oben vorgeführten separate()-Funktion zerlegen oder mit Funktionen des Programms lubridate. Im Beispiel verwenden wir die lubridate-Funktionen year(), month() und day(). Durch die Umwandlung erhalten wir drei numerische Tabellenspalten (Jahr, Monat, Tag). Mit dem Programm hms und der Funktion as_hms() extrahieren wir die Uhrzeit aus der Spalte created_at im hms-Format. Dieses Format eignet sich beispielsweise für die Berechnung von zeitlichen Abständen (Dauer). Braucht man die Uhrzeit dagegen im alphabetischen Format, kann man die lubdridate-Funktion ymd_hms() und die Base-R-Funktion format() nutzen. library(lubridate) library(hms) tweets_rollingstones %&gt;% select(screen_name, created_at) %&gt;% mutate(Jahr = year(created_at), Monat = month(created_at), Tag = day(created_at), Uhrzeit = as_hms(created_at), Zeit = ymd_hms(created_at), Zeit = format(Zeit, format = &quot;%H:%M:%S&quot;)) %&gt;% select(-created_at) %&gt;% rmarkdown::paged_table() # prettier table Es gibt noch viele weitere Funktionen zur Datenmanipulation. Im Internet sind zahlreiche Portale zu finden (z.B. stackoverflow), wo man nach entsprechenden Lösungen für sein eigenes Programmskript suchen kann. 4.14 Umwandlung eines R-Skripts Ein Programmskript oder ein Datensatz mit dem Suffix R kann in ein Rmarkdown-Dokument (mit dem Suffix Rmd) umgewandelt werden, indem man dies tut: [Strg + Umschalt + K]` oder knitr::spin(\"t_preskus.R\"). In beiden Fällen handelt es sich um ein Textformat, so dass die Umwandlung in verschiedene andere Formate relativ einfach ist. "],["stichprobentests.html", "Kapitel 5 Stichprobentests 5.1 Nominalskalierte Größen 5.2 Intervallskalierte Größen", " Kapitel 5 Stichprobentests 5.1 Nominalskalierte Größen Chi-Quadrat-Test (\\(\\chi^2\\)-Test): Der \\(\\chi^2\\)-Test ist einer der grundlegenden statistischen Tests zum Vergleich von nominalskalierten Kategorien, z.B. biologisches Geschlecht: Frauen vs. Männer; Größe: klein vs. groß; Texte: Text A vs. Text B vs. Text C  Mit dem \\(\\chi^2\\)-Test testen wir, ob eine beobachtete Verteilung der Daten der erwarteten Verteilung entspricht. Der Test funktioniert auf allen Skalenniveaus. Es gibt aber verschiedene Anwendungsspielarten: als Anpassungstest (z.B. ist ein Merkmal normalverteilt?); als Homogenitätstest (z.B. ähneln sich Frauen und Männer bezüglich eines Merkmals, etwa ob sie rauchen oder nicht?); als Unabhängigkeitstest (z.B. ist der Dieselverbrauch unabhängig von elektronischer Regulierung des Motors oder nicht?). Ein Beispiel aus einem empfehlenswerten Video aus Kurzes Tutorium Statistik, in dem der \\(@chi³2\\)-Test und seine Anwendungen erklärt werden. Im Beispiel geht es um den \\(@chi³2\\)-Anpassungstest: Eine Firma verkauft Armbanduhren in vier Farben (blau, grün, gelb, rot). Im letzten Monat wurden 1000 Stück verkauft. Ein Verkaufsleiter behauptet, dass die Nachfrage nach der Uhr in allen vier Farben gleich gut sei. Das können wir mit dem \\(\\chi^2\\)-Test überprüfen. Wenn die Behauptung des Verkaufsleiters stimmt, dann erwarten wir, dass 250 blaue Uhren, 250 grüne Uhren, 250 gelbe Uhren und 250 rote Uhren verkauft wurden - dass also Gleichverteilung der erwarteten Häufigkeiten vorliegt (250 + 250 + 250 + 250). Also ein Viertel der verkauften Uhren war blau, ein Viertel war gelb, ein Viertel war rot und ein Viertel war grün. Wären die (beobachteten) Verkaufszahlen im vergangenen Monat (unserer Stichprobe) 245 + 252 + 254 + 249, dann würde der \\(@chi^2\\)-Test bestätigen, dass Gleichverteilung der Uhrfarben vorliegt und damit die Hypothese \\(H_0\\) bestätigen. Die Unterschiede sind ja gering. Wenn aber die beobachteten Verkaufszahlen in unserer Stichprobe 60 + 320 + 100 + 520 wären, dann würde der \\(@chi^2\\)-Test die Gleichverteilung der Farben nicht bestätigen und die Nullhypothese \\(H_0\\) verwerfen. Die Statistikexpertin erhält vom Verkaufsleiter die tatsächlichen Verkaufszahlen: 300 blaue Urhen + 200 gelbe Uhren + 400 rote Uhren + 100 grüne Uhren wurden im vergangenen Monat verkauft. Kann man das noch immer als Gleichverteilung der Farben auffassen? Wir verwenden die folgende Teststatistik: - Wir subtrahieren die jeweilige erwartete Häufikgeit von der beobachteten und erhalten somit Differenzen; - dann quadrieren wir jede Differenz, so dass wir nur mit positiven Zahlenwerten zu tun haben; - dann dividieren wir jede der quadrierten Differenzen mit der erwarteten Häufigkeit (hier: 250) und erhalten somit Quotienten; - dann addieren wir die Quotienten und erhalten somit den empirischen \\(@chi^2\\)-Wert (im Beispiel beträgt dieser 200). \\[ \\frac{(300 - 250)^2}{250} + \\frac{(200 - 250)^2}{250} + \\frac{(400 - 250)^2}{250} + \\frac{(100 - 250)^2}{250} = 200 = \\chi^2_{empirisch} \\] Das Test- oder Signifikanzniveau (auch Irrtumswahrscheinlichkeit genannt) wird gewöhnlich auf 5% festgelegt (p = 0,05). Die Wahrscheinlichkeit, dass wir fälschlicherweise die Nullhypothese verwerfen, soll demnach bei diesem Testniveau höchstens 5% betragen. Da die Warscheinlichkeit ein Viertel pro Uhrfarbe beträgt (250 von 1000; siehe oben), liegt eine Binomialverteilung vor. Bei ausreichend großen Stichproben (wie der hier vorliegenden) kann man diese durch die Normalverteilung ersetzen. Mit der Normalverteilung lässt sich einfacher rechnen. Da wir in unserer Teststatistik die erwarteten Häufigkeiten von den beobachteten abziehen und danach dividieren, wird die Normalverteilung zum Nullpunkt des Koordinatensystems verschoben. Die Werte der Teststatistik werden durch diesen Rechenvorgang normalisiert. Durch Quadrieren der Differenzen erreichen wir, dass wir keine negativen Werte mehr erhalten können. Alle Werte sind damit positiv und befinden sich im ersten Quadranten des Koordinatensystems. Da wir mehrere Terme addieren (hier sind es 4) und damit potentiell mehrere Zufallsvariablen in die Summe einbeziehen, kann die Verteilungskurve verschiedene Formen annehmen. Das Ergebnis ist eine \\(\\chi^2\\)-Verteilung. Diese Verteilung sagt uns, welche Werte die Teststatistik mit welcher Wahrscheinlichkeit annehmen wird, wenn die Nullhypothese \\(H_0\\) stimmt. Danach sind die Werte in der Nähe des Koordinatenursprungs (der Null) wahrscheinlich. Die meisten Werte unserer Teststatistik sollten gemäß der Nullhypothese in diesem Bereich, dem Annahmebereich, liegen. Werte, die weit entfernt von der Null (dem Koordinatenursprung) vorkommen, sind weniger wahrscheinlich. Sie liegen im Ablehnungsbereich (Verwerfungsbereich). Eine grundlegende Bedingung für die Anwendung des \\(\\chi^2\\)-Tests ist, dass die erwarteten Häufigkeiten nicht kleiner als fünf sein dürfen: \\(Freq_{erwartet}\\geq{5}\\). In unserem Beispiel ist das der Fall (hier: 250). In unserem Beispiel haben wir vier Summenterme, die die Gesamtsumme 1000 (Uhren) ergeben müssen. Die ersten drei Summen könnten vom Zufall abhängen, die letzte ist dagegen immer die Differenz zur Gesamtsumme (hier: 1000). In unserem Beispiel gibt es demnach nur drei Größen (Summen), die frei variieren können. In unserem Beispiel liegen demnach drei Freiheitsgrade vor. Das ist notwendig zu wissen, falls man (noch) mit Tabellen arbeitet und wenn man sich sicher sein möchte, dass man den Test richtig durchgeführt hat. Bei drei Freiheitsgraden und einem Signifikanzniveau von 5% beträgt der kritische \\(\\chi^2\\)-Wert (Schwellenwert für die Annahme bzw. Ablehnung der Nullhypothese) etwa 7,815. Wenn die Nullhypothese stimmt, dann beträgt unsere Teststatistik mit 95%-iger Wahrscheinlichkeit höchstens 7,815. Unser empirischer \\(\\chi^2\\)-Wert beträgt jedoch 200 und ist damit größer als der Schwellenwert (kritische Wert). Das bedeutet, dass wir die Nullhypothese verwerfen und die alternative Hypothese \\(H_1\\) annehmen. Zusammenfassung Frage: Werden die Uhrfarben gleichhäufig verkauft? Hypothese \\(H_0\\): Die Farben werden gleichhäufig verkauft. Hypothese \\(H_1\\): Die Farben werden NICHT gleichhäufig verkauft. Testverteilung: \\(\\chi^2\\)-Verteilung. Testniveau: \\(\\alpha = 5%\\) Teststatistik: \\[ \\chi^2_{emp} = \\Sigma{\\frac{(Freq_{beobachtet} - Freq_{erwartet})^2}{Freq_{erwartet}}} \\] Ergebnis (im obigen Beispiel): Die Nachfrage nach den verschiedenfarbigen Uhren ist NICHT gleichmäßig verteilt: \\(\\chi^2_{empirisch} &gt; \\chi^2_{erwartet}\\) bei 3 Freiheitsgraden und 5%-iger Irrtumswahrscheinlichkeit. Wir lehnen die Nullhypothese damit ab und akzeptieren die alternative Hypothese. 5.1.1 Lange und kurze Kommentare Die Verwendung des \\(@chi³2\\)-Tests im sprachlichen Bereich wollen wir zunächst am Beispiel eines erfundenen Datensatzes kennen lernen. neugeschriebener_satz kurzer_kommentar_a ausf_a_lhrlicher_kommentar inkorrekt 13 29 korrekt 67 55 Im Datensatz wird zwischen langen und kurzen Kommentaren einer Lehrerin unterschieden und die jeweilige Anzahl sprachlicher Fehler von Schülern in ihren Aufsätzen. Geklärt werden soll die Frage, welche Wirkung lange und kurze Kommentare der Lehrerin auf die Anzahl der sprachlichen Fehler hatten. 5.1.1.1 Programme library(tidyverse) library(janitor) library(scales) library(rmarkdown) library(kableExtra) 5.1.1.2 Kurzversion: Wie sinnvoll sind lange bzw. kurze Kommentare einer Lehrerin zu sprachlichen Fehlern in Essays? library(tidyverse) library(janitor) # Datei laden und die Variablennamen vereinheitlichen kommentare = read.delim(&quot;data/chisq_kommentare.txt&quot;, sep = &quot;\\t&quot;) %&gt;% clean_names() head(kommentare) ## neugeschriebener_satz kurzer_kommentar_a ausf_a_lhrlicher_kommentar ## 1 inkorrekt 13 29 ## 2 korrekt 67 55 library(janitor) # Chi-Quadrat-Test chisq.test(kommentare[,-1]) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: kommentare[, -1] ## X-squared = 6.2551, df = 1, p-value = 0.01238 Ergebnis: Wir verwerfen die Hypothese H0 und nehmen die Hypothese H1 an: zwischen kurzen und langen Kommentaren besteht ein nicht zufälliger Unterschied. 5.1.1.3 Längere Version 5.1.1.3.1 Datei laden Eine Lehrerin möchte wissen, ob es effektiver ist, wenn sie am Rand der Schüleressays kurze oder ausführlichere Kommentare zu den Fehlern der Schüler_innen notiert. Sie vergleicht somit zwei Schülergruppen (Schüler_innen mit kurzen vs. langen Kommentaren) und zwei Beurteilungskategorien (korrekte vs. inkorrekte Äußerungen in den Essays). library(tidyverse) # von github laden kommentare = read.delim( &quot;https://raw.githubusercontent.com/tpetric7/tpetric7.github.io/main/data/chisq_kommentare.txt&quot;, sep = &quot;\\t&quot;, fileEncoding = &quot;UTF-8&quot;) library(janitor) # Variablennamen konsequent schreiben kommentare = kommentare %&gt;% clean_names() # Von der Festplatte laden kommentare = read.delim(&quot;data/chisq_kommentare.txt&quot;, sep = &quot;\\t&quot;, fileEncoding = &quot;UTF-8&quot;) %&gt;% clean_names() head(kommentare) %&gt;% knitr::kable() neugeschriebener_satz kurzer_kommentar ausfuhrlicher_kommentar inkorrekt 13 29 korrekt 67 55 5.1.1.3.2 Chi-Quadrat-Test Stichproben: kurzer Kommentar vs. langer Kommentar H0: Zwischen den beiden Stichproben besteht kein signifikanter Unterschied (Unterschiede zufällig). H1: Zwischen den beiden Stichproben besteht ein signifikanter Unterschied (Unterschiede nicht zufällig). library(janitor) chisq.test(kommentare[,-1]) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: kommentare[, -1] ## X-squared = 6.2551, df = 1, p-value = 0.01238 Wir verwerfen H0 und nehmen H1 an: zwischen kurzen und langen Kommentaren besteht ein nicht zufälliger Unterschied. 5.1.1.3.3 Graphische Darstellung library(tidyverse) library(scales) kom_lang = kommentare %&gt;% as_tibble() %&gt;% pivot_longer(kurzer_kommentar:ausfuhrlicher_kommentar, names_to = &quot;Kommentar&quot;, values_to = &quot;Fehler&quot;) %&gt;% mutate(pct = Fehler/sum(Fehler)) kom_lang %&gt;% knitr::kable() neugeschriebener_satz Kommentar Fehler pct inkorrekt kurzer_kommentar 13 0.0792683 inkorrekt ausfuhrlicher_kommentar 29 0.1768293 korrekt kurzer_kommentar 67 0.4085366 korrekt ausfuhrlicher_kommentar 55 0.3353659 kom_lang %&gt;% ggplot(aes(Kommentar, pct, fill = neugeschriebener_satz)) + geom_col(position = &quot;dodge&quot;) + scale_y_continuous(labels = percent_format()) + labs(x = &quot;Neugeschriebener Satz&quot;, y = &quot;&quot;, title = &quot;Wirksamkeit kurzer und langer Kommentare&quot;) 5.1.2 Plural von Kunstwörtern 5.1.2.1 Programme laden library(tidyverse) library(scales) library(kableExtra) 5.1.2.2 Dateien laden Für die Durchführung eines \\(\\chi^2\\)-Tests solle eine Tabelle geladen werden, die Ergebnisse eines Experiments mit deutschen Kunstwörtern enthält, von denen slowenische Studierende der Germanistik den Plural bilden sollten. # Branje datoteke je mono na ve nainov plural_subj1 = read.csv(&quot;data/plural_Subj_sum.csv&quot;, sep = &quot;;&quot;) plural_subj1 = read.csv2(&quot;data/plural_Subj_sum.csv&quot;) plural_subj1 = read_csv2(&quot;data/plural_Subj_sum.csv&quot;) # Pokai prvih est vrstic head(plural_subj1) %&gt;% knitr::kable() SubjID WordType Genus Sigstark En E Er S Z 1 NoRhyme Fem 4.983333 8 4 0 0 0 1 NoRhyme Masc 4.600000 6 6 0 0 0 1 NoRhyme Neut 5.366667 10 2 0 0 0 1 Rhyme Fem 3.836667 3 8 0 0 1 1 Rhyme Masc 4.153333 5 5 1 0 1 1 Rhyme Neut 3.784167 3 7 1 0 1 5.1.2.3 Datensatz-Aggregation und Test Zuerst müssen wir die Rohdaten in eine Tabelle umformen, so dass ein \\(\\chi^2\\)-Test durchgeführt werden kann. Eine derartige Transformation eines Datensatzes wird oft als Aggregation bezeichnet (also eine Art von Zusammenfassung). In der neu gebildeten 2x2-Tabelle sind die Beobachtungsdaten (d.h. die Häufigkeiten oder Frequenzen) zu finden. Das Programm berechnet für uns die erwarteten Häufigkeiten (theoretischen Frequenzen) und bewertet dann, ob die Differenz zwischen den Stichproben statistisch signifikant ist. Die statistischen Annahmen können folgendermaßen formuliert werden: - \\(H_0\\): Die Versuchspersonen verwenden sowohl für Reimwörter als auch für Nicht-Reimwörter dieselben deutschen Pluralmarker. Der Worttyp hat demnach keinen Einfluss auf die Auswahl des Pluralmarkers. - \\(H_1\\): Die Versuchspersonen verwenden für Reimwörter nicht dieselben deutschen Pluralmarker wie für Nicht-Reimwörterverschieden für die beiden Worttypen (Reimwort vs. Nicht-Reimwort). Der Worttyp hat demnach Einfluss auf die Auswahl des Pluralmarkers. Wenn der beim statistischen Test erhaltene p-Wert &lt; 0,05 ist (d.h. bei einer Fehlerwahrscheinlichkeit von weniger als 5%), dann gilt die alternative Hypothese \\(H_1\\): die Differenz zwischen den beobachteten und den theoretisch erwarteten Häufigkeiten ist in diesem Fall statistisch signifikant, d.h. die Differenz ist nicht zufällig und bei 5% Fehlerwahrscheinlichkeit hinreichend groß. Wenn der p-Wert jedoch p &gt; 0,05 ist, dann wird die Nullhypothese \\(H_0\\) beibehalten. In diesem Fall wäre die Differenz nicht hinreichend groß und daher vermutlich zufällig entstanden (z.B. durch die geringe Größe der Stichproben oder die Auswahl der Stichprobendaten). Im ersten statischen Test vergleichen wir die Häufigkeiten der Pluralmarker e und s miteinander. # Povzemamo (&quot;aggregate&quot;) # Ergebnisse summieren p = plural_subj1 %&gt;% group_by(WordType) %&gt;% summarise(Sigstark = mean(Sigstark), En = sum(En), E = sum(E), Er = sum(Er), S = sum(S), Z = sum(Z)) # izpis tabele knitr::kable(p) WordType Sigstark En E Er S Z NoRhyme 4.087337 1528 2169 302 307 26 Rhyme 3.916109 1425 2172 561 244 14 # Izberemo tri stolpce q = p %&gt;% select(WordType, E, S) # Razlika med delei mnoinskih pripon E in S (npr. Bal-e oder Bal-s) chisq.test(q[,-1]) # prvi stolpec naj se ne upoteva, zato [, -1] ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: q[, -1] ## X-squared = 6.2424, df = 1, p-value = 0.01247 5.1.2.4 Naslednji preizkus(i) Wir machen noch einen \\(\\chi^2\\)-Test mit einer 2x2-Tabelle, und zwar mit den Pluralmarkern e und er durchgeführt. # Izberemo tri stolpce za naslednji preizkus q = p %&gt;% select(WordType, E, Er) # Razlika med delei mnoinskih pripon E in Er (npr. Bal-e oder Bal-er) chisq.test(q[,-1]) # prvi stolpec naj se ne upoteva, zato [, -1] ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: q[, -1] ## X-squared = 64.106, df = 1, p-value = 1.179e-15 5.1.2.5 Tabela 2 x 3 Der \\(\\chi^2\\)-Test kann auch mit größeren Tabellen durchgeführt werden, z.B. mit einer 2x3-Tabelle. Dies ermöglicht den Vergleich von mehr als zwei Stichproben. Der \\(\\chi^2\\)-Test kann statistisch signifikante Unterschiede zwischen Stichproben melden, kann aber leider nicht darüber Auskunft oben, welche Stichprobe sich von den übrigen unterscheidet. # Izberemo tri stolpce za naslednji preizkus q = p %&gt;% select(WordType, Er, E, S) # Razlika med delei mnoinskih pripon E in Er (npr. Bal-e oder Bal-er) chisq.test(q[,-1]) # prvi stolpec naj se ne upoteva, zato [, -1] ## ## Pearson&#39;s Chi-squared test ## ## data: q[, -1] ## X-squared = 78.148, df = 2, p-value &lt; 2.2e-16 5.1.2.6 Zweite Version Es gibt verschiedene Wege, um die Rohdaten in eine Tabelle umzuformen, die für die Durchführung eines \\(\\chi^2\\)-Tests geeignet ist. Hier folgt eine weitere Aggregationsvariante mit Hilfe von tidyverse-Funktionen. Zuerst gruppieren wir die Rohdaten nach der Spalte, in der die Versuchspersonen eingetragen sind. Dann lassen wir die Summe der ausgewählten Pluralmarker berechnen: (p = plural_subj1 %&gt;% group_by(WordType) %&gt;% summarise(En = sum(En), E = sum (E)) ) ## # A tibble: 2 x 3 ## WordType En E ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 NoRhyme 1528 2169 ## 2 Rhyme 1425 2172 Falls p &lt; 0,05 ist, gilt \\(H_1\\) (die Stichproben unterscheiden sich). Falls p &gt; 0,05 ist, gilt \\(H_0\\) (kein signifikanter Unterschied zwischen Stichproben). (chi = chisq.test(p[,-1]) ) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: p[, -1] ## X-squared = 2.1535, df = 1, p-value = 0.1422 Zum Schluss werfen wir noch einen Blick auf beobachtete und erwartete Häufigkeiten: tabelle &lt;- as_tibble(cbind(chi$observed, chi$expected)) %&gt;% # Spalte wieder hinzufügen mutate(Wordtyp = unlist(p[,1])) %&gt;% # auf deutsch mutate(Wordtyp = str_replace(Wordtyp, &quot;NoRhyme&quot;, &quot;Nicht-Reimwort&quot;), Wordtyp = str_replace(Wordtyp, &quot;Rhyme&quot;, &quot;Reimwort&quot;)) %&gt;% # erwartete Werte, wenn H0 richtig ist rename(En_erwartet = V3, E_erwartet = V4) %&gt;% # Reihenfolge der Variablen verändern select(Wordtyp, En, E, En_erwartet, E_erwartet) tabelle %&gt;% rmarkdown::paged_table() 5.1.3 Modalkonstruktionen In diesem Abschnitt wird die Vorkommenshäufigkeit (token frequency) der slowenischen Modalkonstruktionen morati + Infinitiv und biti + treba + Infinitiv in einer Auswahl von slowenischen Texten miteinander verglichen. Der statistische Vergleich wird mit dem \\(\\chi^2\\)-Test durchgeführt. 5.1.3.1 Packages library(tidyverse) library(scales) library(janitor) library(readxl) 5.1.3.2 Datei laden Bei Recherchen auf dem slowenischen Gigafida-Portal wurden Gebrauchsfrequenzen (Tokenfrequenzen) von zwei Modalkonstruktionen ermittelt, und zwar: - morati + Infinitiv und - biti + treba + Infinitive. Die erste Tabelle mit den Gebrauchsfrequenzen laden wir von der Festplatte: naklonska &lt;- read_xlsx(&quot;data/morati_treba.xlsx&quot;) %&gt;% clean_names() naklonska ## # A tibble: 2 x 3 ## vrsta_besedila treba morati ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 asniki 550572 1501540 ## 2 Drugo 169349 530345 Die zweite Tabelle zeigt die Distribution der beiden Modalkonstruktionen in fünf Funktionalstilen. naklonska2 &lt;- read_xlsx(&quot;data/morati_treba.xlsx&quot;, sheet = &quot;List2&quot;) %&gt;% clean_names() naklonska2 ## # A tibble: 5 x 3 ## vrsta_besedila treba morati ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 asopisi 389479 1086280 ## 2 Revije 161093 415260 ## 3 Internet 124996 376433 ## 4 Stvarna besedila 30998 98981 ## 5 Leposlovje 13355 54931 Die Modalkonstruktion morati + Infinitiv wird ca. dreimal so häufig verwendet wie biti + treba + Infinitiv. 5.1.3.3 Graphische Darstellung Die graphischen Darstellungen zeigen eher geringe Distributionsunterschiede. naklonska %&gt;% pivot_longer(treba:morati, names_to = &quot;konstruktion&quot;, values_to = &quot;freq&quot;) %&gt;% ggplot(aes(konstruktion, freq, fill = vrsta_besedila)) + geom_col(position = &quot;fill&quot;) + scale_y_continuous(labels = percent_format()) + labs(x = &quot;Modalkonstruktion&quot;, y = &quot;Gebrauchsfrequenz&quot;, fill = &quot;Vrsta besedila&quot;) Die Modalkonstruktion morati + Infinitiv scheint in den alltagssprachlich näherstehenden Funktionalstilen Belletristik (leposlovje), Internet und Sachtexten (stvarna besedila) etwas häufiger belegt zu sein als die Modalkonstruktion biti + treba + Infinitiv, dafür aber in Zeitungen (asopisi) etwas seltener. naklonska2 %&gt;% pivot_longer(treba:morati, names_to = &quot;konstruktion&quot;, values_to = &quot;freq&quot;) %&gt;% ggplot(aes(konstruktion, freq, fill = vrsta_besedila)) + geom_col(position = &quot;fill&quot;) + scale_y_continuous(labels = percent_format()) + labs(x = &quot;Modalkonstruktion&quot;, y = &quot;Gebrauchsfrequenz&quot;, fill = &quot;Vrsta besedila&quot;) 5.1.3.4 Chi-Quadrat-Test Linguistische Annahme: Die Modalkonstruktion morati + Infinitiv ist weniger markiert (natürlicher) als die Modalkonstruktion biti + treba + Infinitiv. Formale Begründung: Die erste Konstruktion ist kürzer und daher ökonomischer als die zweite. Semantische Begründung: Die erste Konstruktion ist semantisch weniger spezifisch als die zweite. Dies sollte dazu führen, dass die erste Konstruktion in einer größeren Anzahl von Kontexten erscheint als die zweite. Die statistischen Annahmen lassen sich folgendermaßen formulieren: \\(H_0\\): Die beiden Modalkonstruktionen kommen in denselben Funktionalstilen vor. \\(H_1\\): Die beiden Modalkonstruktionen kommen nicht in denselben Funktionalstilen vor. Der erste \\(\\chi^2\\)-Test zeigt, dass die beiden Stichproben (morati vs. treba) unabhängig voneinander sind. Dies bestätigt der geringe p-Wert (p &lt; 0,001), der unterhalb dem Grenzwert von p = 0,05 (5%) liegt. Damit können wir die Nullhypothese (\\(H_0\\)) verwerfen und die alternative Hypothese (\\(H_1\\)) akzeptieren. Die beiden Modalkonstruktionen kommen demnach nicht im gleichen Maße in denselben Funktionalstilen vor. chisq.test(naklonska[ , -1]) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: naklonska[, -1] ## X-squared = 1862.9, df = 1, p-value &lt; 2.2e-16 Der zweite \\(@chi³2\\)-Test, der mit den Zahlenwerten der zweiten Tabelle durchgeführt wird, bestätigt Hypothese \\(H_1\\). Die Distribution der beiden Modalkonstruktionen unterscheidet sich. Die graphische Darstellung deutet an, dass dies vor allem am vergleichsweise selteneren Gebrauch der (natürlicheren) Modalkonstruktion morati + Infinitiv in publizistischen Texten liegen könnte. Nach unser Annahme wird die (weniger natürliche) Modalkonstruktion biti + treba + Infinitiv häufiger in weniger natürlichen Textsorten mit dem Merkmal [+Distanz] eingesetzt. chisq.test(naklonska2[ , -1]) ## ## Pearson&#39;s Chi-squared test ## ## data: naklonska2[, -1] ## X-squared = 3292, df = 4, p-value &lt; 2.2e-16 5.2 Intervallskalierte Größen Statistische Tests: t-Test, lineare Regression, lineare Regression mit gemischten Effekten. 5.2.1 Äußerungslänge in einer Kurzgeschichte Im Wikipedia-Artikel zum Thema Satzlänge (nach unserer Terminologie: Äußerungslänge) wird angegeben, dass die durchschnittliche Satzlänge in Prosatexten für literarische Prosa im 20. Jahrhundert gemäß Best(2002) zwischen 7,08 und 19,62 Wörtern liegt. 5.2.1.1 Programme und Tabelle laden library(tidyverse) library(scales) library(janitor) library(readxl) wiki_utter_length &lt;- read_xlsx(&quot;data/wikipedia_satzlaenge_durchschnitt.xlsx&quot;) wiki_utter_length %&gt;% rmarkdown::paged_table() Roman-Dialoge weisen laut Pieper(1979) einen Medianwert von 6,01 Wörtern auf, Roman-Nichtdialoge dagegen einen Medianwert von 12,98 Wörtern. Der Median ist ein Mittelwert, der genau in der Mitte aller Werte liegt: 50% aller Werte liegen unterhalb des Medians, 50% oberhalb davon. wiki_utter_length_median &lt;- read_xlsx(&quot;data/wikipedia_satzlaenge_median.xlsx&quot;) wiki_utter_length_median %&gt;% rmarkdown::paged_table() 5.2.1.2 Hypothesen für den t-Test Aufgrund der in den beiden Tabellen vorgestellten Mittelwerte gehen wir im Fall von Borcherts Kurzgeschichte Die Küchenuhr von der folgende Nullhypothese (\\(H_0\\)) aus: Die durchschnittliche Äußerungslänge beträgt so wie in literarischer Prosa etwa \\(\\mu\\) = 12 Wörter pro Äußerung. Die Kurzgeschichte ist ein Beispiel für literarische Prosa, in der auch Dialoge vorkommen. Die Alternativhypothese (\\(H_1\\)) besagt dagegen, dass das arithmetische Mittel (der Durchschnittswert) in Borcherts Kurzgeschichte von dem erwarteten Durchschnitt für literarische Prosa (mit Dialog- und Nicht-Dialog-Passagen) \\(\\mu\\) abweicht. Der t-Test wird auf folgende Weise berechnet: \\[ t = \\frac{m - \\mu}{s/{\\sqrt{n}}} \\] - m ist der arithmetische Mittelwert der Stichprobe; - \\(\\mu\\) ist der arithmetische Mittelwert für literarische Prosa; - n ist die Stichprobengröße (d.h. die Anzahl der Äußerungen); - s ist die Standardabweichung mit \\(n - 1\\) Freiheitsgraden. Da wir die Standardabweichung in der Grundgesamtheit \\(\\sigma\\) nicht kennen, sind wir bei diesem t-Test mit einer Stichprobe auf die Standardabweichung der Stichprobe \\(s\\) (d.h. einer Zufallsvariable) angewiesen. Der t-Wert und der p-Wert werden bei df = \\(n - 1\\) Freiheitsgraden berechnet. Falls der p-Wert &lt; 0,05, verwerfen wir die Nullhypothese, liegt er dagegen oberhalb dieses Signifikanzniveaus, behalten wir die Nullhypothese bei. 5.2.1.3 Textvorbereitung Zuerst muss der Text geöffnet und dann in Äußerungen zerlegt werden. Dann kann quanteda die Anzahl der Tokens (d.h. Wörter + Interpunktionszeichen) zählen. Die Interpunktionszeichen werden hier der Einfachheit halber nicht herausgefiltert, so dass die ermittelten Zahlen etwas höher ausfallen. Für die Durchführung eines t-Test erstellen wir einen Datensatz, in dem die Wortanzahl (Tokenanzahl) für jede Äußerung auftritt. library(quanteda) library(quanteda.textstats) # open the text file borchert_kuechenuhr &lt;- read_lines( &quot;data/borchert/borchert_kuechenuhr.txt&quot;) # create corpus borchert_corp_basic &lt;- corpus (borchert_kuechenuhr) # corpus reshaped by utterances borchert_corp_utter &lt;- corpus_reshape(borchert_corp_basic, to = &quot;sentences&quot;) # create a dataframe borchert_df &lt;- as.character(borchert_corp_utter) %&gt;% as.data.frame() %&gt;% rename(text = &quot;.&quot;) %&gt;% rownames_to_column(., var = &quot;doc_id&quot;) # mandatory for t-test: count tokens per utterance borchert_textstats &lt;- summary(borchert_corp_utter, n = 130) %&gt;% rename(doc_id = Text) # join both datasets borchert_df_all &lt;- left_join(borchert_df, borchert_textstats, by = &quot;doc_id&quot;) %&gt;% filter(Tokens &gt; 0) # remove title and author from dataframe # 109 sentences remain borchert_df_all &lt;- borchert_df_all %&gt;% filter(doc_id != &quot;text1.1&quot;) 5.2.1.4 Durchführung des t-Tests Nun führen wir den Ein-Stichproben t-Test durch. Dazu genügt der Datensatz borchert_textstats. # install.packages(&quot;ggpubr&quot;) library(ggpubr) # remove 0 values and title borchert_textstats &lt;- borchert_textstats %&gt;% filter(Tokens &gt; 0, doc_id != &quot;text1.1&quot;) # test t.test(borchert_textstats$Tokens, mu = 12, alternative = &quot;two.sided&quot;) ## ## One Sample t-test ## ## data: borchert_textstats$Tokens ## t = -3.5698, df = 108, p-value = 0.0005346 ## alternative hypothesis: true mean is not equal to 12 ## 95 percent confidence interval: ## 9.288998 11.224764 ## sample estimates: ## mean of x ## 10.25688 Das Ergebnis: Die durchschnittliche Äußerungslänge in Borcherts Kurzgeschichte (mean = 9,25 Wörter pro Äußerung) unterscheidet sich signifikant vom erwarteten arithmetischen Mittelwert für literarische Prosa \\(\\mu\\) = 12 Wörter. Der folgende t-Test wird mit dem vollständigen Datensatz borchert_df_all durchgeführt, aber dieses Mal mit der Nullhypothese, dass der Mittelwert m der Stichprobe gleich dem Mittelwert \\(\\mu\\) = 7,08 beträgt (d.h. der Untergrenze der Äußerungslänge literarischer Prosa). # install.packages(&quot;ggpubr&quot;) library(ggpubr) t.test(borchert_df_all$Tokens, mu = 7.08, alternative = &quot;two.sided&quot;) ## ## One Sample t-test ## ## data: borchert_df_all$Tokens ## t = 6.5061, df = 108, p-value = 2.48e-09 ## alternative hypothesis: true mean is not equal to 7.08 ## 95 percent confidence interval: ## 9.288998 11.224764 ## sample estimates: ## mean of x ## 10.25688 Die durchschnittliche Äußerungslänge in Borcherts Kurzgeschichte unterscheidet sich demnach mit statistischer Signifikanz sowohl vom angenommenen Mittelwert \\(\\mu\\) = 12 Wörter für literarische Prosa als auch von der Untergrenze \\(\\mu\\) = 7,08 Wörter. Der schwarze Balken im Boxplot zeigt den Medianwert unserer Stichprobe (median = 9 Wlrter bzw. genauer: Tokens), der nur knapp unter dem arithmetischen Mittelwert liegt (mean = 9,29 Tokens). Die gestrichelten blauen Linien kennzeichnen die Ober- und Untergrenze der durchschnittlichen Äußerungslängen für literarische Prosa (s. Wikipedia-Tabelle oben). Genau 50% aller Äußerungslängen aus Borcherts Kurzgeschichte liegen im blauen Kasten. borchert_df_all %&gt;% ggplot(aes(y = Tokens)) + geom_boxplot(fill = &quot;cyan&quot;) + geom_hline(yintercept = 19.62, color = &quot;blue&quot;, lty = 3, size = 2) + geom_hline(yintercept = 7.08, color = &quot;blue&quot;, linetype = &quot;dotted&quot;, size = 2) 5.2.2 Wirkung von Unterrichtsmethoden Welche Wirkung haben zwei verschiedene Unterrichtsmethoden auf die Ergebnisse von Sprachtests? Welche Gruppe von Studierenden erreichte eine höhere Punktzahl beim Test? Diese Frage soll mit Hilfe eines t-Tests für zwei unabhängige Stichproben geprüft werden. 5.2.2.1 Data # Two teaching methods and the scores in a language test. metode &lt;- read.csv(&quot;data/ttest2a.csv&quot;, dec=&quot;,&quot;) attach(metode) head(metode) ## Testpersonen Resultat Methode ## 1 1 23 A ## 2 2 34 A ## 3 3 54 A ## 4 4 33 A ## 5 5 26 A ## 6 6 27 A 5.2.2.2 Deskriptive Statistik Arithmetische Mittelwerte beider Studentengruppen (Average scores of students): tapply(Resultat, list(Methode), mean) ## A B ## 32.65 31.55 Standardabweichungen in beiden Studentengruppen (Standard deviations of averages): tapply(Resultat, list(Methode), sd) ## A B ## 9.906271 7.897201 Graphische Darstellung barplot(tapply(Resultat, list(Methode), mean), col=c(3:2)) Flexiblere Gestaltung mit dem Programm ggplot2. metode %&gt;% ggplot(aes(Methode, Resultat, fill = Methode)) + geom_boxplot() + geom_jitter(width = 0.1) + theme(legend.position = &quot;none&quot;) 5.2.2.3 Zwei-Stichproben t- Test In diesem t-Test werden zwei die arithmetischen Mittelwerte von zwei Stichproben verglichen, die unabhängig voneinander sind. Eine Studentengruppe hatten Unterricht gemäß Methode A, die andere gemäß Methode B. Ist das durchschnittliche Ergebnis beider Gruppen gleich oder unterschiedlich? Nullhypothese \\(H_0\\): Die Ergebnisse beider Methoden unterscheiden sich nicht signifikant. Alternativhypothese \\(H_1\\): Die Ergebnisse beider Mehtoden unterscheiden sich signifikant. # Do the means of the two samples differ significantly? # Hypothesis H0: they don&#39;t (if p &gt; 0.05. # Hypothesis H1: they do (if p &lt; 0.05. t.test(Resultat ~ Methode, data=metode, paired = F, var.equal = T) ## ## Two Sample t-test ## ## data: Resultat by Methode ## t = 0.3883, df = 38, p-value = 0.7 ## alternative hypothesis: true difference in means between group A and group B is not equal to 0 ## 95 percent confidence interval: ## -4.634791 6.834791 ## sample estimates: ## mean in group A mean in group B ## 32.65 31.55 Ergebnis des t-Tests: In unserem erfundenen Datensatz wird die Nullhypothese angenommen. Zwischen den Ergebnissen nach Methode A und B gab es keinen signifikanten Unterschied. Der p-Wert lag mit p = 0,7 oberhalb dem Signifikanzniveau von p = 0,05. Der t-test erfordert normalverteilte Daten. Ob Normalverteilung vorliegt, kann man - mit dem shapiro-Test oder - (meist zuverlässiger) mit Hilfe eines Histrogramms überprüfen. Die arithmetischen Mittelwert der Gruppe A sind gemäß dem Shapiro-Wilks-Test normalverteilt, denn p &gt; 0,05. Bei einem p-Wert von weniger als 0,05 müssten wir die Nullhypothese, dass die Variable normalverteilt ist, verwerfen. metode %&gt;% filter(Methode == &quot;A&quot;) %&gt;% select(Resultat) %&gt;% pull() %&gt;% shapiro.test() ## ## Shapiro-Wilk normality test ## ## data: . ## W = 0.98368, p-value = 0.9723 Die arithmetischen Mittelwert der Gruppe B sind gemäß dem Shapiro-Wilks-Test ebenfalls normalverteilt, denn p &gt; 0,05. metode %&gt;% filter(Methode == &quot;B&quot;) %&gt;% select(Resultat) %&gt;% pull() %&gt;% shapiro.test() ## ## Shapiro-Wilk normality test ## ## data: . ## W = 0.96007, p-value = 0.5452 Die beiden Dichte-Diagramme (oder Histogramme) bestätigen den Befund des Shapiro-Wilks-Tests, dass die Variable Resultat in beiden Gruppen (A und B) in etwa normalverteilt sind. Das ist eine der Voraussetzungen für die Durchführung des t-Tests. metode %&gt;% filter(Methode == &quot;A&quot;) %&gt;% ggplot(aes(Resultat)) + geom_density() + geom_vline(xintercept = 31) # median metode %&gt;% filter(Methode == &quot;B&quot;) %&gt;% ggplot(aes(Resultat)) + geom_density() + geom_vline(xintercept = 31) # median Stellt man nun fest, dass die geprüfte Variable nicht normalverteilt ist, kann man auf nicht-parametrische Tests zurückgreifen, z.B. den Mann-Whitney-Wilcoxon-Test (auch: Mann-Whitney U-Test, Wilcoxon Rangsummentest) für zwei abhängige Stichproben (paired = TRUE) bzw. für zwei unabhängige Stichproben (paired = FALSE. 5.2.2.4 Vergleich von Medianwerten Für den Vergleich von Medianwerten von nicht-normal verteilten Daten kann man den nicht-parametrischen wilcox.test() Nonparametric statistics verwenden, bei dem die Rangzahlen von zwei Stichproben addiert und verglichen werden (in unserem Beispiel: Resultate der Methode A und B). In Fall unserer erfundenen Stichproben (zwei Studentengruppen, die sich durch die Unterrichtsmethode unterscheiden), ist der p-Wert p = 1, also oberhalb des Signifikanzniveaus von p = 0,05. Die Alternativhypothese konnte nicht bestätigt werden. Wir akzeptieren die Nullhypothese, dass zwischen den Ergebnissen der beiden Unterrichtsmethoden kein signifikanter Unterschied vorliegt. # create two separated datasets metodeA &lt;- subset(metode, Methode = &quot;A&quot;) metodeB &lt;- subset(metode, Methode = &quot;B&quot;) # both median values are equal median(metodeA$Resultat) ## [1] 31 median(metodeB$Resultat) ## [1] 31 # wilcox.test wtest &lt;- wilcox.test(metodeA$Resultat, metodeB$Resultat, paired=FALSE) wtest ## ## Wilcoxon rank sum test with continuity correction ## ## data: metodeA$Resultat and metodeB$Resultat ## W = 800, p-value = 1 ## alternative hypothesis: true location shift is not equal to 0 Mit der summary()-Funktion können wir den Medianwert, den Minimal- und Maximalwert sowie den interquartilen Bereich der Variable unseres erfundenen Datensatzes ausgeben lassen. summary(metodeA$Resultat) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 16.00 26.00 31.00 32.10 37.25 54.00 summary(metodeB$Resultat) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 16.00 26.00 31.00 32.10 37.25 54.00 5.2.2.5 Effektstärke 5.2.2.5.1 Nicht-parametrisch Der Wilcoxon-Rangsummentest (Mann-Whitney U-Test) hat zwar keinen signifikanten Unterschied angezeigt, aber man Cohens Effektstärke (Cohen 1992) nach einem nicht-parametrischen Mann-Whitney-Wilcoxon Test folgendermaßen berechnen lassen: \\[ r = \\frac{z}{\\sqrt{n}} \\] Die Variable r ist Pearsons Korrelationskoeffizient (Pearson product-moment correlation), der die Assoziationsstärke (d.h. die Stärke des linearen Zusammenhangs) zwischen zwei Variablen angibt. Ein Z-Score z beschreibt das Verhältnis eines Wertes zum Mittelwert einer Gruppe von Werten. Der Z-Score wird in Form von Standardabweichungen vom Mittelwert gemessen. Die Variable n steht für die Stichprobengröße. wil &lt;- wilcox.test(Resultat ~ Methode, paired = FALSE, exact = FALSE, data = metode) z &lt;- qnorm(wil$p.value) r &lt;- z/sqrt(length(metode$Resultat)) r ## [1] 0.1434898 Die Effektstärke für den Korrelationswert r = 0,14 (maximal r = 1) wird gemäß (Cohen1992power?) als schwache Effektstärke eingeschätzt (ab r = 0,3 moderate Effektstärke, ab r = 0,5 große Effektstärke). Der Unterschied zwischen den beiden Gruppen ist demnach nicht unbedingt zu vernachlässigen, da der (oben berechnete) nicht-signifikante p-Wert möglicherweise wegen einer zu kleinen Stichprobengröße zustande gekommen ist. 5.2.2.5.2 Parametrisch Die Effektstärke d kann man aus dem Unterschied zwischen den Mittelwerten, dividiert durch die gepaarte Standardabweichung sd (pooled standard deviation, berechnen: - d ist Cohens Effektstärke (effect size); - \\(means_1\\), \\(means_2\\) sind die beiden Mittelwerte, - \\(s_1\\), \\(s_2\\) die Standardabweichungen; - \\(n_1\\), \\(n_2\\) die Größen der beiden Stichproben. \\[ d = \\frac{mean_1 - mean_2}{\\sqrt{(n_1-1)s_1^2 + (n_2-1)s_2^2}/{(n_1+n_2-2)}} \\] Am schnellsten berechnet man die Effektstärke d wohl mit dem Programm effectsize. library(effectsize) coh &lt;- cohens_d(Resultat ~ Methode, data = metode) coh ## Cohen&#39;s d | 95% CI ## ------------------------- ## 0.12 | [-0.50, 0.74] ## ## - Estimated using pooled SD. Die geschätzte Effekstärke (d.h. der geschätzte Unterschied zwischen den beiden Gruppen in unserer Stichprobe) beträgt etwa d = 0,12 (ab d = 0,2 gilt meist: schwacher Effekt). Cohens d sagt uns, wie viele Standardabweichungen zwischen den beiden Mittelwerten liegen. Manuelle Berechnung: test &lt;- t.test(Resultat ~ Methode, var.equal = T, data = metode) # means mean1 = as.numeric(test$estimate[1]) mean2 = as.numeric(test$estimate[2]) #find sample standard deviation of each sample s1 &lt;- sd(metode$Resultat[metode$Method == &quot;A&quot;]) s2 &lt;- sd(metode$Resultat[metode$Method == &quot;B&quot;]) #find sample size of each sample n1 &lt;- length(metode$Resultat[metode$Method == &quot;A&quot;]) n2 &lt;- length(metode$Resultat[metode$Method == &quot;B&quot;]) #calculate pooled standard deviation pooled_sd &lt;- sqrt(((n1-1)*s1^2 + (n2-1)*s2^2) / (n1+n1-2)) d = (mean1 - mean2)/pooled_sd d ## [1] 0.122792 Teilweise manuelle Berechung, aber Berechnung der gepaarten Standardabweichung mit Hilfe des Programms effectsize: library(effectsize) pooled_sd &lt;- sd_pooled(metode$Resultat[metode$Methode == &quot;A&quot;], metode$Resultat[metode$Methode == &quot;B&quot;]) mean1 = mean(metode$Resultat[metode$Methode == &quot;A&quot;]) mean2 = mean(metode$Resultat[metode$Methode == &quot;B&quot;]) d = (mean1 - mean2)/pooled_sd d ## [1] 0.122792 5.2.2.6 Lineare Regression Statt des t-Tests kann man bei Mittelwert-Vergleichen auch eine lineare Regression durchführen. Da wir es in diesem Fall mit nur einem Prädiktor (Methode) zu tun haben, sind die Ergebnisse der linearen Regression (etwa der p-Wert) gleich denen, die uns der t-Test gebracht hat. Darüber hinaus erhalten wir noch andere Informationen und erweitern Vergleichsmöglichkeiten. # Check the same hypotheses with the linear regression method # Since there is only one predictor (&quot;Methode&quot;), we obtain the same result as with the t-test. # Since p &gt; 0.05, the score means of the two methods do not differ significantly. m &lt;- lm(Resultat ~ Methode, data=metode) summary(m) ## ## Call: ## lm(formula = Resultat ~ Methode, data = metode) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.65 -6.65 -0.55 5.45 21.35 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 32.650 2.003 16.300 &lt;2e-16 *** ## MethodeB -1.100 2.833 -0.388 0.7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.958 on 38 degrees of freedom ## Multiple R-squared: 0.003952, Adjusted R-squared: -0.02226 ## F-statistic: 0.1508 on 1 and 38 DF, p-value: 0.7 library(effects) Predicted scores allEffects(m) ## model: Resultat ~ Methode ## ## Methode effect ## Methode ## A B ## 32.65 31.55 Das Diagramm zeigt deutlich, dass kein signifikanter Unterschied zwischen den Mittelwerten der Methoden A und B vorliegt: Die Konfidenzintervalle für beide arithmetische Mittelwerte überschneidet sich fast völlig und schließen damit den jeweiligen Mittelwert der anderen Methode ein. plot(allEffects(m), multiline=TRUE, grid=TRUE, rug=FALSE, as.table=TRUE) Zum Abschluss dieses Kapitels: Zur unkomplizierten Visualisierung von Datensatzvariablen, ohne programmieren zu müssen, eignet sich die library(esquisse). Das Programm ermöglicht die Auswahl von Variablen mit der Maus. 5.2.3 Höflichkeit und Grundfrequenz library(tidyverse) library(scales) # detach(&quot;package:rlang&quot;, unload=TRUE) Datensatz von: Bodo Winter (Winter and Grawunder 2012; Winter 2013) Thema: Politeness and Pitch (F0) Tutorials: - Lineare Regression - Lineare Regression mit gemischten Effekten Artikel: The phonetic profile of Korean formal and informal speech registers Gliederung unserer quantitativen Analyse 1. Laden der Datei 2. Kennenlernen der Daten und Säubern 3. Hypothesen 4. Test und Ergebnisse 5. Schluss Eine einfache lineare Regression oder einen t-Test kann man auch in Excel berechnen, aber in Statistikprogrammen ist das bequemer. 5.2.3.1 Datei laden # politeness &lt;- read.csv(&quot;/cloud/project/data/politeness_data.csv&quot;) politeness &lt;- read.csv(&quot;data/politeness_data.csv&quot;) 5.2.3.2 Kennenlernen der Daten und Säubern Welche Variablen enthält die Datei? head(politeness) ## subject gender scenario attitude frequency ## 1 F1 F 1 pol 213.3 ## 2 F1 F 1 inf 204.5 ## 3 F1 F 2 pol 285.1 ## 4 F1 F 2 inf 259.7 ## 5 F1 F 3 pol 203.9 ## 6 F1 F 3 inf 286.9 Eine weitere Funktion, um die Datenstruktur zu betrachten: glimpse(politeness) ## Rows: 84 ## Columns: 5 ## $ subject &lt;chr&gt; &quot;F1&quot;, &quot;F1&quot;, &quot;F1&quot;, &quot;F1&quot;, &quot;F1&quot;, &quot;F1&quot;, &quot;F1&quot;, &quot;F1&quot;, &quot;F1&quot;, &quot;F1&quot;, ~ ## $ gender &lt;chr&gt; &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, ~ ## $ scenario &lt;int&gt; 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 1, 1, 2, 2, 3, 3, ~ ## $ attitude &lt;chr&gt; &quot;pol&quot;, &quot;inf&quot;, &quot;pol&quot;, &quot;inf&quot;, &quot;pol&quot;, &quot;inf&quot;, &quot;pol&quot;, &quot;inf&quot;, &quot;pol~ ## $ frequency &lt;dbl&gt; 213.3, 204.5, 285.1, 259.7, 203.9, 286.9, 250.8, 276.8, 231.~ Und noch eine Übersicht, die uns noch mehr zeigt, z.B. ob bestimmte Datenzellen leer sind (NA). Die Variable frequency enthält eine leere Datenzelle (s. n_missing). Das müssen wir bei der Berechnung des Durchschnitts berücksichtigen. library(skimr) skim(politeness) Table 5.1: Data summary Name politeness Number of rows 84 Number of columns 5 _______________________ Column type frequency: character 3 numeric 2 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace subject 0 1 2 2 0 6 0 gender 0 1 1 1 0 2 0 attitude 0 1 3 3 0 2 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist scenario 0 1.00 4.00 2.01 1.0 2.00 4.0 6.00 7.0 &lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2583&gt;&lt;U+2583&gt;&lt;U+2587&gt; frequency 1 0.99 193.58 65.54 82.2 131.55 203.9 248.55 306.8 &lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2586&gt; Am Experiment nahmen 6 Versuchspersonen teil (F1, , M7). Von jeder Versuchsperson (subject) haben wir 14 Messpunkte (n = 14). politeness %&gt;% count(subject) ## subject n ## 1 F1 14 ## 2 F2 14 ## 3 F3 14 ## 4 M3 14 ## 5 M4 14 ## 6 M7 14 Versuchspersonen: 3 weibliche und 3 männliche. politeness %&gt;% count(subject, gender) ## subject gender n ## 1 F1 F 14 ## 2 F2 F 14 ## 3 F3 F 14 ## 4 M3 M 14 ## 5 M4 M 14 ## 6 M7 M 14 Pro Verhaltensweise (attitude) stehen uns 42 Messpunkte zur Verfügung, um unsere (unten folgende) Hypothese zu überprüfen. politeness %&gt;% count(attitude) ## attitude n ## 1 inf 42 ## 2 pol 42 Berechnen wir mal die Grundfrequenz! politeness %&gt;% mean(frequency) ## [1] NA NA: Hoppla! In unserer Datenreihe fehlt eine Frequenz. In solch einem Fall haben wir zwei Möglichkeiten: entweder entfernen wir diese Datenzeile aus unserer Berechnung oder wir lassen unser Programm eine Schätzung des Wertes vornehmen, die aber an der Datendistribution und am Mittelwert nichts verändert. Letzteres machen wir mit einer impute-Funktion, die beispielsweise den Medianwert für den fehlend Datenpunkt einsetzt. Entfernen der leeren Datenzelle (NA) ist die einfachste Lösung, um die durchschnittliche Frequenz mit mean() berechnen zu können. Das erledigen wir mit der tidyverse-Funktion drop_na(). politeness %&gt;% drop_na(frequency) %&gt;% summarise(av_freq = mean(frequency)) ## av_freq ## 1 193.5819 Eine andere Möglichkeit, eine leere Datenzeile aus der Mittelwertberechnung zu entfernen, ist die Option na.rm = TRUE zur mean()-Funktion hinzuzufügen. politeness %&gt;% summarise(av_freq = mean(frequency, na.rm = TRUE)) ## av_freq ## 1 193.5819 Wir haben gerade die Durchschnittsfrequenz für alle Versuchspersonen berechnet. Berechnen wir sie nun getrennt nach weiblichen und männlichen Versuchspersonen! Zu diesem Zweck müssen wir vor der Mittelwertberechnung die Daten mit der group_by()-Funktion gruppieren. politeness %&gt;% drop_na(frequency) %&gt;% group_by(gender) %&gt;% summarise(av_freq = mean(frequency)) ## # A tibble: 2 x 2 ## gender av_freq ## &lt;chr&gt; &lt;dbl&gt; ## 1 F 247. ## 2 M 139. Erwartungsgemäß ist der Durchschnittswert bei Frauen höher als bei Männern: Frauen haben ja meist eine höhere Stimme als Männer. Ein Blick auf die Durchschnittsfrequenzen bei höflicher und informeller Sprechweise: In unserer Stichprobe mit 6 Versuchspersonen (je 14 Frequenzmessungen) zeigt sich ein Unterschied von etwa 18,2 Hz, und zwar 202,59 - 184,36. Um zu diesem Ergebnis zu gelangen, haben wir vor der summarise()-Funktion die group_by()-Funktion entsprechend angewandt. politeness %&gt;% drop_na() %&gt;% group_by(attitude) %&gt;% summarise(avg_freq = mean(frequency), sd_freq = sd(frequency)) ## # A tibble: 2 x 3 ## attitude avg_freq sd_freq ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 inf 203. 66.9 ## 2 pol 184. 63.6 # politeness %&gt;% # drop_na %&gt;% # transmute(attitude, frequency) %&gt;% # mutate(attitude = str_replace(attitude, &quot;pol&quot;, &quot;1&quot;), # attitude = str_replace(attitude, &quot;inf&quot;, &quot;0&quot;)) %&gt;% # mutate(attitude = parse_number(attitude)) 5.2.3.3 Hypothesen \\(H_0\\): Der durchschnittliche Grundfrequenzverlauf (F0) bei höflichem oder informellem Sprechverhalten (attitude) ist gleich. \\(H_1\\): Der durchschnittliche Grundfrequenzverlauf (F0) bei höflichem Sprechverhalten unterscheidet sich vom informellen. Nach unserem bisherigen Wissen erwarten wir, dass unsere Daten die Hypothese \\(H_1\\) bestätigen werden. Das überprüfen wir zunächst mit einem t-Test, anschließend mit einer linearen Regression. 5.2.3.4 t-Test Zunächst ein Blick auf die Durchschnittsfrequenzen bei höflicher und informeller Sprechweise. In unserer Stichprobe mit 6 Versuchspersonen (je 14 Frequenzmessungen) zeigt sich ein Unterschied von etwa 18,2 Hz. Gemäß Hypothese \\(H_1\\) ist der Unterschied nicht zufällig entstanden, sondern kann auf die Gesamtpopulation der Sprecher verallgemeinert werden. Nicht so gemäß Hypothese \\(H_0\\): Der Mittelwertunterschied zwischen den Stichproben kann zufällig entstanden sein, denn wenn wir eine andere Stichprobe genommen hätten, wäre der Unterschied vielleicht gleich Null gewesen. Mit statistischen Tests können wir diese beiden Hypothesen überprüfen. Einer davon ist der t-Test. politeness %&gt;% drop_na() %&gt;% group_by(attitude) %&gt;% summarise(avg_freq = mean(frequency), sd_freq = sd(frequency)) ## # A tibble: 2 x 3 ## attitude avg_freq sd_freq ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 inf 203. 66.9 ## 2 pol 184. 63.6 Die Varianzen und damit auch die Standardabweichungen (sd_freq) vom Mittelwert (avg_freq) sind in beiden Gruppen (inf und pol) ungefähr gleich groß. Beim t-Test können wir dies berücksichtigen, und zwar mit der Option var.equal = TRUE. Die Option paired = FALSE besagt, dass die beiden Gruppen unabhängig vom Messzeitpunkt sind. Der t-Test bestätigt \\(H_1\\) nicht (p &gt; 0,05): t.test(frequency ~ attitude, data = politeness, paired = FALSE, var.equal = TRUE) ## ## Two Sample t-test ## ## data: frequency by attitude ## t = 1.2718, df = 81, p-value = 0.2071 ## alternative hypothesis: true difference in means between group inf and group pol is not equal to 0 ## 95 percent confidence interval: ## -10.29058 46.75458 ## sample estimates: ## mean in group inf mean in group pol ## 202.5881 184.3561 Eine weitere Form, wie man den t-Test durchführen könnte. In den eckigen Klammern wird eine Bedingung oder Filter formuliert. # frequencies if polite pol = politeness$frequency[politeness$attitude == &quot;pol&quot;] # frequencies if informal inf = politeness$frequency[politeness$attitude == &quot;inf&quot;] t.test(pol, inf, var.equal = TRUE) ## ## Two Sample t-test ## ## data: pol and inf ## t = -1.2718, df = 81, p-value = 0.2071 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -46.75458 10.29058 ## sample estimates: ## mean of x mean of y ## 184.3561 202.5881 Oder eine dritte (längere) Variante, den t-Test durchzuführen: polite &lt;- politeness %&gt;% select(attitude, frequency) %&gt;% filter(attitude == &quot;pol&quot;) %&gt;% select(-attitude) informal &lt;- politeness %&gt;% select(attitude, frequency) %&gt;% filter(attitude == &quot;inf&quot;) %&gt;% select(-attitude) t.test(polite, informal, var.equal = TRUE) ## ## Two Sample t-test ## ## data: polite and informal ## t = -1.2718, df = 81, p-value = 0.2071 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -46.75458 10.29058 ## sample estimates: ## mean of x mean of y ## 184.3561 202.5881 Wenn man die Option var.equal = TRUE nicht angibt, wird der Welch-t-Test durchgeführt, d.h. das Programm geht davon aus, dass die Varianzen (bzw. Standardabweichungen) der beiden Gruppen sich signifikant unterscheiden. 5.2.3.5 Lineare Regression Mit dem t-Test konnten wir immer nur die Wirkung einer Variablen (z.B. attitude) auf den Frequenzverlauf prüfen. Mit einem linearen Regressionsmodell können wir dagegen die gleichzeitige Wirkung mehrerer Größen auf den Frequenzverlauf herausfinden. Eine lineare Regression hat den großen Vorteil, dass man mehr als eine unabhängige Variable (Prädiktor) verwenden kann, um eine Hypothese zu testen. Wir wählen Geschlecht (gender) und Sprechverhalten (attitude) als unabhängige Variablen, der Grundfrequenzverlauf (frequency) als abhängige Variable. Die grundlegende Formulierung des Programmcodes (für eventuelle Vergleiche mit anderen Modellversionen haben wir dem Modell auch den neuen Namen m1 gegeben): m &lt;- lm(frequency ~ gender + attitude, data = politeness) m1 &lt;- m summary(m) ## ## Call: ## lm(formula = frequency ~ gender + attitude, data = politeness) ## ## Residuals: ## Min 1Q Median 3Q Max ## -82.409 -26.561 -4.262 24.690 100.140 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 256.762 6.756 38.006 &lt;2e-16 *** ## genderM -108.349 7.833 -13.832 &lt;2e-16 *** ## attitudepol -19.553 7.833 -2.496 0.0146 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35.68 on 80 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.7109, Adjusted R-squared: 0.7037 ## F-statistic: 98.38 on 2 and 80 DF, p-value: &lt; 2.2e-16 Wie liest man die Regressionsergebnisse?2 Beginnen wir am Ende! Die F-Statistik am Ende besagt, dass das Regressionsmodell insgesamt gesehen einen signifikanten Beitrag zur Erklärung des Frequenzverlaufs leistet, denn der sehr kleine p-Wert (p-value: &lt; 2.2e-16) liegt deutlich unter dem 5% Signifikanzniveau. Die vorletzte Zeile gibt den \\(R^2\\)-Wert (Bestimmtheitsmaß) an, also wie viel Prozent der gesamten Varianz der abhängigen Variable (frequency) vom Modell erklärt wird (hier: 0,71, demnach 71 % bzw. mit adjusted \\(R^2\\) mehr als 70%, wenn die Korrektur berücksichtigt wird, die bei Einbezug mehr als einer unabhängigen Variable gilt und immer etwas niedriger ist). Der Intercept oder Konstante ist die Stelle, an der die Frequenzkurve die y-Achse schneidet (also die Ordinate). In diesem Fall beträgt der Wert etwa 257 Hz. Der Intercept-Wert ist meistens nicht sinnvoll interpretierbar (auch hier nicht). Aber wenn wir das unten folgende Diagramm gender effect plot betrachten und in Gedanken die Linie von dem Punkt für die weiblichen Versuchspersonen (F) in Richtung y-Achse verlängern, dann können wir uns vorstellen, dass die Linie etwa beim Wert 257 die y-Achse schneidet. Der Intercept ist somit der (mathematisch festgelegte) Basiswert für die weiblichen Versuchspersonen. Die weiblichen Versuchspersonen werden als Basis verwendet, weil das Programm alphabetisch vorgeht und F im Alphabet vor M erscheint. Der Koeffizient für genderM zeigt an, dass bei männlichen Versuchspersonen 108,35 Hz vom Basiswert der weiblichen Versuchspersonen (256,762) subtrahiert werden müssen. Das ist der Intercept für die männlichen Versuchspersonen. Der p-Wert ist erwartungsgemäß hochsignifikant (p &lt; 2e-16), denn die meisten Männer haben eine tiefere Stimme als Frauen. In der nächsten Zeile folgt der Koeffizient für attitudepol (polite). Der Koeffizient (-19,553) ist negativ und muss daher vom Basiswert, dem Intercept für die weiblichen Versuchspersonen (256,762), subtrahiert werden. Demnach ist die Tonlage beim höflichen Sprechverhalten (attitudepol) um 19,55 Hz tiefer als beim informellen Sprechverhalten. Der p-Wert ist signifikant (p = 0,0146). Grundfrequenz für Frauen bei informellem Sprechen: 256.762 + (-108.349)*0 + (-19.553)*0 = 256.762 Hz Grundfrequenz für Frauen bei höflichem Sprechen: 256.762 + (-108.349)*0 + (-19.553)*1 = 237.209 Hz Grundfrequenz für Männer bei informellem Sprechen: 256.762 + (-108.349)*1 + (-19.553)*0 = 148.413 Hz Grundfrequenz für Männer bei höflichem Sprechen: 256.762 + (-108.349)*1 + (-19.553)*1 = 128.86 Hz Durchschnittliche Grundfrequenz bei informellem Sprechen (Frauen + Männer): (256.762 + 148.413)/2 = 202.5875 Hz. Durchschnittliche Grundfrequenz bei höflichem Sprechen: (Frauen + Männer): (237.209 + 128.86)/2 = 183.0345 Hz. Das lineare Regressionsmodell bestätigt somit die Hypothese \\(H_1\\): F(2;80 = 98,38; p &lt; 0,001). Die Versuchspersonen sprechen demnach in einer tieferen Tonlage, wenn sie höflich sprechen, und zwar um ca. 19,5 Hz tiefer als wenn sie informell sprechen (p = 0,0146). Außerdem bestätigt das Regressionsmodell (erwartungsgemäß) auch, dass die männlichen Versuchspersonen mit einer tieferen Stimme sprechen als die weiblichen, und zwar um durchschnittlich 108 Hz. Aber da uns das bereits aus unserer Alltagserfahrung bekannt ist, interessiert uns dieses Ergebnis nicht. Das Bestimmtheitmaß, d.h. der \\(R^2\\)-Wert, beträgt 0,71 (d.h. etwa 71%). Das bedeutet, dass mit dem Regressionsergebnis ca. 71% der Variabilität unserer abhängigen Variable (frequency) erklärt wird. Das ist ein guter Wert in den Sozialwissenschaften. Das Regressionsmodell wollen wir auch mit Hilfe Programms effects graphisch veranschaulichen. library(effects) allEffects(m) ## model: frequency ~ gender + attitude ## ## gender effect ## gender ## F M ## 247.1035 138.7549 ## ## attitude effect ## attitude ## inf pol ## 203.2408 183.6875 plot(allEffects(m), multiline=TRUE, grid=TRUE, rug=FALSE, as.table=TRUE, confint=list(style=&quot;bars&quot;), x.var = &quot;gender&quot;) Man kann Regressionsmodelle auch mit tidyverse-Funktionen formulieren (der . bedeutet, dass der Datensatz politeness aus der vorherigen Zeile übernommen werden soll). Die tidy(()-Funktion des broom-Pakets sorgt für die Umformung in eine Tabelle. library(broom) politeness %&gt;% lm(frequency ~ attitude + gender, data = .) %&gt;% summary() %&gt;% broom::tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 257. 6.76 38.0 5.75e-53 ## 2 attitudepol -19.6 7.83 -2.50 1.46e- 2 ## 3 genderM -108. 7.83 -13.8 6.40e-23 Die unterschiedliche Tonlage bei informellem und höflichem Sprechen veranschaulichen wir noch mit einem Boxplot. politeness %&gt;% ggplot(aes(attitude, frequency, group = attitude, fill = attitude)) + geom_boxplot() + stat_summary(fun.y=mean, geom=&quot;point&quot;, shape=&quot;*&quot;, size=7, color=&quot;red&quot;, fill=&quot;red&quot;) + geom_jitter(width = 0.2) + # geom_hline(yintercept = c(202.5), # lty = 2, col = &quot;darkred&quot;) + # Polite-Mittelwert # geom_hline(yintercept = c(184.3), # lty = 2, col = &quot;darkgreen&quot;) + # Informal-Mittelwert facet_wrap(~ gender) Der rote Stern markiert den Durchschnittswert der jeweiligen Gruppe, der schwarze Balken den Median (d.h. den Wert, der genau in der Mitte aller Daten der jeweiligen Gruppe liegt). Im Kasten eines Boxplots liegen 50% aller Werte, darunter liegen 25% und darüber ebenfalls 25%. Bei den Männern (M) ist zu sehen, dass der Median (der schwarze Balken) und das arithmetische Mittel (der rote Stern) nicht übereinstimmen. Das deutet auf extremere Unterschiede zwischen den männlichen Versuchspersonen (Schiefe oder Asymmetrie). Die Schiefe (engl. skewness) oder Asymmetrie der Frequenzverteilung (Distribution) kann man in einem Histogramm oder Dichte-Diagramm (density) veranschaulichen. Das Histogram der weiblichen Versuchspersonen ist der Normalverteilung (einer Glockenform, mit den meisten Frequenzwerten in der Mitte) ähnlich, während das der männlichen deutlich schief ist. politeness %&gt;% ggplot(aes(frequency, fill = attitude)) + geom_density(alpha = 0.7) + facet_wrap(~ gender) politeness %&gt;% ggplot(aes(frequency, fill = attitude)) + geom_histogram(aes(y = ..count..), # density binwidth = 50, alpha = 0.7, color = &quot;white&quot;) + facet_wrap(~ gender) politeness %&gt;% filter(gender == &quot;F&quot;) %&gt;% ggplot(aes(frequency)) + geom_histogram(aes(y = ..density.., fill = attitude), # count binwidth = 50, alpha = 0.7, color = &quot;white&quot;) + stat_function( fun = dnorm, args = list( mean = mean( politeness$frequency[politeness$gender == &quot;F&quot;], na.rm = T), sd = sd(politeness$frequency[politeness$gender == &quot;F&quot;], na.rm = T)), col = &quot;#1b98e0&quot;, size = 2) politeness %&gt;% filter(gender == &quot;M&quot;) %&gt;% ggplot(aes(frequency)) + geom_histogram(aes(y = ..density.., fill = attitude), # count binwidth = 50, alpha = 0.7, color = &quot;white&quot;) + stat_function( fun = dnorm, args = list( mean = mean( politeness$frequency[politeness$gender == &quot;M&quot;], na.rm = T), sd = sd(politeness$frequency[politeness$gender == &quot;M&quot;], na.rm = T)), col = &quot;#1b98e0&quot;, size = 2) Wird das Sprechverhalten (attitude) durch das Geschlecht (gender) modifiziert (z.B. verändern Frauen ihre Tonlage beim höflichem Sprechen, Männer dagegen nicht oder kaum)? Das kann man durch Hinzufügung eines Interaktionsterms prüfen. Eine Interaktion kennzeichnet man in der Regressionsgleichung mit einem Stern zwischen den beteiligten Variablen (also wie beim Multiplizieren). Hier prüfen wir die Interaktion zwischen den beiden unabhängigen Variablen Geschlecht (gender) und Verhalten (attitude). Die Indikatorterme attitude (informal vs. polite) und gender (female vs. male), beide also mit zwei Stufen oder levels, sind vergleichbar mit An-/Aus-Schaltern. Sie zeigen an, um welchen Wert die Frequenzkurve nach unten (bei negativem Koeffizient) oder oben (bei positivem Koeffizient) verschoben wird. Der Interaktionsterm der beiden Indikatorterme zeigt an, um welchen zusätzlichen Wert der Frequenzverlauf verändert wird. Wäre eine kontinuierliche Variable (z.B. Zeit) in der Interaktion einbezogen, dann würde der Koeffizient der Interaktion die zusätzliche Steigung (slope) der abhängigen Variable anzeigen. m &lt;- lm(frequency ~ attitude*gender, data = politeness) m2 &lt;- m summary(m) ## ## Call: ## lm(formula = frequency ~ attitude * gender, data = politeness) ## ## Residuals: ## Min 1Q Median 3Q Max ## -78.486 -27.383 -0.986 20.570 96.020 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 260.686 7.784 33.491 &lt;2e-16 *** ## attitudepol -27.400 11.008 -2.489 0.0149 * ## genderM -116.195 11.008 -10.556 &lt;2e-16 *** ## attitudepol:genderM 15.890 15.664 1.014 0.3135 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35.67 on 79 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.7147, Adjusted R-squared: 0.7038 ## F-statistic: 65.95 on 3 and 79 DF, p-value: &lt; 2.2e-16 Allerdings ist der p-Wert für die Interaktion in unserem Fall nicht signifikant (p = 0,3135 liegt oberhalb des 5% Signifikanzniveaus, p = 0,05). Das bedeutet, dass die Interaktion zur Erklärung des Frequenzverlaufs keinen Beitrag leistet. Daher ist es sinnvoll, den Interaktionsterm aus der Regressionsgleichung zu entfernen und nur die (signifkanten) Haupteffekte beizubehalten. Wir bevorzugen demnach immer das einfachere Modell, wenn das komplexere keinen signifikanten Erklärungsbeitrag leistet. Mit der anova()-Funktion kann man Regressionsmodelle (hier: m1 und m2) miteinander vergleichen und prüfen, welches geeigneter ist, den Frequenzverlauf zu erklären. Modell m1 ist das Modell ohne Interaktion, Modell m2 das Modell mit Interaktionsterm. anova(m1, m2) ## Analysis of Variance Table ## ## Model 1: frequency ~ gender + attitude ## Model 2: frequency ~ attitude * gender ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 80 101820 ## 2 79 100511 1 1309.1 1.029 0.3135 Der p-Wert (p = 0,3135) ist nicht signifikant. In diesem Fall bevorzugen wir das einfachere Regressionsmodell, d.h. das Modell ohne Interaktion (m1). Mit Hilfe des Programms effects stellen wir das Regressionsmodell mit hinzugefügter Interaktion zwischen den beiden unabhängigen Variablen Geschlecht (gender) und Verhalten (attitude) auch graphisch dar. library(effects) allEffects(m) ## model: frequency ~ attitude * gender ## ## attitude*gender effect ## gender ## attitude F M ## inf 260.6857 144.4905 ## pol 233.2857 132.9800 plot(allEffects(m), multiline=TRUE, grid=TRUE, rug=FALSE, as.table=TRUE, confint=list(style=&quot;bars&quot;), x.var = &quot;gender&quot;) Die sich überschneidenden Konfidenzintervalle im Diagramm zeigen, dass die Durchschnittswerte keinen signifikanten Unterschied aufweisen. Außerdem gilt sowohl für die weiblichen als auch die männlichen Versuchspersonen, dass Frequenzwerte beim höflichen Sprechverhalten geringer sind. Die Interaktion liefert somit keinen signifikanten Erklärungsbeitrag. Es ist sinnvoll, nur die beiden Haupteffekte beizubehalten und die Interaktion aus dem Regressionsmodell herauszunehmen. Das nächste Diagramm bestätigt, dass die Variablen Geschlecht (gender) und Verhalten (attitude) mit statistischer Signifikanz die Höhe des Grundfrequnezverlaufs (frequency) beeinflussen, nicht jedoch die Interaktion beider Variablen (deren Konfidenzintervall überschreitet im Diagramm die Null-Linie). library(parameters) library(see) p1 = plot(parameters(m)) + ggplot2::labs(title = &quot;A Dot-and-Whisker Plot&quot;) p1 Das nächste Diagramm bestätigt, dass die Residuen (d.h. die jeweiligen Abweichungen der einzelnen Werte vom Durchschnitt) normalverteilt sind (p = 0.396, also größer als der Grenzwert 0.05). Damit ist eine der erforderlichen Bedingungen für die Durchführung einer linearen Regression erfüllt. library(performance) check &lt;- check_normality(m) ## OK: residuals appear as normally distributed (p = 0.396). ## Warning: Non-normality of residuals detected (p = 0.016). p2 = plot(check, type = &quot;qq&quot;) p2 library(performance) check &lt;- check_normality(m, effects = &quot;fixed&quot;) ## OK: residuals appear as normally distributed (p = 0.396). ## Warning: Non-normality of residuals detected (p = 0.016). p2a = plot(check, type = &quot;pp&quot;) p2a \\(Omega^2\\) ist eine alternative Größe zu \\(R^2\\), womit ebenfalls die erklärte Varianz eines linearen Regressionsmodells angegeben wird. Im Diagramm ist zu sehen, dass die Variable Geschlecht (gender) den größten Beitrag leistet (fast 70%), die Variable Verhalten (attitude) ca. 5%, während die Interaktion beider Variablen keinen signifikanten Beitrag zu Erklärung der Varianz leistet (Wert liegt bei 0%). library(effectsize) library(see) m &lt;- aov(frequency ~ attitude*gender, data = politeness) p3 = plot(omega_squared(m)) p3 Das nächste Diagramm zeigt die Verteilung der Daten für die beiden Geschlechter. p4 = ggplot(politeness, aes(x = attitude, y = frequency, color = gender)) + geom_point2() + theme_modern() p4 Weitere Darstellungsmöglichkeiten der Datendistribution: p4 = ggplot(politeness, aes(x = attitude, y = frequency, fill = gender)) + geom_violin() + theme_modern(axis.text.angle = 45) + scale_fill_material_d(palette = &quot;ice&quot;) p4 p5 = ggplot(politeness, aes(x = attitude, y = frequency, fill = gender)) + geom_violindot(fill_dots = &quot;black&quot;) + geom_jitter(width = 0.05) + theme_modern() + scale_fill_material_d() p5 Ob die Bedingungen für die Durchführung einer linearen Regression erfüllt sind, kann man mit einem Befehl ausführen, und zwar mit Hilfe des Programms performance. Hier wählen wir die Funktion check_model() mit dem Modell ohne Interaktion (da diese nicht signifikant war). library(performance) m &lt;- lm(frequency ~ attitude + gender, data = politeness) summary(m) ## ## Call: ## lm(formula = frequency ~ attitude + gender, data = politeness) ## ## Residuals: ## Min 1Q Median 3Q Max ## -82.409 -26.561 -4.262 24.690 100.140 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 256.762 6.756 38.006 &lt;2e-16 *** ## attitudepol -19.553 7.833 -2.496 0.0146 * ## genderM -108.349 7.833 -13.832 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35.68 on 80 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.7109, Adjusted R-squared: 0.7037 ## F-statistic: 98.38 on 2 and 80 DF, p-value: &lt; 2.2e-16 check &lt;- check_model(m) p6 = plot(check) p6 Collage mehrerer der oben einzeln gezeigten Diagramme mit Hilfe der plots()-Funktion im Programm performance: plots(p1,p2,p3,p4, n_columns = 2, tags = paste0(&quot;B&quot;, 1:4)) Eine Bayesianische Regressionsberechnung erlauben die Programmen bayestestR und rstanarm. Mit dem Programm see können wir die Datendistribution sichtbar machen. library(bayestestR) library(rstanarm) library(see) set.seed(123) m &lt;- stan_glm(frequency ~ attitude + gender, data = politeness, refresh = 0) result &lt;- hdi(m, ci = c(0.5, 0.75, 0.89, 0.95)) plot(result) 5.2.3.6 Ergebnis Die Regressionsanalyse hat \\(H_1\\) bestätigt, d.h. die Grundfrequenz beim höflichen Sprechverhalten unterscheidet sich vom informellen Sprechen. Beim höflichen Sprechen sprachen die Versuchspersonen mit einer durchschnittlich 19,5 Hz tieferen Stimme: bei den weiblichen Versuchspersonen mehr als 27 Hz (261 - 233 Hz), bei den männlichen mehr als 11 Hz (144 - 133 Hz)). 5.2.3.7 Lineare Regression Politeness data (B. Winter tutorial) Programme laden: library(tidyverse) Datei laden: # LOAD polite &lt;- read.csv(&quot;data/politeness_data.csv&quot;, dec=&quot;.&quot;) Ansicht der Datenlage zu Orientierungszwecken: head(polite) ## subject gender scenario attitude frequency ## 1 F1 F 1 pol 213.3 ## 2 F1 F 1 inf 204.5 ## 3 F1 F 2 pol 285.1 ## 4 F1 F 2 inf 259.7 ## 5 F1 F 3 pol 203.9 ## 6 F1 F 3 inf 286.9 Variablentyp festlegen: polite$frequency = as.numeric(polite$frequency) polite$scenario = as.factor(polite$scenario) polite$subject = as.factor(polite$subject) polite$gender = as.factor(polite$gender) polite$attitude = as.factor(polite$attitude) Kontraste für den statistischen Test setzen: # In this session we use contr. sum contrasts options(contrasts=c(&#39;contr.sum&#39;, &#39;contr.poly&#39;)) options(&quot;contrasts&quot;) ## $contrasts ## [1] &quot;contr.sum&quot; &quot;contr.poly&quot; Kontraste zurücksetzen: # To reset default settings run: options(contrasts=c(&#39;contr.treatment&#39;, &#39;contr.poly&#39;)) # (all afex functions should be unaffected by this) # # Setting contrasts of chosen variables only # contrasts(polite$attitude) &lt;- contr.treatment(2, base = 1) Einfacher Boxplot: boxplot(frequency ~ attitude*gender, col=c(&quot;red&quot;,&quot;green&quot;), data = polite) Bild speichern: - z.B. im jpg-Format oder - im pdf-Format. # 1. Open jpeg file jpeg(&quot;pictures/politeness_boxplot.jpg&quot;, width = 840, height = 535) # 2. Create the plot boxplot(frequency ~ attitude*gender, col=c(&quot;red&quot;,&quot;green&quot;), data = polite) # 3. Close the file dev.off() ## svg ## 2 # Open a pdf file pdf(&quot;pictures/politeness_boxplot.pdf&quot;) # 2. Create a plot boxplot(frequency ~ attitude*gender, col=c(&quot;red&quot;,&quot;green&quot;), data = polite) # Close the pdf file dev.off() ## svg ## 2 Beziehungen zwischen Variablenpaaren anzeigen: library(psych) pairs.panels(polite[c(2,4,5)]) Lineare Regression mit mehreren unabhängigen Variablen und einer abhängigen Variable, im Englischen auch als Ordinary Least Squares Regression (OLS) bekannt. Mit allen unabhängigen Variablen: # model 1 m &lt;- lm(frequency ~ gender + attitude + subject + scenario, data = polite) summary(m) ## ## Call: ## lm(formula = frequency ~ gender + attitude + subject + scenario, ## data = polite) ## ## Residuals: ## Min 1Q Median 3Q Max ## -53.673 -16.686 1.039 12.027 86.630 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 225.150 10.020 22.470 &lt; 2e-16 *** ## genderM -129.857 9.606 -13.518 &lt; 2e-16 *** ## attitudepol -19.794 5.585 -3.544 0.000707 *** ## subjectF2 26.150 9.606 2.722 0.008179 ** ## subjectF3 18.700 9.606 1.947 0.055592 . ## subjectM3 66.800 9.606 6.954 1.52e-09 *** ## subjectM4 41.854 9.807 4.268 6.09e-05 *** ## subjectM7 NA NA NA NA ## scenario2 25.017 10.376 2.411 0.018537 * ## scenario3 31.025 10.376 2.990 0.003847 ** ## scenario4 42.508 10.376 4.097 0.000111 *** ## scenario5 14.408 10.376 1.389 0.169351 ## scenario6 1.405 10.629 0.132 0.895227 ## scenario7 3.117 10.376 0.300 0.764783 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 25.42 on 70 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.8716, Adjusted R-squared: 0.8496 ## F-statistic: 39.61 on 12 and 70 DF, p-value: &lt; 2.2e-16 Regression mit denjenigen Variablen, die als Prädiktoren für die abhängige Variable gewählt wurden: # model 2 m &lt;- lm(frequency ~ gender + attitude, data=polite) summary(m) ## ## Call: ## lm(formula = frequency ~ gender + attitude, data = polite) ## ## Residuals: ## Min 1Q Median 3Q Max ## -82.409 -26.561 -4.262 24.690 100.140 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 256.762 6.756 38.006 &lt;2e-16 *** ## genderM -108.349 7.833 -13.832 &lt;2e-16 *** ## attitudepol -19.553 7.833 -2.496 0.0146 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35.68 on 80 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.7109, Adjusted R-squared: 0.7037 ## F-statistic: 98.38 on 2 and 80 DF, p-value: &lt; 2.2e-16 Koeffizienten der Variablen anzeigen: library(effects) allEffects(m) ## model: frequency ~ gender + attitude ## ## gender effect ## gender ## F M ## 247.1035 138.7549 ## ## attitude effect ## attitude ## inf pol ## 203.2408 183.6875 Visuelle Darstellung der Regressionsergebnisse: plot(allEffects(m), multiline=TRUE, grid=TRUE, rug=FALSE, as.table=TRUE) Bild sichern: # Save plot of the effects to disk # 1. Open jpeg file jpeg(&quot;pictures/politeness_lineplot.jpg&quot;, width = 840, height = 535) # 2. Create the plot plot(allEffects(m), multiline=TRUE, grid=TRUE, rug=FALSE, as.table=TRUE) # 3. Close the file dev.off() ## svg ## 2 Ein weiteres Regressionsmodell mit einer Interaktion zwischen den unabhängigen Variablen (Prädiktoren): # model 3 (with interaction) m &lt;- lm(frequency ~ gender*attitude, data=polite) summary(m) ## ## Call: ## lm(formula = frequency ~ gender * attitude, data = polite) ## ## Residuals: ## Min 1Q Median 3Q Max ## -78.486 -27.383 -0.986 20.570 96.020 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 260.686 7.784 33.491 &lt;2e-16 *** ## genderM -116.195 11.008 -10.556 &lt;2e-16 *** ## attitudepol -27.400 11.008 -2.489 0.0149 * ## genderM:attitudepol 15.890 15.664 1.014 0.3135 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35.67 on 79 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.7147, Adjusted R-squared: 0.7038 ## F-statistic: 65.95 on 3 and 79 DF, p-value: &lt; 2.2e-16 Koeffizienten der Variablen anzeigen: library(effects) allEffects(m) ## model: frequency ~ gender * attitude ## ## gender*attitude effect ## attitude ## gender inf pol ## F 260.6857 233.2857 ## M 144.4905 132.9800 Visuelle Darstellung der Regressionsergebnisse: plot(allEffects(m), multiline=TRUE, grid=TRUE, rug=FALSE, as.table=TRUE) Bild als jpg-Datei sichern: # Save plot of the effects to disk # 1. Open jpeg file jpeg(&quot;pictures/politeness_effects.jpg&quot;, width = 840, height = 535) # 2. Create the plot plot(allEffects(m), multiline=TRUE, grid=TRUE, rug=FALSE, as.table=TRUE) # 3. Close the file dev.off() ## svg ## 2 Bild als pdf-Datei sichern: # Open a pdf file pdf(&quot;pictures/politeness_effects.pdf&quot;) # 2. Create a plot plot(allEffects(m), multiline=TRUE, grid=TRUE, rug=FALSE, as.table=TRUE) # Close the pdf file dev.off() ## svg ## 2 Diagnostische Analyse (sind die Bedingungen für eine Regression erfüllt?): # plot diagnostic diagrams par(mfrow = c(3,2)) plot(m, which = 1) # variance of residuals vs. fitted values? plot(m, which = 2) # normal distributed residuals? plot(m, which = 3) # variance of residuals standardized plot(m, which = 4) # Cook&#39;s distance (outliers / influencing data points?) plot(m, which = 5) # Leverage vs. standardized variance of residuals plot(m, which = 6) # Cook&#39;s distance vs. Leverage par(mfrow = c(1,1)) Entfernung eines Datenpunktes und die dabei entstehende Veränderung des Koeffizienten: # Change of estimates if one datapoint is removed from the model d &lt;- dfbetas(m) head(d) %&gt;% as.data.frame %&gt;% rmarkdown::paged_table() Koeffizienten visuell darstellen: # plot the dfbetas (are there any outliers or data points with high influence?) par(mfrow = c(1,3)) plot(d[,1], col = &quot;orange&quot;) plot(d[,2], col = &quot;blue&quot;) plot(d[,3], col = &quot;purple&quot;) par(mfrow = c(1,1)) 5.2.3.8 Regression mit gemischten Effekten (Mixed effects Regression, Multilevel Regression) Programme laden: # The variables &#39;subject&#39; and &#39;scenario&#39; have been chosen as random effects library(afex) library(lmerTest) library(LMERConvenienceFunctions) Regressionsmodell mit einem individuell variierenden Intercept (Ordinate): # random intercepts model m &lt;- lmer(frequency ~ (1|subject), REML=F, data=politeness) m0.1 &lt;- m summary(m) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s ## method [lmerModLmerTest] ## Formula: frequency ~ (1 | subject) ## Data: politeness ## ## AIC BIC logLik deviance df.resid ## 833.2 840.5 -413.6 827.2 80 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.4921 -0.6514 -0.1596 0.6511 2.6732 ## ## Random effects: ## Groups Name Variance Std.Dev. ## subject (Intercept) 3289.7 57.36 ## Residual 941.2 30.68 ## Number of obs: 83, groups: subject, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 193.027 23.656 6.001 8.16 0.000182 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Regressionsmodell mit zwei individuell variierenden Intercepts (Ordinaten): # random intercepts model m &lt;- lmer(frequency ~ (1|subject) + (1|scenario), REML=F, data=politeness) m0.2 &lt;- m summary(m) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s ## method [lmerModLmerTest] ## Formula: frequency ~ (1 | subject) + (1 | scenario) ## Data: politeness ## ## AIC BIC logLik deviance df.resid ## 826.6 836.3 -409.3 818.6 79 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.40887 -0.53279 -0.09441 0.63269 2.80566 ## ## Random effects: ## Groups Name Variance Std.Dev. ## scenario (Intercept) 202.5 14.23 ## subject (Intercept) 3344.0 57.83 ## Residual 751.4 27.41 ## Number of obs: 83, groups: scenario, 7; subject, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 192.886 24.400 6.587 7.905 0.000135 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Regressionsmodell mit zwei individuell variierenden Intercepts (Ordinaten) und einer kategorischen Variable: # random intercepts model m &lt;- lmer(frequency ~ gender + (1|subject) + (1|scenario), REML=F, data=politeness) m1 &lt;- m summary(m) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s ## method [lmerModLmerTest] ## Formula: frequency ~ gender + (1 | subject) + (1 | scenario) ## Data: politeness ## ## AIC BIC logLik deviance df.resid ## 816.7 828.8 -403.4 806.7 78 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.49969 -0.57100 -0.06373 0.60229 2.86559 ## ## Random effects: ## Groups Name Variance Std.Dev. ## scenario (Intercept) 191.2 13.83 ## subject (Intercept) 409.6 20.24 ## Residual 751.9 27.42 ## Number of obs: 83, groups: scenario, 7; subject, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 246.986 13.481 7.676 18.321 1.3e-07 *** ## genderM -108.236 17.588 5.939 -6.154 0.000877 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## genderM -0.651 Regressionsmodell mit zwei individuell variierenden Intercepts (Ordinaten) und zwei Prädiktoren, zwei kategorischen Variablen. Von Interesse ist die Variable attitude (hier: sprachliches Verhalten). m &lt;- lmer(frequency ~ gender + attitude + (1|subject) + (1|scenario), REML=F, data=politeness) m2 &lt;- m summary(m) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s ## method [lmerModLmerTest] ## Formula: frequency ~ gender + attitude + (1 | subject) + (1 | scenario) ## Data: politeness ## ## AIC BIC logLik deviance df.resid ## 807.1 821.6 -397.6 795.1 77 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.2958 -0.6456 -0.0776 0.5448 3.5121 ## ## Random effects: ## Groups Name Variance Std.Dev. ## scenario (Intercept) 205.2 14.33 ## subject (Intercept) 417.0 20.42 ## Residual 637.4 25.25 ## Number of obs: 83, groups: scenario, 7; subject, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 256.847 13.827 8.500 18.576 3.53e-08 *** ## genderM -108.517 17.571 5.929 -6.176 0.000866 *** ## attitudepol -19.722 5.547 70.920 -3.555 0.000677 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) gendrM ## genderM -0.635 ## attitudepol -0.201 0.004 Regressionsmodell mit zwei individuell variierenden Intercepts (Ordinaten) und zwei interagierenden Prädiktoren. Von Interesse ist die Variable attitude. m &lt;- lmer(frequency ~ gender*attitude + (1|subject) + (1|scenario), REML=F, data=politeness) m3 &lt;- m summary(m) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s ## method [lmerModLmerTest] ## Formula: frequency ~ gender * attitude + (1 | subject) + (1 | scenario) ## Data: politeness ## ## AIC BIC logLik deviance df.resid ## 807.1 824.0 -396.6 793.1 76 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.1678 -0.5559 -0.0628 0.5103 3.3903 ## ## Random effects: ## Groups Name Variance Std.Dev. ## scenario (Intercept) 205.0 14.32 ## subject (Intercept) 418.8 20.47 ## Residual 620.0 24.90 ## Number of obs: 83, groups: scenario, 7; subject, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 260.686 14.086 9.140 18.506 1.48e-08 *** ## genderM -116.195 18.392 7.094 -6.318 0.000376 *** ## attitudepol -27.400 7.684 70.881 -3.566 0.000655 *** ## genderM:attitudepol 15.568 10.943 70.925 1.423 0.159229 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) gendrM atttdp ## genderM -0.653 ## attitudepol -0.273 0.209 ## gndrM:tttdp 0.192 -0.293 -0.702 Mit dem Programm jtools erhält man die Regressionsergebnisse in übersichtlicherer Form und mit zusätzlichen Größenberechnungen: library(jtools) summ(m3) Observations 83 Dependent variable frequency Type Mixed effects linear regression AIC 807.11 BIC 824.04 Pseudo-R2 (fixed effects) 0.71 Pseudo-R2 (total) 0.86 Fixed Effects Est. S.E. t val. d.f. p (Intercept) 260.69 14.09 18.51 9.14 0.00 genderM -116.20 18.39 -6.32 7.09 0.00 attitudepol -27.40 7.68 -3.57 70.88 0.00 genderM:attitudepol 15.57 10.94 1.42 70.92 0.16 p values calculated using Satterthwaite d.f. Random Effects Group Parameter Std. Dev. scenario (Intercept) 14.32 subject (Intercept) 20.47 Residual 24.90 Grouping Variables Group # groups ICC scenario 7 0.16 subject 6 0.34 Vergleich der Modelle: anova(m0.1, m0.2,m1,m2,m3) ## Data: politeness ## Models: ## m0.1: frequency ~ (1 | subject) ## m0.2: frequency ~ (1 | subject) + (1 | scenario) ## m1: frequency ~ gender + (1 | subject) + (1 | scenario) ## m2: frequency ~ gender + attitude + (1 | subject) + (1 | scenario) ## m3: frequency ~ gender * attitude + (1 | subject) + (1 | scenario) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## m0.1 3 833.25 840.51 -413.62 827.25 ## m0.2 4 826.63 836.30 -409.31 818.63 8.6246 1 0.0033166 ** ## m1 5 816.72 828.81 -403.36 806.72 11.9059 1 0.0005596 *** ## m2 6 807.10 821.61 -397.55 795.10 11.6178 1 0.0006532 *** ## m3 7 807.11 824.04 -396.55 793.11 1.9963 1 0.1576796 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Mit Hilfe der anova()-Funktion kann man eine Anova-Tabelle erstellen. anova(m3) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## gender 23570.5 23570.5 1 5.929 38.0164 0.0008739 *** ## attitude 7969.4 7969.4 1 70.925 12.8536 0.0006146 *** ## gender:attitude 1254.8 1254.8 1 70.925 2.0239 0.1592288 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Die Anova (mit Grundfrequenz als abhängige Variable, Geschlecht, Verhalten und ihrer Interaktion als Prädiktoren sowie Versuchspersonen und Szenario als Zufallsvariablen) ergab Geschlecht als signifikanten Haupteffekt (F(1; 5,929) = 38,0164; p = 0,0009; \\(\\eta_{p}^2\\) = 0,87) und Verhalten als signifikanten Haupteffekt auf die Höhe der Grundfrequenz (F(1; 70,925) = 12,8536; p = 0,0006; \\(\\eta_{p}^2\\) = 0,15). Die Interaktion zwischen Geschlecht und Verhalten war nicht signifikant (F(1; 70,925) = 2,0239; p = 0,15923; \\(\\eta_{p}^2\\) = 0,03). Das Pseudo-\\(R^2\\) für die Koeffizienten der Prädiktoren beträgt 0,71 (d.h. 71% der Varianz der Grundfrequenz wurden mit den Prädiktoren erklärt), das Pseudo-\\(R^2\\) für die Koeffizienten aller Effekte (fixed effects + random effects) beträgt 0,857 (d.h. mit allen Variablen wurden fast 86% der Grundfrequenzvariation erklärt). Der Post-hoc-Test für die Interaktion von Geschlecht und Verhalten ergab außerdem signifikant Unterschiede zwischen weiblichen und männlichen Testpersonen hinsichtlich der Grundfrequenzhöhe, und zwar sowohl bei informellen Sprechen (p = 0,0003) als auch bei höflichem Sprechen (p = 0,001). Da unter beiden Bedingungen (informelles vs. höfliches sprachliches Verhalten) ein signifikante Unterschied zwischen weiblichen und männlichen Versuchspersonen festgestellt wurde, führte die Interaktion beider Prädiktoren zu keinem signifikanten Einfluss auf den Verlauf der Grundfrequenz. Die \\(\\eta^2\\)-Funktion: library(sjstats) eta_sq(m3, partial = TRUE) ## # Effect Size for ANOVA (Type III) ## ## Parameter | Eta2 (partial) | 95% CI ## ----------------------------------------------- ## gender | 0.87 | [0.56, 1.00] ## attitude | 0.15 | [0.05, 1.00] ## gender:attitude | 0.03 | [0.00, 1.00] ## ## - One-sided CIs: upper bound fixed at (1). library(effectsize) eta_squared(m3, partial = TRUE) ## # Effect Size for ANOVA (Type III) ## ## Parameter | Eta2 (partial) | 95% CI ## ----------------------------------------------- ## gender | 0.87 | [0.56, 1.00] ## attitude | 0.15 | [0.05, 1.00] ## gender:attitude | 0.03 | [0.00, 1.00] ## ## - One-sided CIs: upper bound fixed at (1). Die Pseudo-\\(R^2\\)-Funktion: library(MuMIn) r.squaredGLMM(m3) ## R2m R2c ## [1,] 0.7122865 0.8565861 Der Post-hoc-Test für die Interaktion von Geschlecht und Verhalten: library(emmeans) emmeans(m3, pairwise ~ gender) ## $emmeans ## gender emmean SE df lower.CL upper.CL ## F 247 15.9 9.71 211 283 ## M 139 15.9 9.74 103 174 ## ## Results are averaged over the levels of: attitude ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## F - M 108 21 8.18 5.155 0.0008 ## ## Results are averaged over the levels of: attitude ## Degrees-of-freedom method: kenward-roger # with interaction emmeans(m3, pairwise ~ gender | attitude) ## $emmeans ## attitude = inf: ## gender emmean SE df lower.CL upper.CL ## F 261 16.3 11.3 224.8 297 ## M 144 16.3 11.3 108.6 180 ## ## attitude = pol: ## gender emmean SE df lower.CL upper.CL ## F 233 16.3 11.3 197.4 269 ## M 133 16.4 11.5 96.8 169 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## $contrasts ## attitude = inf: ## contrast estimate SE df t.ratio p.value ## F - M 116 21.7 9.79 5.348 0.0003 ## ## attitude = pol: ## contrast estimate SE df t.ratio p.value ## F - M 101 21.8 9.88 4.623 0.0010 ## ## Degrees-of-freedom method: kenward-roger Die oben berechneten Regressionsmodelle berücksichtigen die beiden Zufallsvariablen (random effects) Versuchsperson und Szenario. Damit berücksichtigen wir interindividuelle Unterschiede zwischen den Testpersonen und Unterschiede zwischen den verschiedenen Szenarien, die alle die Höhe der Grundfrequenz beeinflussen könnten. Dies ergibt individuelle Regressionskonstanten (Intercepts) für die einzelnen Versuchspersonen und Szenarien. Unterscheiden sich die Versuchspersonen nun auch darin, dass z.B. bestimmte Szenarien sie eher zu Grundfrequenzvariationen bewegen, d.h. die Steigung des Regressionskoeffizienten individuell beeinflussen (random slope)? Zuerst stellen wir ein Basismodell mit individuellen Steigungskoeffizienten auf: # politeness affected pitch (2(1)=11.62, p=0.00065), # lowering it by about 19.7 Hz ± 5.6 (standard errors) # random slopes model m &lt;- lmer(frequency ~ gender + (attitude + 1|subject) + (1|scenario), REML=F, data=politeness) m00 &lt;- m summary(m) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s ## method [lmerModLmerTest] ## Formula: frequency ~ gender + (attitude + 1 | subject) + (1 | scenario) ## Data: politeness ## ## AIC BIC logLik deviance df.resid ## 817.7 834.6 -401.9 803.7 76 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.2027 -0.6377 -0.1128 0.5095 3.0282 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## scenario (Intercept) 197.9 14.07 ## subject (Intercept) 380.6 19.51 ## attitudepol 276.8 16.64 0.64 ## Residual 670.4 25.89 ## Number of obs: 83, groups: scenario, 7; subject, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 267.329 13.337 7.798 20.044 5.47e-08 *** ## genderM -120.049 17.301 5.919 -6.939 0.000471 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## genderM -0.648 Dann fügen wir die uns interessierende Variable attitude hinzu: m &lt;- lmer(frequency ~ gender + attitude + (1|subject) + (attitude + 1|scenario), REML=F, data=politeness) m01 &lt;- m summary(m) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s ## method [lmerModLmerTest] ## Formula: frequency ~ gender + attitude + (1 | subject) + (attitude + 1 | ## scenario) ## Data: politeness ## ## AIC BIC logLik deviance df.resid ## 810.9 830.3 -397.5 794.9 75 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.2155 -0.6618 -0.0594 0.5256 3.4391 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## scenario (Intercept) 183.3 13.538 ## attitudepol 31.1 5.577 0.20 ## subject (Intercept) 417.2 20.425 ## Residual 628.3 25.066 ## Number of obs: 83, groups: scenario, 7; subject, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 256.863 13.704 8.129 18.744 5.61e-08 *** ## genderM -108.550 17.563 5.931 -6.181 0.000862 *** ## attitudepol -19.755 5.898 7.136 -3.350 0.011914 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) gendrM ## genderM -0.641 ## attitudepol -0.161 0.004 Wenn das Regressionsmodell mit den individuell variierenden Steigungskoeffizienten nicht berechnet werden kann, könnte auch ein Modell mit nur einem individuell variierenden Intercept in Frage kommen, z.B. diesem hier: Eine weitere mögliche Variante mit nur einem individuell variierenden Steigungskoeffizienten: Die Steigungskoeffizienten: m &lt;- m01 library(effects) allEffects(m) ## model: frequency ~ gender + attitude ## ## gender effect ## gender ## F M ## 247.1047 138.5548 ## ## attitude effect ## attitude ## inf pol ## 203.2420 183.4873 Visuelle Darstellung der Regressionsergebnisse: plot(allEffects(m), multiline=TRUE, grid=TRUE, rug=FALSE, as.table=TRUE) Das volle Regressionsmodell mit zwei individuell variierenden Intercepts und einem individuell variierenden Steigungskoeffizienten sowie einer Interaktion zweier kategorieller Prädiktoren: m &lt;- lmer(frequency ~ gender*attitude + (1|subject) + (attitude + 1|scenario), REML=F, data=politeness) Die Regressionsergebnisse: m02 &lt;- m summary(m) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s ## method [lmerModLmerTest] ## Formula: frequency ~ gender * attitude + (1 | subject) + (attitude + 1 | ## scenario) ## Data: politeness ## ## AIC BIC logLik deviance df.resid ## 810.9 832.7 -396.5 792.9 74 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.0820 -0.5585 -0.0174 0.4937 3.3116 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## scenario (Intercept) 186.40 13.653 ## attitudepol 36.41 6.034 0.13 ## subject (Intercept) 419.22 20.475 ## Residual 609.35 24.685 ## Number of obs: 83, groups: scenario, 7; subject, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 260.686 13.978 8.779 18.650 2.3e-08 *** ## genderM -116.195 18.372 7.075 -6.325 0.000378 *** ## attitudepol -27.400 7.952 22.083 -3.446 0.002296 ** ## genderM:attitudepol 15.506 10.850 64.159 1.429 0.157813 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) gendrM atttdp ## genderM -0.657 ## attitudepol -0.247 0.199 ## gndrM:tttdp 0.191 -0.291 -0.673 library(jtools) summ(m) Observations 83 Dependent variable frequency Type Mixed effects linear regression AIC 810.92 BIC 832.69 Pseudo-R2 (fixed effects) 0.71 Pseudo-R2 (total) 0.86 Fixed Effects Est. S.E. t val. d.f. p (Intercept) 260.69 13.98 18.65 8.78 0.00 genderM -116.20 18.37 -6.32 7.07 0.00 attitudepol -27.40 7.95 -3.45 22.08 0.00 genderM:attitudepol 15.51 10.85 1.43 64.16 0.16 p values calculated using Satterthwaite d.f. Random Effects Group Parameter Std. Dev. scenario (Intercept) 13.65 scenario attitudepol 6.03 subject (Intercept) 20.47 Residual 24.68 Grouping Variables Group # groups ICC scenario 7 0.15 subject 6 0.35 Vergleich der Modelle: Das Modell m02 (mit Interaktion) ist nicht signifikant besser als Modell m01 (ohne Interaktion). Demnach entscheiden wir uns für das einfachere Regressionsmodell (d.h. ohne Interaktion). Beide Modelle weisen individuell variierende Intercepts (random intercepts) und einen individuell variierenden Steigungskoeffizienten (random slope) auf. anova(m00,m01,m02) ## Data: politeness ## Models: ## m00: frequency ~ gender + (attitude + 1 | subject) + (1 | scenario) ## m01: frequency ~ gender + attitude + (1 | subject) + (attitude + 1 | scenario) ## m02: frequency ~ gender * attitude + (1 | subject) + (attitude + 1 | scenario) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## m00 7 817.71 834.64 -401.85 803.71 ## m01 8 810.93 830.28 -397.47 794.93 8.7733 1 0.003057 ** ## m02 9 810.92 832.69 -396.46 792.92 2.0117 1 0.156095 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Die step()-Funktion ermittelt (mittels Rückwärtseliminierung nicht signifikanter Variablen) die entsprechenden Bestandteile der Regressionsgleichung: library(lmerTest) s &lt;- step(m) s ## Backward reduced random-effect table: ## ## Eliminated npar logLik AIC LRT Df ## &lt;none&gt; 9 -396.46 810.92 ## attitude in (attitude + 1 | scenario) 1 7 -396.55 807.11 0.1827 2 ## (1 | subject) 0 6 -410.45 832.90 27.7921 1 ## (1 | scenario) 0 6 -402.35 816.71 11.6007 1 ## Pr(&gt;Chisq) ## &lt;none&gt; ## attitude in (attitude + 1 | scenario) 0.9126813 ## (1 | subject) 1.351e-07 *** ## (1 | scenario) 0.0006593 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Backward reduced fixed-effect table: ## Degrees of freedom method: Satterthwaite ## ## Eliminated Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## gender:attitude 1 1254.8 1254.8 1 70.925 2.0239 0.1592288 ## gender 0 24310.7 24310.7 1 5.929 38.1404 0.0008664 *** ## attitude 0 8057.2 8057.2 1 70.920 12.6408 0.0006768 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Model found: ## frequency ~ gender + attitude + (1 | subject) + (1 | scenario) Diagnostik mit Hilfe des Programms library(LMERConvenienceFunctions) am Beispiel des Modells ohne Interaktion, aber mit individuell variierenden Intercetps und Steigungskoeffizienten: m &lt;- lmer(frequency ~ gender + attitude + (1|subject) + (attitude + 1|scenario), REML=F, data=politeness) m01 &lt;- m summary(m) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s ## method [lmerModLmerTest] ## Formula: frequency ~ gender + attitude + (1 | subject) + (attitude + 1 | ## scenario) ## Data: politeness ## ## AIC BIC logLik deviance df.resid ## 810.9 830.3 -397.5 794.9 75 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.2155 -0.6618 -0.0594 0.5256 3.4391 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## scenario (Intercept) 183.3 13.538 ## attitudepol 31.1 5.577 0.20 ## subject (Intercept) 417.2 20.425 ## Residual 628.3 25.066 ## Number of obs: 83, groups: scenario, 7; subject, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 256.863 13.704 8.129 18.744 5.61e-08 *** ## genderM -108.550 17.563 5.931 -6.181 0.000862 *** ## attitudepol -19.755 5.898 7.136 -3.350 0.011914 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) gendrM ## genderM -0.641 ## attitudepol -0.161 0.004 library(LMERConvenienceFunctions) # Check model asumptions mcp.fnc(m) Das Programm library(performance) hat ebenfalls mehrere Funktionen, um zu überprüfen, ob die Bedingungen für die Durchführung der linearen Regression erfüllt sind: library(performance) model_parameters(m) ## # Fixed Effects ## ## Parameter | Coefficient | SE | 95% CI | t(75) | p ## ------------------------------------------------------------------------- ## (Intercept) | 256.86 | 13.70 | [ 229.56, 284.16] | 18.74 | &lt; .001 ## gender [M] | -108.55 | 17.56 | [-143.54, -73.56] | -6.18 | &lt; .001 ## attitude [pol] | -19.75 | 5.90 | [ -31.50, -8.01] | -3.35 | 0.001 ## ## # Random Effects ## ## Parameter | Coefficient ## --------------------------------------------------- ## SD (Intercept: scenario) | 13.54 ## SD (Intercept: subject) | 20.43 ## SD (attitudepol: scenario) | 5.58 ## Cor (Intercept~attitudepol: scenario) | 0.20 ## SD (Residual) | 25.07 model_performance(m) ## # Indices of model performance ## ## AIC | BIC | R2 (cond.) | R2 (marg.) | ICC | RMSE | Sigma ## --------------------------------------------------------------------- ## 810.934 | 830.285 | 0.855 | 0.709 | 0.501 | 23.346 | 25.066 check_normality(m) ## OK: residuals appear as normally distributed (p = 0.109). check_heteroscedasticity(m) ## Warning: Heteroscedasticity (non-constant error variance) detected (p &lt; .001). check_collinearity(m) ## # Check for Multicollinearity ## ## Low Correlation ## ## Term VIF Increased SE Tolerance ## gender 1.00 1.00 1.00 ## attitude 1.00 1.00 1.00 check_distribution(m) ## # Distribution of Model Family ## ## Predicted Distribution of Residuals ## ## Distribution Probability ## normal 72% ## tweedie 12% ## gamma 9% ## ## Predicted Distribution of Response ## ## Distribution Probability ## lognormal 22% ## tweedie 22% ## weibull 12% # check_model(m) Überprüfung der Varianzhomogenität (für Regression ohne gemischte Effekte): fligner.test(frequency ~ attitude, politeness) ## ## Fligner-Killeen test of homogeneity of variances ## ## data: frequency by attitude ## Fligner-Killeen:med chi-squared = 0.21737, df = 1, p-value = 0.6411 fligner.test(frequency ~ gender, politeness) ## ## Fligner-Killeen test of homogeneity of variances ## ## data: frequency by gender ## Fligner-Killeen:med chi-squared = 0.7388, df = 1, p-value = 0.39 Überprüfung auf Normalität der abhängigen Variable mit Hilfe eines statistischen Tests, der aber bei großen Stichproben nicht zuverlässig ist: shapiro.test(politeness$frequency) ## ## Shapiro-Wilk normality test ## ## data: politeness$frequency ## W = 0.94456, p-value = 0.001347 Welcher Datenpunkt fehlt im Datensatz? which(is.na(politeness$frequency)) ## [1] 39 Entfernen des fehlenden Datenpunktes aus dem Datensatz: # delete NA from data frame in row 39 polite1 &lt;- politeness[-39,] Programmfunktion, die Ausreißer (outlier) im Datensatz feststellt und entfernt: # Remove outliers freqout &lt;- romr.fnc(m, polite1, trim=2.5) ## n.removed = 1 ## percent.removed = 1.204819 Anzahl der entfernten Ausreißer: freqout$n.removed ## [1] 1 Anteil der entfernten Ausreißer: freqout$percent.removed ## [1] 1.204819 Auswahl des neuen Datensatzes, aus dem die Ausreißer entfernt wurden: freqout &lt;- freqout$data attach(freqout) Regression mit dem Datensatz, aus dem die Ausreißer entfernt wurden: # update model m &lt;- lmer(frequency ~ gender + attitude + (1|subject) + (1|scenario), REML=F, data=freqout) m01 &lt;- m summary(m) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s ## method [lmerModLmerTest] ## Formula: frequency ~ gender + attitude + (1 | subject) + (1 | scenario) ## Data: freqout ## ## AIC BIC logLik deviance df.resid ## 782.9 797.4 -385.5 770.9 76 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.50453 -0.54969 -0.04816 0.55710 2.73489 ## ## Random effects: ## Groups Name Variance Std.Dev. ## scenario (Intercept) 207.0 14.39 ## subject (Intercept) 417.6 20.44 ## Residual 518.4 22.77 ## Number of obs: 82, groups: scenario, 7; subject, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 258.180 13.692 8.470 18.856 3.26e-08 *** ## genderM -111.184 17.431 5.940 -6.378 0.000726 *** ## attitudepol -22.389 5.043 70.028 -4.440 3.28e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) gendrM ## genderM -0.637 ## attitudepol -0.184 0.008 library(jtools) summ(m) Observations 82 Dependent variable frequency Type Mixed effects linear regression AIC 782.92 BIC 797.36 Pseudo-R2 (fixed effects) 0.74 Pseudo-R2 (total) 0.88 Fixed Effects Est. S.E. t val. d.f. p (Intercept) 258.18 13.69 18.86 8.47 0.00 genderM -111.18 17.43 -6.38 5.94 0.00 attitudepol -22.39 5.04 -4.44 70.03 0.00 p values calculated using Satterthwaite d.f. Random Effects Group Parameter Std. Dev. scenario (Intercept) 14.39 subject (Intercept) 20.44 Residual 22.77 Grouping Variables Group # groups ICC scenario 7 0.18 subject 6 0.37 Erneute Überprüfung der Varianzhomoskedastizität (Gleichförmigkeit der Varianz) und Normalität der Residuen (Abweichungen vom Mittelwert): # Re-Check model asumptions mcp.fnc(m) Andere Varianztests (vor allem für Regression ohne gemischte Effekte geeignet): fligner.test(frequency ~ attitude, freqout) ## ## Fligner-Killeen test of homogeneity of variances ## ## data: frequency by attitude ## Fligner-Killeen:med chi-squared = 0.34994, df = 1, p-value = 0.5541 fligner.test(frequency ~ gender, freqout) ## ## Fligner-Killeen test of homogeneity of variances ## ## data: frequency by gender ## Fligner-Killeen:med chi-squared = 0.25815, df = 1, p-value = 0.6114 Normalitätstest (geeignet für kleinere Stichproben): shapiro.test(freqout$frequency) ## ## Shapiro-Wilk normality test ## ## data: freqout$frequency ## W = 0.9441, p-value = 0.001373 politeness %&gt;% drop_na() %&gt;% group_by(gender, attitude) %&gt;% summarise(M = mean(frequency)) ## # A tibble: 4 x 3 ## # Groups: gender [2] ## gender attitude M ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 F inf 261. ## 2 F pol 233. ## 3 M inf 144. ## 4 M pol 133. References "],["vokalformanten-im-deutschen-als-fremdsprache.html", "Kapitel 6 Vokalformanten im Deutschen als Fremdsprache 6.1 Programme starten 6.2 Daten laden &amp; anpassen 6.3 IPA syms 6.4 Vergleich der Vokalformanten 6.5 Aggregierter Datensatz 6.6 Abbildungen mit phonR", " Kapitel 6 Vokalformanten im Deutschen als Fremdsprache Vowel formants in German as a foreign language Akustische Messungen der ersten beiden Vokalformanten und Vokaldauer mit Praat, durchgeführt von Studierenden während einer Unterrichtsstunde in einem Einführungsseminar zur computerunterstützten Auswertung von deutschen Sprachkorpora und -datensätzen. 6.1 Programme starten library(tidyverse) library(scales) library(readxl) library(writexl) library(phonR) library(extrafont) 6.2 Daten laden &amp; anpassen 6.2.1 Datensatz 1 vokale &lt;- read_xlsx(&quot;data/S03_Vokalformanten_Diagramme.xlsx&quot;, sheet =&quot;A1-4_alle&quot;) %&gt;% janitor::clean_names() %&gt;% dplyr::select(-studierende) %&gt;% mutate(geschlecht = &quot;f&quot;) %&gt;% dplyr::select(sprecherin, geschlecht, vokal, vowel, f1, f2, dauer, lange, wort, phrase) %&gt;% mutate(l1_l2 = ifelse(sprecherin == &quot;Deutsche&quot;, &quot;L1&quot;, &quot;L2&quot;)) %&gt;% mutate(vokal = str_replace(vokal, &quot;F:&quot;, &quot;E:&quot;)) # %&gt;% # mutate(vowel = vokal) vokale %&gt;% rmarkdown::paged_table() 6.2.2 Datensatz 2 vergleich &lt;- read_xlsx(&quot;data/S03_Vokalformanten_Diagramme.xlsx&quot;, sheet =&quot;A10_Vgl_L1_L2_tab&quot;) %&gt;% janitor::clean_names() %&gt;% mutate(phonem = str_replace(phonem, &quot;EE&quot;, &quot;E:&quot;)) %&gt;% rename(f1_l1 = f1_in_hz, f2_l1 = f2_in_hz, dauer_l1 = dauer_in_ms, vokal = phonem) %&gt;% dplyr::select(-phonem_ipa_1, -phonem_ipa_2) vergleich %&gt;% rmarkdown::paged_table() 6.2.3 Datensatz 3 Dieser Datensatz wird aus einzelnen Datensätzen, die Messungen von Studierenden enthalten, zusammengesetzt. df0 &lt;- read.csv(&quot;data/Deutsche_formants.Table.csv&quot;, stringsAsFactors = FALSE, fileEncoding = &quot;UTF-8&quot;) df1a &lt;- read.csv(&quot;data/Monika_I_formants.Table.csv&quot;, stringsAsFactors = FALSE, fileEncoding = &quot;UTF-8&quot;) df1b &lt;- read.csv(&quot;data/Monika_II_formants.Table.csv&quot;, stringsAsFactors = FALSE, fileEncoding = &quot;UTF-8&quot;) df2 &lt;- read.csv(&quot;data/Donna_formants.Table.csv&quot;, stringsAsFactors = FALSE, fileEncoding = &quot;UTF-8&quot;) df3 &lt;- read.csv(&quot;data/Metka_formants.Table.csv&quot;, stringsAsFactors = FALSE, fileEncoding = &quot;UTF-8&quot;) df4 &lt;- read.csv(&quot;data/Jasmina_formants.Table.csv&quot;, stringsAsFactors = FALSE, fileEncoding = &quot;UTF-8&quot;) df5 &lt;- read.csv(&quot;data/Teodor_II_formants.Table.csv&quot;, stringsAsFactors = FALSE, fileEncoding = &quot;UTF-8&quot;) df0 &lt;- df0 %&gt;% mutate(speaker = &quot;Deutsche&quot;) df1a &lt;- df1a %&gt;% mutate(speaker = &quot;Monika1&quot;) df1b &lt;- df1b %&gt;% mutate(speaker = &quot;Monika2&quot;) df2 &lt;- df2 %&gt;% mutate(speaker = &quot;Donna&quot;) df3 &lt;- df3 %&gt;% mutate(speaker = &quot;Metka&quot;) df4 &lt;- df4 %&gt;% mutate(speaker = &quot;Jasmina&quot;) df5 &lt;- df5 %&gt;% mutate(speaker = &quot;Teodor&quot;) df &lt;- rbind(df0,df1a,df1b,df2,df3,df4,df5) df %&gt;% rmarkdown::paged_table() 6.3 IPA syms Die Listen wurden mit Hilfe der Angaben im Wikipedia-Artikel Phonetic symbols in Unicode, Abschnitt 2.2 Vowels erstellt. list1 &lt;- levels(factor(vergleich$vowel)) %&gt;% paste(collapse = &quot;,&quot;) list2 &lt;- levels(factor(vokale$vowel)) %&gt;% paste(collapse = &quot;,&quot;) list3 &lt;- levels(factor(df$vowel)) %&gt;% paste(collapse = &quot;,&quot;) ipa_list1 &lt;- c( a = &quot;\\u0061&quot;, AA = &quot;\\u0251\\u02D0&quot;, E = &quot;\\u025B&quot;, ee = &quot;\\u0065\\u02D0&quot;, EE = &quot;\\u025B\\u02D0&quot;, I = &quot;\\u026A&quot;, ii = &quot;\\u0069\\u02D0&quot;, O = &quot;\\u0254&quot;, oe = &quot;\\u00F8\\u02D0&quot;, OE = &quot;\\u0153&quot;, oo = &quot;\\u006F\\u02D0&quot;, schwa = &quot;\\u0259&quot;, U = &quot;\\u028A&quot;, uu = &quot;\\u0075\\u02D0&quot;, Y = &quot;\\u028F&quot;, yy = &quot;\\u0079\\u02D0&quot; ) # %&gt;% paste(collapse = &quot;,&quot;) ipa_list2 &lt;- c( a = &quot;\\u0061&quot;, AA = &quot;\\u0251\\u02D0&quot;, E = &quot;\\u025B&quot;, ee = &quot;\\u0065\\u02D0&quot;, EE = &quot;\\u025B\\u02D0&quot;, I = &quot;\\u026A&quot;, ii = &quot;\\u0069\\u02D0&quot;, O = &quot;\\u0254&quot;, oe = &quot;\\u00F8\\u02D0&quot;, OE = &quot;\\u0153&quot;, oo = &quot;\\u006F\\u02D0&quot;, U = &quot;\\u028A&quot;, uu = &quot;\\u0075\\u02D0&quot;, Y = &quot;\\u028F&quot;, yy = &quot;\\u0079\\u02D0&quot; ) # %&gt;% paste(collapse = &quot;,&quot;) ipa_list3 &lt;- c( a = &quot;\\u0061&quot;, AA = &quot;\\u0251\\u02D0&quot;, E = &quot;\\u025B&quot;, Ea = &quot;\\u025B\\u0061&quot;, ee = &quot;\\u0065\\u02D0&quot;, EE = &quot;\\u025B\\u02D0&quot;, I = &quot;\\u026A&quot;, ii = &quot;\\u0069\\u02D0&quot;, O = &quot;\\u0254&quot;, oe = &quot;\\u00F8\\u02D0&quot;, OE = &quot;\\u0153&quot;, oo = &quot;\\u006F\\u02D0&quot;, OO = &quot;\\u0254\\u02D0&quot;, OOE = &quot;\\u0153\\u02D0&quot;, schwa = &quot;\\u0259&quot;, U = &quot;\\u028A&quot;, uu = &quot;\\u0075\\u02D0&quot;, UU = &quot;\\u028A\\u02D0&quot;, Y = &quot;\\u028F&quot;, yy = &quot;\\u0079\\u02D0&quot;, YY = &quot;\\u028F\\u02D0&quot; ) # %&gt;% paste(collapse = &quot;,&quot;) Umwandlung der IPA-Zeichen mit dem Ziel, die phonetischen Symbole auch im html- oder pdf-Format richtig anzuzeigen. vergleich$vowel_ipa &lt;- ipa_list1[as.numeric(factor(vergleich$vowel))] vokale$vowel_ipa &lt;- ipa_list2[as.numeric(factor(vokale$vowel))] df$vowel_ipa &lt;- ipa_list3[as.numeric(factor(df$vowel))] vergleich %&gt;% pull(as.numeric(factor(vowel_ipa))) ## [1] &quot;a&quot; &quot;a:&quot; &quot;e:&quot; &quot;E&quot; &quot;E:&quot; &quot;I&quot; &quot;i:&quot; &quot;O&quot; &quot;o:&quot; &quot;U&quot; &quot;u:&quot; &quot;Ü&quot; &quot;ü:&quot; &quot;Ö&quot; &quot;ö:&quot; ## [16] &quot;@&quot; vergleich %&gt;% pull(vowel_ipa) ## a AA ee E EE I ii O oo U uu Y yy ## &quot;a&quot; &quot;&lt;U+0251&gt;&lt;U+02D0&gt;&quot; &quot;e&lt;U+02D0&gt;&quot; &quot;&lt;U+025B&gt;&quot; &quot;&lt;U+025B&gt;&lt;U+02D0&gt;&quot; &quot;&lt;U+026A&gt;&quot; &quot;i&lt;U+02D0&gt;&quot; &quot;&lt;U+0254&gt;&quot; &quot;o&lt;U+02D0&gt;&quot; &quot;&lt;U+028A&gt;&quot; &quot;u&lt;U+02D0&gt;&quot; &quot;&lt;U+028F&gt;&quot; &quot;y&lt;U+02D0&gt;&quot; ## OE oe schwa ## &quot;&quot; &quot;ø&lt;U+02D0&gt;&quot; &quot;&lt;U+0259&gt;&quot; Ursprüngliche IPA-Symbolisierungen sind ausgeblendet. Matthew Winn, cf. Listen Lab oder Working with Vowels part 1 und Working with Vowels part 2, wandelt die IPA-Symbole für die graphische Darstellung um, und zwar mit as.numeric(vowel). Translation of code: - Index [] the list of vowel IPA symbols - using the vector of values in the Vowel column - as if the Vowels were treated as their corresponding numeric indices in the automatic ordering. data.melt$Vowel.IPA &lt;- IPA_vowels[as.numeric(data.melt$Vowel)] 6.4 Vergleich der Vokalformanten Verglichen werden Vokale deutscher Muttersprachler_innen mit Vokalen von Studierenden des Deutschen als Fremdsprache. Verglichen werden die ersten beiden Vokalformanten (F1 und F2). In den Diagrammen werden IPA-Symbole verwendet. 6.4.1 Datensatz 2 6.4.1.1 Diagramm 1 (statisch) vgl_pivot &lt;- vergleich %&gt;% group_by(vokal) %&gt;% pivot_longer(f1_l1:dauer_l2, names_to = &quot;category&quot;, values_to = &quot;value&quot;) %&gt;% separate(category, into = c(&quot;category&quot;, &quot;l1_l2&quot;)) %&gt;% drop_na() %&gt;% pivot_wider(names_from = category, values_from = value) vgl_pivot %&gt;% rmarkdown::paged_table() # par(family=&#39;Charis SIL&#39;) (graph1 &lt;- vgl_pivot %&gt;% drop_na() %&gt;% group_by(vowel_ipa, l1_l2, lange) %&gt;% ggplot(aes(f2,f1, label = vowel_ipa)) + geom_hex(alpha = 0.2, show.legend = F) + theme(text=element_text(size=16)) + # family = &quot;Charis SIL&quot; geom_text(aes(label = vowel_ipa, color = vowel_ipa), # family = &quot;Charis SIL&quot; vjust = 1, hjust = 1, check_overlap = T, show.legend = F, size = 6) + # geom_label(aes(x = mean(f2), y = mean(f1)), color = &quot;black&quot;) + # stat_ellipse() + scale_y_reverse() + scale_x_reverse(breaks = c(1000, 1250, 1500, 1750, 2000, 2250, 2500)) + facet_wrap(~ lange + l1_l2) + theme_light() + labs(y = &quot;Formant F1: tief &gt;&gt; hoch&quot;, x = &quot;Formant F2: &lt;&lt; vorne - hinten &gt;&gt;&quot;) + theme(#panel.grid.major=element_blank(), #panel.grid.minor=element_blank(), # text = element_text(family=&#39;Charis SIL&#39;), plot.title = element_text(hjust = 0.5), legend.position = &quot;none&quot;) ) ggsave(&quot;pictures/vergleich_vokalformanten_lang_kurz_ipa.jpg&quot;) 6.4.1.2 Diagramm 1 (interaktiv) Interaktives Diagramm mit dem plotly-Programm: library(plotly) ggplotly(graph1) %&gt;% layout(showlegend = FALSE) 6.4.1.3 Diagramm 1 (interaktiv2) Interaktives Diagramm mit zusätzlichen Einstellungen im plotly-Programm. font = list( # family = &#39;Charis SIL&#39;, family = &#39;Arial&#39;, size = 15, color = &quot;black&quot; ) label = list( bgcolor = &quot;white&quot;, bordercolor = &quot;transparent&quot;, font = font ) library(plotly) (graph1_interactive &lt;- ggplotly(graph1, tooltip=c(&quot;x&quot;, &quot;y&quot;, &quot;text&quot;)) %&gt;% style(hoverlabel = label) %&gt;% layout(showlegend = FALSE, font = font, yaxis = list(fixedrange = TRUE), xaxis = list(fixedrange = TRUE)) %&gt;% config(displayModeBar = FALSE, showTips = T) ) library(htmlwidgets) saveWidget(graph1_interactive, &quot;pictures/vokalformanten_interaktiv_l1_l2_lang_kurz.html&quot;, selfcontained = T) # Sys.setenv(&quot;plotly_username&quot;=&quot;dataslice&quot;) # Sys.setenv(&quot;plotly_api_key&quot;=&quot;x&quot;) # # api_create(space_times, &quot;Space Times&quot;) # save it in html library(&quot;htmlwidgets&quot;) saveWidget(graph1_interactive,&quot;tmp.html&quot;, selfcontained = F) # and in pdf library(webshot) webshot(&quot;tmp.html&quot;,&quot;pictures/vokalformanten_interaktiv_l1_l2_lang_kurz.png&quot;, delay =5, vwidth = 1000, vheight=800) webshot(&quot;tmp.html&quot;,&quot;pictures/vokalformanten_interaktiv_l1_l2_lang_kurz.pdf&quot;, delay =5, vwidth = 800, vheight=600) 6.4.1.4 Diagramm 2 (statisch) vgl_pivot &lt;- vergleich %&gt;% group_by(vokal) %&gt;% pivot_longer(f1_l1:dauer_l2, names_to = &quot;category&quot;, values_to = &quot;value&quot;) %&gt;% separate(category, into = c(&quot;category&quot;, &quot;l1_l2&quot;)) %&gt;% drop_na() %&gt;% pivot_wider(names_from = category, values_from = value) vgl_pivot %&gt;% rmarkdown::paged_table() (graph2 &lt;- vgl_pivot %&gt;% drop_na() %&gt;% group_by(vowel_ipa, l1_l2, lange) %&gt;% ggplot(aes(f2,f1)) + geom_hex(alpha = 0.2, show.legend = F) + geom_text(aes(label = vowel_ipa, color = vowel_ipa), vjust = 1, hjust = 1, check_overlap = T, show.legend = F, size = 6) + scale_y_reverse() + scale_x_reverse(breaks = c(1000, 1250, 1500, 1750, 2000, 2250, 2500)) + facet_wrap(~ l1_l2) + theme_light() + labs(y = &quot;Formant F1: tief &gt;&gt; hoch&quot;, x = &quot;Formant F2: &lt;&lt; vorne - hinten &gt;&gt;&quot;) ) ggsave(&quot;pictures/vergleich_vokalformanten_lang_kurz_ipa.jpg&quot;) 6.4.1.5 Diagramm 2 (interaktiv) library(plotly) ggplotly(graph2) %&gt;% layout(showlegend = FALSE) 6.4.1.6 Diagramm 5 (statisch) vgl_pivot &lt;- vergleich %&gt;% group_by(vowel_ipa) %&gt;% pivot_longer(f1_l1:dauer_l2, names_to = &quot;category&quot;, values_to = &quot;value&quot;) %&gt;% separate(category, into = c(&quot;category&quot;, &quot;l1_l2&quot;)) %&gt;% drop_na() %&gt;% pivot_wider(names_from = category, values_from = value) vgl_pivot %&gt;% rmarkdown::paged_table() (graph5 &lt;- vgl_pivot %&gt;% drop_na() %&gt;% group_by(vowel_ipa, l1_l2) %&gt;% ggplot(aes(f2,f1)) + geom_hex(alpha = 0.2, show.legend = F) + geom_text(aes(label = vowel_ipa, color = vowel_ipa), size = 6, vjust = 1, hjust = 1, check_overlap = T, show.legend = F) + scale_y_reverse() + scale_x_reverse(breaks = c(1000, 1250, 1500, 1750, 2000, 2250, 2500)) + facet_wrap(~ l1_l2) + theme_light() + labs(y = &quot;Formant F1: tief &gt;&gt; hoch&quot;, x = &quot;Formant F2: &lt;&lt; vorne - hinten &gt;&gt;&quot;) ) ggsave(&quot;pictures/vergleich_vokalformanten.jpg&quot;) 6.4.1.7 Diagramm 5 (interaktiv) library(plotly) ggplotly(graph5) %&gt;% layout(showlegend = FALSE) 6.4.1.8 Diagramm 6 (statisch) vgl_pivot &lt;- vergleich %&gt;% group_by(vowel_ipa) %&gt;% pivot_longer(f1_l1:dauer_l2, names_to = &quot;category&quot;, values_to = &quot;value&quot;) %&gt;% separate(category, into = c(&quot;category&quot;, &quot;l1_l2&quot;)) %&gt;% drop_na() %&gt;% pivot_wider(names_from = category, values_from = value) vgl_pivot %&gt;% rmarkdown::paged_table() (graph6 &lt;- vgl_pivot %&gt;% drop_na() %&gt;% group_by(vowel_ipa, l1_l2, lange) %&gt;% ggplot(aes(f2,f1)) + geom_hex(alpha = 0.2, show.legend = F) + geom_text(aes(label = vowel_ipa, color = vowel_ipa), size = 6, vjust = 1, hjust = 1, check_overlap = T, show.legend = F) + scale_y_reverse() + scale_x_reverse(breaks = c(1000, 1250, 1500, 1750, 2000, 2250, 2500)) + facet_wrap(~ lange + l1_l2) + theme_light() + labs(y = &quot;Formant F1: tief &gt;&gt; hoch&quot;, x = &quot;Formant F2: &lt;&lt; vorne - hinten &gt;&gt;&quot;) ) ggsave(&quot;pictures/vergleich_vokalformanten_lang_kurz.jpg&quot;) 6.4.1.9 Diagramm 6 (interaktiv) library(plotly) ggplotly(graph6) %&gt;% layout(showlegend = FALSE) 6.4.2 Datensatz 3 6.4.2.1 Diagramm 3a alle Vokale (statisch) Messungen von TP mit Praat-Script ( Matt Winn: https://github.com/mwinn83) library(ggrepel) (graph3a &lt;- df %&gt;% filter(speaker != &quot;Monika1&quot;) %&gt;% group_by(vowel, speaker, vowel_ipa) %&gt;% summarise(f1 = mean(F1), f2 = mean(F2)) %&gt;% ggplot(aes(f2,f1)) + geom_hex(alpha = 0.2, show.legend = F) + geom_text(aes(label = vowel_ipa, color = vowel_ipa), vjust = 1, hjust = 1, check_overlap = T, show.legend = F, size = 5) + # geom_label_repel(aes(label = IPA, color = IPA), # vjust = 1, hjust = 1, check_overlap = T, show.legend = F, size = 5) + scale_y_reverse() + scale_x_reverse(breaks = c(1000, 1250, 1500, 1750, 2000, 2250, 2500)) + facet_wrap(~ speaker) + # theme_light() + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + labs(y = &quot;Formant F1: tief &gt;&gt; hoch&quot;, x = &quot;Formant F2: &lt;&lt; vorne - hinten &gt;&gt;&quot;) ) ggsave(&quot;pictures/messungen_tp_vokalformanten_ipa.jpg&quot;, dpi = 100, width = 10, height = 10) 6.4.2.2 Diagramm 3a alle Vokale (interaktiv) library(plotly) ggplotly(graph3a) %&gt;% layout(showlegend = FALSE) 6.4.2.3 Diagramm 3b kurze Vokale (statisch) library(ggrepel) (graph3b &lt;- df %&gt;% mutate(lange = ifelse( vowel %in% c(&quot;I&quot;,&quot;E&quot;,&quot;Y&quot;,&quot;OE&quot;,&quot;a&quot;,&quot;O&quot;,&quot;U&quot;), &quot;kurz&quot;,&quot;lang&quot;)) %&gt;% filter(speaker != &quot;Monika1&quot;) %&gt;% filter(lange == &quot;kurz&quot;) %&gt;% group_by(vowel, speaker, vowel_ipa) %&gt;% summarise(f1 = mean(F1), f2 = mean(F2)) %&gt;% ggplot(aes(f2,f1)) + geom_hex(alpha = 0.2, show.legend = F) + geom_text(aes(label = vowel_ipa, color = vowel_ipa), vjust = 1, hjust = 1, check_overlap = T, show.legend = F, size = 5) + # geom_label_repel(aes(label = IPA, color = IPA), # vjust = 1, hjust = 1, check_overlap = T, show.legend = F, size = 5) + scale_y_reverse() + scale_x_reverse(breaks = c(1000, 1250, 1500, 1750, 2000, 2250, 2500)) + facet_wrap(~ speaker) + # theme_light() + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + labs(y = &quot;Formant F1: tief &gt;&gt; hoch&quot;, x = &quot;Formant F2: &lt;&lt; vorne - hinten &gt;&gt;&quot;) ) ggsave(&quot;pictures/messungen_tp_vokalformanten_ipa_kurz.jpg&quot;, dpi = 100, width = 10, height = 10) 6.4.2.4 Diagramm 3b kurze Vokale (interaktiv) library(plotly) ggplotly(graph3b) %&gt;% layout(showlegend = FALSE) 6.4.2.5 Diagramm 3c lange Vokale (statisch) library(ggrepel) (graph3c &lt;- df %&gt;% mutate(lange = ifelse( vowel %in% c(&quot;I&quot;,&quot;E&quot;,&quot;Y&quot;,&quot;OE&quot;,&quot;a&quot;,&quot;O&quot;,&quot;U&quot;), &quot;kurz&quot;,&quot;lang&quot;)) %&gt;% filter(speaker != &quot;Monika1&quot;) %&gt;% filter(lange == &quot;lang&quot;) %&gt;% group_by(vowel, speaker, vowel_ipa) %&gt;% summarise(f1 = mean(F1), f2 = mean(F2)) %&gt;% ggplot(aes(f2,f1)) + geom_hex(alpha = 0.2, show.legend = F) + geom_text(aes(label = vowel_ipa, color = vowel_ipa), vjust = 1, hjust = 1, check_overlap = T, show.legend = F, size = 5) + # geom_label_repel(aes(label = IPA, color = IPA), # vjust = 1, hjust = 1, check_overlap = T, show.legend = F, size = 5) + scale_y_reverse() + scale_x_reverse(breaks = c(1000, 1250, 1500, 1750, 2000, 2250, 2500)) + facet_wrap(~ speaker) + # theme_light() + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + labs(y = &quot;Formant F1: tief &gt;&gt; hoch&quot;, x = &quot;Formant F2: &lt;&lt; vorne - hinten &gt;&gt;&quot;) ) ggsave(&quot;pictures/messungen_tp_vokalformanten_ipa_lang.jpg&quot;, dpi = 100, width = 10, height = 10) 6.4.2.6 Diagramm 3c lange Vokale (interaktiv) library(plotly) ggplotly(graph3c) %&gt;% layout(showlegend = FALSE) 6.5 Aggregierter Datensatz Grundlage ist Datensatz 1 6.5.1 IPA syms ipa_vow2 &lt;- c( a = &quot;\\u0061&quot;, AA = &quot;\\u0251\\u02D0&quot;, E = &quot;\\u025B&quot;, ee = &quot;\\u0065\\u02D0&quot;, EE = &quot;\\u025B\\u02D0&quot;, I = &quot;\\u026A&quot;, ii = &quot;\\u0069\\u02D0&quot;, O = &quot;\\u0254&quot;, OE = &quot;\\u0153&quot;, oo = &quot;\\u006F\\u02D0&quot;, oe = &quot;\\u00F8\\u02D0&quot;, U = &quot;\\u028A&quot;, Y = &quot;\\u028F&quot;, uu = &quot;\\u0075\\u02D0&quot;, yy = &quot;\\u0079\\u02D0&quot; ) ipavow2 = ipa_vow2 %&gt;% as_tibble() %&gt;% rename(vowel = value) 6.5.2 Tabelle Mittelwerte aus Datensatz 1 vokale_agg &lt;- vokale %&gt;% group_by(vokal, lange, l1_l2) %&gt;% summarise(f1_avg = mean(f1), f2_avg = mean(f2), dauer_avg = mean(dauer)) vokale_agg1 &lt;- vokale_agg %&gt;% filter(l1_l2 == &quot;L1&quot;) %&gt;% cbind(ipavow2) vokale_agg2 &lt;- vokale_agg %&gt;% filter(l1_l2 == &quot;L2&quot;) %&gt;% cbind(ipavow2) vokale_agg &lt;- rbind(vokale_agg1, vokale_agg2) %&gt;% as_tibble() vokale_agg %&gt;% rmarkdown::paged_table() 6.5.3 Datensatz 1 mit aggregierten Daten 6.5.3.1 Diagramm 4 (statisch) library(tidytext) library(ggrepel) (graph4 &lt;- vokale %&gt;% group_by(vokal, l1_l2, color = vowel_ipa, label = vowel_ipa, fill = vowel_ipa, shape = vowel_ipa) %&gt;% ggplot(aes(f2,f1)) + geom_hex(alpha = 0.2, show.legend = F) + geom_label(data = vokale_agg, label = vokale_agg$vowel, aes(x = f2_avg, y = f1_avg), color = &quot;black&quot;) + stat_ellipse(level = 0.67, geom = &quot;polygon&quot;, alpha = 0.2) + scale_color_discrete(breaks = c(&quot;a&quot;,&quot;a:&quot;,&quot;e:&quot;,&quot;E&quot;,&quot;E:&quot;,&quot;I&quot;,&quot;i:&quot;,&quot;O&quot;,&quot;o:&quot;,&quot;U&quot;,&quot;u:&quot;,&quot;Y&quot;,&quot;y:&quot;,&quot;Ö&quot;,&quot;ö&quot;)) + # geom_text(aes(label = vokal, color = vokal), vjust = 1, hjust = 1, check_overlap = T, show.legend = F) + scale_y_reverse() + scale_x_reverse() + facet_wrap(~ l1_l2) + theme_light() + guides(color = FALSE) + labs(y = &quot;Formant F1: tief &gt;&gt; hoch&quot;, x = &quot;Formant F2: &lt;&lt; vorne - hinten &gt;&gt;&quot;) ) 6.5.3.2 Diagramm 7 (statisch) (graph7 &lt;- vokale_agg %&gt;% group_by(vokal, l1_l2, label = vowel) %&gt;% ggplot(aes(f2_avg,f1_avg)) + geom_hex(alpha = 0.2, show.legend = F) + geom_text(aes(label = vowel, color = vowel), size = 6, vjust = 1, hjust = 1, check_overlap = T, show.legend = F) + scale_y_reverse() + scale_x_reverse() + facet_wrap(~ l1_l2) + theme_light() + labs(y = &quot;Formant F1: tief &gt;&gt; hoch&quot;, x = &quot;Formant F2: &lt;&lt; vorne - hinten &gt;&gt;&quot;) ) ggsave(&quot;pictures/vokalformanten.jpg&quot;) 6.5.3.3 Diagramm 5 (interaktiv) Interaktives Diagramm mit dem plotly-Programm: library(plotly) ggplotly(graph7) %&gt;% layout(showlegend = FALSE) 6.6 Abbildungen mit phonR Abbildungen mit dem phonR-Paket, spezialisiert auf die graphische Darstellung von akustischen, insbesondere phonetischen Daten. 6.6.0.1 Diagramm 8 Aggregierter Datensatz library(phonR) par(mfrow = c(1, 1)) with(vokale_agg, plotVowels(f1_avg, f2_avg, vowel, group = lange, pch.tokens = vowel, cex.tokens = 1.2, alpha.tokens = 0.3, plot.means = TRUE, pch.means = vowel, cex.means = 2, var.col.by = vowel, var.sty.by = lange, hull.fill = TRUE, hull.line = TRUE, fill.opacity = 0.1, pretty = TRUE)) # Bild auf Festplatte speichern in drei Schritten: # 1. Open jpeg file jpeg(&quot;pictures/phonR_vowel_space.jpg&quot;, width = 840, height = 535) # 2. Create the plot par(mfrow = c(1, 1)) with(vokale_agg, plotVowels(f1_avg, f2_avg, vowel, group = lange, pch.tokens = vowel, cex.tokens = 1.2, alpha.tokens = 0.3, plot.means = TRUE, pch.means = vowel, cex.means = 2, var.col.by = vowel, var.sty.by = lange, hull.fill = TRUE, hull.line = TRUE, fill.opacity = 0.1, pretty = TRUE)) # 3. Close the file dev.off() ## cairo_pdf ## 2 6.6.0.2 Diagramm 9 Einzelne Messungen library(phonR) par(mfrow = c(1, 1)) with(vokale, plotVowels(f1, f2, vowel_ipa, group = lange, pch.tokens = vowel_ipa, cex.tokens = 1.2, alpha.tokens = 0.3, plot.means = TRUE, pch.means = vowel_ipa, cex.means = 2, var.col.by = lange, var.sty.by = lange, hull.fill = TRUE, hull.line = TRUE, fill.opacity = 0.1, pretty = TRUE)) # Bild auf Festplatte speichern in drei Schritten: # 1. Open jpeg file jpeg(&quot;pictures/phonR_vowel_space2.jpg&quot;, width = 840, height = 535) # 2. Create the plot par(mfrow = c(1, 1)) with(vokale, plotVowels(f1, f2, vowel_ipa, group = lange, pch.tokens = vowel_ipa, cex.tokens = 1.2, alpha.tokens = 0.3, plot.means = TRUE, pch.means = vowel_ipa, cex.means = 2, var.col.by = lange, var.sty.by = lange, hull.fill = TRUE, hull.line = TRUE, fill.opacity = 0.1, pretty = TRUE)) # 3. Close the file dev.off() ## cairo_pdf ## 2 "],["textzerlegung.html", "Kapitel 7 Textzerlegung 7.1 Packages 7.2 Text öffnen 7.3 Zerlegung des Textes 7.4 Tabelle speichern 7.5 Tabelle öffnen 7.6 Zählen", " Kapitel 7 Textzerlegung 7.1 Packages library(tidyverse) library(tidytext) 7.2 Text öffnen In der R-Umgebung haben wir mehrere Möglichkeiten, Textdateien zu öffnen. Hier verwenden wir read_lines(), ein Programm, das mit dem Programmbündel tidyverse geladen wird. Bei langen Texten vermeiden wir es, den ganzen Text in den Arbeitsspeicher (RAM) zu laden, weil das lange dauern kann. Stattdessen verwenden wir Befehle wie substr() oder die tidyverse-Funktion str_sub(), um uns einen Teil des Textes anzuschauen. novels_r = read_lines(&quot;data/books/tom.txt&quot;) # Ansicht der ersten 300 Zeichen substr(novels_r, start = 1, stop = 300) ## [1] &quot;Tom Sawyer by Mark Twain Aligned by : András Farkas ( autoalignment ) Source : Project Gutenberg Die Abenteuer Tom Sawyers Mark Twain Vorwort des Autors . Die meisten der hier erzählten Abenteuer haben sich tatsächlich zugetragen . Das eine oder das andere habe ich selbst er&quot; # oder str_sub(novels_r, start = 1, end = 300) ## [1] &quot;Tom Sawyer by Mark Twain Aligned by : András Farkas ( autoalignment ) Source : Project Gutenberg Die Abenteuer Tom Sawyers Mark Twain Vorwort des Autors . Die meisten der hier erzählten Abenteuer haben sich tatsächlich zugetragen . Das eine oder das andere habe ich selbst er&quot; 7.3 Zerlegung des Textes 7.3.1 Tabelle Um Texteinheiten wie z.B. Wörter, Buchstaben oder Sätze / Äußerungen zählen zu können, müssen wir den Text zuerst in kleinere Teile zerlegen, in der Fachsprache als Tokenisierung bekannt (tokens extrahieren). Für die Tokenisierung von Zeichenfolgen (Texten) gibt es in der R-Sprache viele Programme (z.B. quanteda, tidytext) und verschiedene Möglichkeiten (mit ihren Vor- und Nachteilen). Hier verwenden wir das zuletzt genannte Programm und seine Funktion unnest_tokens(). Im ersten Schritt wandeln wir die Textdatei in einen Datensatz um, so dass wir mit den gut verständlichen tidyverse-Funktionen arbeiten können. Das machen wir mit der Funktion as_tibble(), die in diesem Fall eine Tabelle mit einer einzigen Spalte erzeugt. Der Default-Name der Spalte value wird mit rename() umbenannt. Der neue (bessere) Spaltenname ist text, was wir mit colnames() überprüfen können. Wir geben dieser Tabelle einen neuen Namen (novels_df). novels_df = as_tibble(novels_r) %&gt;% rename(text = value) colnames(novels_df) ## [1] &quot;text&quot; 7.3.2 Äußerungen Im zweiten Schritt verwenden wir die Tabelle novels_df als Ausgangspunkt für die Tokenisierung. Wir zerlegen die Zeichenfolge in der Spalte text in Sätze (genau genommen: Äußerungen), und zwar mit Hilfe der Funktion unnest_tokens() des Programms tidytext. Wir geben dieser Tabelle einen neuen Namen, und zwar novels_sents. Die Funktion unnest_tokens verlangt die folgende Reihenfolge: zuerst muss der Datensatz angegeben werden (hier: novels_df), dann der Name der Tabellenspalte, die in kleinere Einheiten zerlegt werden soll (hier: text), gefolgt von der Angabe, in welche Einheit wir die Zeichenfolge umformen wollen (hier: sentences). Die neue Tabelle enthält nur eine Spalte (nur die Äußerungen), da wir die Option drop = TRUE beibehalten (denn sonst würde die neue Tabelle eine weitere Spalte mit dem ganzen Text enthalten, und zwar in jeder Tabellenzeile - sehr belastend für den Arbeitsspeicher!). Außerdem verhindern wir mit to_lower = FALSE, dass alle Wörter in der Spalte klein geschrieben werden. Die im Deutschen übliche Groß- und Kleinschreibung von Wörtern soll in dieser Spalte beibehalten werden. Zuletzt entfernen wir noch die erste Zeile in der Tabelle, da sie nicht zum Text gehört. Das machen wir mit novels_sents[-1,]: dem Namen der Tabelle folgen eckige Klammern; die Zahl -1 vor dem Komma bezieht sich auf die erste Tabellenzeile; das Minuszeichen vor der Zahl besagt, dass diese Zeile entfernt werden soll; nach dem Komma steht nichts, d.h. alle Spalten in der Tabelle bleiben unverändert. Zur Identifizierung der Reihenfolge der Äußerungen fügen wir der Tabelle noch eine Spalte hinzu: mit mutate() erzeugen wir die Spalte sentence_id, und zwar indem wir die einzelnen Zeilenzahlen row_number() mit Hilfe der Anzahl der Tabellenzeilen length() festlegen. novels_sents = novels_df %&gt;% unnest_tokens(sentence, text, token = &quot;sentences&quot;, drop = TRUE, to_lower = FALSE) novels_sents = novels_sents[-1,] novels_sents = novels_sents %&gt;% mutate(sentence_id = row_number(1:length(novels_sents$sentence))) head(novels_sents) ## # A tibble: 6 x 2 ## sentence sentence_id ## &lt;chr&gt; &lt;int&gt; ## 1 Die meisten der hier erzählten Abenteuer haben sich tatsächlich z~ 1 ## 2 Das eine oder das andere habe ich selbst erlebt , die anderen mei~ 2 ## 3 Huck Finn ist nach dem Leben gezeichnet , nicht weniger Tom Sawye~ 3 ## 4 Ich muß hier bemerken , daß zur Zeit meiner Erzählung -- vor drei~ 4 ## 5 Obwohl dies Buch vor allem zur Unterhaltung der kleinen Welt gesc~ 5 ## 6 Erstes Kapitel . , ,Tom ! 6 tail(novels_sents) ## # A tibble: 6 x 2 ## sentence sentence_id ## &lt;chr&gt; &lt;int&gt; ## 1 &quot;\\&quot; Schluß .&quot; 4679 ## 2 &quot;So endet diese Geschichte .&quot; 4680 ## 3 &quot;Da es nur die Geschichte eines Jungen sein soll , muß sie hier e~ 4681 ## 4 &quot;Wenn jemand eine Erzählung über erwachsene Leute schreibt , weiß~ 4682 ## 5 &quot;Die meisten der in diesem Buch vorkommenden Personen leben noch ~ 4683 ## 6 &quot;Vielleicht erscheint es eines Tages als angebracht , die Geschic~ 4684 7.3.3 Wörter Unser nächstes Ziel ist die Zerlegung der Äußerungen in kleinere Einheiten: Wörter. Das können wir auf ähnliche Weise bewerkstelligen wie im vorherigen Schritt. Ausgangspunkt ist nun die Tabelle novels_sents. Im unnest_tokens()-Befehl geben wir den Namen der hier gewünschten Einheit an (word), dann den Namen der Tabellenspalte mit den Äußerungen (sentence) und die gewünschte Einheit, in die wir die Äußerungen umformen möchten (words). Außerdem wollen wir die Tabellenspalte sentence beibehalten, weshalb wir drop = FALSE auswählen. In der neuen Tabellenspalte word sollen alle Wörter klein geschrieben werden, was wir mit to_lower = TRUE erreichen. Wir fügen der Tabelle (ähnlich wie im Fall der sentence_id) auch eine weitere Spalte hinzu, der wir die ursprüngliche Reihenfolge der Wörter entnehmen können: word_id. novels_words = novels_sents %&gt;% unnest_tokens(word, sentence, token = &quot;words&quot;, drop = FALSE, to_lower = TRUE) novels_words = novels_words %&gt;% mutate(word_id = row_number(1:length(novels_words$word))) Bei der Durchsicht des Textes oder auch der Tabelle fallen ungewohnte Wortformen mit einem Unterstrich - auf. Wir entfernen den Unterstrich am Anfang und Ende der Wortformen. novels_words = novels_words %&gt;% mutate(word = str_replace_all(word, &quot;_&quot;, &quot;&quot;)) head(novels_words) ## # A tibble: 6 x 4 ## sentence sentence_id word word_id ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Die meisten der hier erzählten Abenteuer haben sich~ 1 die 1 ## 2 Die meisten der hier erzählten Abenteuer haben sich~ 1 meis~ 2 ## 3 Die meisten der hier erzählten Abenteuer haben sich~ 1 der 3 ## 4 Die meisten der hier erzählten Abenteuer haben sich~ 1 hier 4 ## 5 Die meisten der hier erzählten Abenteuer haben sich~ 1 erzä~ 5 ## 6 Die meisten der hier erzählten Abenteuer haben sich~ 1 aben~ 6 7.3.4 Buchstaben Wenn wir auch Buchstaben zählen wollen, können wir die Wörter auf ähnliche Weise zerlegen und eine char_id (ursprüngliche Reihenfolge der Buchstaben) hinzufügen. novels_ch = novels_words %&gt;% unnest_tokens(char, word, token = &quot;characters&quot;, drop = FALSE, to_lower = TRUE) novels_ch = novels_ch %&gt;% mutate(char_id = row_number(1:length(novels_ch$char))) head(novels_ch) ## # A tibble: 6 x 6 ## sentence sentence_id word word_id char char_id ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Die meisten der hier erzählten Abente~ 1 die 1 d 1 ## 2 Die meisten der hier erzählten Abente~ 1 die 1 i 2 ## 3 Die meisten der hier erzählten Abente~ 1 die 1 e 3 ## 4 Die meisten der hier erzählten Abente~ 1 meis~ 2 m 4 ## 5 Die meisten der hier erzählten Abente~ 1 meis~ 2 e 5 ## 6 Die meisten der hier erzählten Abente~ 1 meis~ 2 i 6 7.3.5 Ziffern umwandeln Unter den tokens (Vorkommnisse von Wortformen) sind auch Ziffern. Da sie im Text nicht wie Wörter geschrieben werden, zählen wir sie gewöhnlich nicht mit. Wir könnten die Ziffern aus der Tabelle entfernen oder als unbestimmte Ziffernfolge (z.B. 0000) zusammenfassen. Entfernen ist die einfachere Möglichkeit und oft auch die sinnvollere, wenn man davon ausgeht, dass die Zahlen keine wichtige Information zum Gesamtext beitragen. Wir erhalten 29 leere Zeilen. novels_ch %&gt;% mutate(word = str_remove_all(word, &quot;[^a-zA-Z]&quot;)) %&gt;% count(word) ## # A tibble: 9,603 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 &quot;&quot; 29 ## 2 &quot;a&quot; 15 ## 3 &quot;ab&quot; 60 ## 4 &quot;abbekommen&quot; 10 ## 5 &quot;abbildung&quot; 18 ## 6 &quot;abbrach&quot; 7 ## 7 &quot;abend&quot; 60 ## 8 &quot;abendessen&quot; 20 ## 9 &quot;abendessens&quot; 11 ## 10 &quot;abendgebet&quot; 10 ## # ... with 9,593 more rows Ersetzen ist die andere Möglichkeit. Dazu verwenden wir einen regulären Ausdruck, nämlich [\\\\d], in dem das d sich auf digit (Ziffer) bezieht. Die beiden Backslash-Zeichen vor dem d sagen dem Programm, dass nicht nach dem Buchstaben d gesucht wird, sondern nach Ziffern (digits). Als Ergebnis erhalten wir ein-, zwei- und vierstellige Zahlen in der Wortspalte. Die vierstelligen Zahlen stellen wahrscheinlich Jahreszahlen dar. novels_ch %&gt;% mutate(word = str_replace_all(word, &quot;[\\\\d]&quot;, &quot;0&quot;)) %&gt;% count(word) ## # A tibble: 9,641 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 0 2 ## 2 00 12 ## 3 0000 4 ## 4 a 11 ## 5 ä 11 ## 6 ab 60 ## 7 abbekommen 10 ## 8 abbildung 18 ## 9 abbrach 7 ## 10 abend 60 ## # ... with 9,631 more rows Wir wählen die letzte Variante und speichern die Tabelle: eine oder mehrere Ziffern [\\\\d]+ werden mit vier Nullen 0000 ersetzt. Diese 0000 kann als Wortform mitgezhlt werden (oder nicht). novels_ch = novels_ch %&gt;% mutate(word = str_replace(word, &quot;[\\\\d]+&quot;, &quot;0000&quot;)) 7.4 Tabelle speichern Diese Tabelle speichern wir als Datei, falls wir ein anderes Mal mit ihr arbeiten möchten. Wir speichern die Tabelle als Excel-Datei (Programm: writexl) und als Tabelle im Textformat (Endung: csv, comma-separated-variables). Excel hat einen Nachteil: falls die Zeilen sehr lang sind, werden sie nicht vollständig gespeichert, sondern abgekürzt. Das Dateiformat csv und andere Textformate sind daher besser. Außerdem kann man csv-Dateien, da sie nur unformatierten Text enthalten, mit jedem Programm öffnen und lesen, Excel-Dateien dagegen nicht. write_csv(novels_ch, &quot;data/tom_sawyer_tabelle.csv&quot;) # install.packages(&quot;writexl&quot;) library(writexl) write_xlsx(novels_ch, &quot;data/tom_sawyer_tabelle.xlsx&quot;) 7.5 Tabelle öffnen Falls wir die Arbeit an einem anderen Tag fortsetzen, können wir die gespeicherte Tabelle folgendermaßen öffnen und brauchen nicht noch einmal alle oben durchgeführten Schritte durchzuführen: # install.packages(&quot;readxl&quot;) library(readxl) novels_ch = read_xlsx(&quot;data/tom_sawyer_tabelle.xlsx&quot;) # oder: novels_ch = read_csv(&quot;data/tom_sawyer_tabelle.csv&quot;) 7.6 Zählen Als Ausgangspunkt für die Auszählung von Äußerungen, Wörtern und Buchstaben verwenden wir die zuletzt erstellte Tabelle novels_ch. Zum Zählen einer Kategorie eignet sich die tidyverse-Funktion count(). Wie viele Äußerungen hat das Programm identifiziert? Hier müssen wir berücksichtigen, dass jede Äußerung in mehreren Zeilen wiederholt wird, da wir die Äußerungen auch in Wörter und Buchstaben zerlegt haben. Die Funktion distinct() berücksichtigt das. novels_ch %&gt;% distinct(sentence) %&gt;% count(sentence) ## # A tibble: 4,647 x 2 ## sentence n ## &lt;chr&gt; &lt;int&gt; ## 1 &#39; Da hab &#39; ich ihm einen Stein ins Fenster geschmissen -- aber s~ 1 ## 2 &#39; Das heilt _jede_ Warze . 1 ## 3 &#39; Und du sagst , so ganz beiläufig , als wenn&#39;s nichts wär &#39; , d~ 1 ## 4 &#39; Du sagtest das immer wieder . 1 ## 5 &#39; Sagen -- was ? 1 ## 6 &#39; Und dann du raus mit den Pfeifen und wir ordentlich drauf los ,~ 1 ## 7 &#39; und dann trittst du langsam zurück , elf Schritt , mit geschlos~ 1 ## 8 &#39; Und ich sag &#39; : , O , &#39;s ist schon recht , wenn er uns stark genug~ 1 ## 9 &#39;n ganzes Jahr werd &#39; ich jede Nacht wachen ! 1 ## 10 &#39;ne Schande ! 1 ## # ... with 4,637 more rows Als Antwort auf diese Abfrage erhalten wir 4647 Zeilen, d.h. dass 4647 Äußerungen (number_sents) unterschieden wurden. Außerdem entsteht beim Zählen eine neue Spalte, und zwar mit dem Namen n. Die Zahl ist in allen Tabellenzeilen dieselbe, nämlich 1. Dies zeigt uns, dass jede Äußerung nur einmal gezählt wurde (also distinkt / unterschiedlich ist), so wie wir es ja verlangt haben. novels_ch %&gt;% distinct(sentence) %&gt;% count(sentence) %&gt;% summarise(number_sents = sum(n)) ## # A tibble: 1 x 1 ## number_sents ## &lt;int&gt; ## 1 4647 Wenn wir distinct() weglassen, erhalten wir das folgende Ergebnis: Die Tabellenspalte n enthält nun unterschiedliche Zahlen, die sich auf die Anzahl der Buchstaben in der Spalte sentence bezieht, denn wir haben die Äußerungen vorher nicht nur in Wörter (Spalte word) sondern auch in Buchstaben (Spalte char) zerlegt. novels_ch %&gt;% count(sentence) ## # A tibble: 4,647 x 2 ## sentence n ## &lt;chr&gt; &lt;int&gt; ## 1 &#39; Da hab &#39; ich ihm einen Stein ins Fenster geschmissen -- aber s~ 61 ## 2 &#39; Das heilt _jede_ Warze . 17 ## 3 &#39; Und du sagst , so ganz beiläufig , als wenn&#39;s nichts wär &#39; , d~ 115 ## 4 &#39; Du sagtest das immer wieder . 23 ## 5 &#39; Sagen -- was ? 8 ## 6 &#39; Und dann du raus mit den Pfeifen und wir ordentlich drauf los ,~ 83 ## 7 &#39; und dann trittst du langsam zurück , elf Schritt , mit geschlos~ 131 ## 8 &#39; Und ich sag &#39; : , O , &#39;s ist schon recht , wenn er uns stark genug~ 46 ## 9 &#39;n ganzes Jahr werd &#39; ich jede Nacht wachen ! 33 ## 10 &#39;ne Schande ! 9 ## # ... with 4,637 more rows Auf diese Weise könnten wir die Anzahl der Buchstaben insgesamt und die Anzahl der Buchstaben pro Äußerung berechnen. 7.6.1 Buchstaben Wie lang ist der Text, gemessen in characters? Fast 333 Tausend alphanumerische Zeichen. Wie viele alphanumerische Zeichen (Buchstaben und Ziffern) kommen durchschnittlich pro Äußerung vor? Beinahe 72 Zeichen. Die Standardabweichung von diesem Mittelwert ist ziemlich groß: 71,5 +/- 61,9 Zeichen. In diesem Text kommen sehr kurze Äußerungen vor, aber auch deutlich längere. (char_summary = novels_ch %&gt;% count(sentence) %&gt;% summarise(char_sum = sum(n), char_avg = mean(n, na.rm = TRUE), char_sd = sd(n, na.rm = TRUE)) ) ## # A tibble: 1 x 3 ## char_sum char_avg char_sd ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 332677 71.6 61.9 Kurze Äußerungen (hier: mit wenigen Buchstaben) kommen sehr häufig vor, sehr lange (hier: mit vielen Buchstaben) dagegen selten. novels_ch %&gt;% count(sentence) %&gt;% ggplot(aes(n)) + geom_density(fill = &quot;magenta&quot;, alpha = 0.7) + labs(x = &quot;Anzahl der Buchstaben&quot;, y = &quot;Dichte&quot;) Folgt die Äußerungslänge (in Buchstaben gemessen) einer Zipf-Verteilung? alpha = 1 novels_ch %&gt;% count(sentence, sort = TRUE) %&gt;% # Sortieren nicht vergessen! mutate(rank = row_number(), zipfs_freq = ifelse(rank == 1, n, dplyr::first(n) / rank^alpha)) %&gt;% ggplot(aes(rank, n)) + geom_point() + labs(y = &quot;Anzahl der Buchstaben&quot;, x = &quot;Rangfolge&quot;) Im Vergleich dazu unten: so sähe die entsprechende Zipf-Verteilung (Power-Law-Distribution) aus, die für ausreichend lange Texte gültig ist. In einer Zipfverteilung käme die zweithäufigste Äußerungslänge etwa nur noch halb so häufig vor wie die häufigste Äußerungslänge, die dritthäufigste nur noch ein Drittel der ersthäufigsten, die vierthäufigste nur noch ein Viertel der ersthäufigsten usw. alpha = 1 novels_ch %&gt;% count(sentence, sort = TRUE) %&gt;% # Sortieren nicht vergessen! # mutate(rank = row_number(), mutate(rank = rank(row_number(), ties.method = &quot;average&quot;), zipfs_freq = ifelse(rank == 1, n, dplyr::first(n) / rank^alpha)) %&gt;% ggplot(aes(rank, zipfs_freq)) + geom_point() + labs(y = &quot;Anzahl der Buchstaben&quot;, x = &quot;Rangfolge&quot;) Wenn man die beiden Variablen (Rang und Häufigkeit) in einer Zipfverteilung logarithmiert, ist das Ergebnis eine Gerade. Die Äußerungslänge (hier: Buchstabenanzahl) im Roman Tom Sawyer scheint davon abzuweichen. Die mittellangen Äußerungen kommen häufiger vor, als nach der Zipfverteilung erwartet wird. library(scales) alpha = 1 novels_ch %&gt;% count(sentence, sort = TRUE) %&gt;% # sortieren ! mutate(rank = rank(row_number(), ties.method = &quot;first&quot;), zipfs_freq = ifelse(rank == 1, n, dplyr::first(n) / rank^alpha)) %&gt;% ggplot(aes((n), (rank), color = &quot;Roman&quot;)) + geom_point() + geom_point(aes(((zipfs_freq)), ((rank)), color = &quot;theoretisch&quot;)) + labs(x = &quot;Anzahl der Buchstaben&quot;, y = &quot;Rangfolge&quot;, color = &quot;Verteilung&quot;) + scale_x_log10() + scale_y_log10() Welche Buchstaben kommen im Text häufiger / seltener vor? Unter den auf Vokale bezogenen Buchstaben ist das e am häufigsten, unter den auf Konsonanten bezogenen das n. alpha = 1 novels_ch %&gt;% filter(str_detect(char, &quot;[:alpha:]&quot;)) %&gt;% count(char, sort = T) ## # A tibble: 31 x 2 ## char n ## &lt;chr&gt; &lt;int&gt; ## 1 e 53778 ## 2 n 33906 ## 3 i 24336 ## 4 r 22040 ## 5 s 20912 ## 6 t 20295 ## 7 a 18994 ## 8 h 18505 ## 9 d 16432 ## 10 u 13668 ## # ... with 21 more rows novels_ch %&gt;% filter(str_detect(char, &quot;[:alpha:]&quot;)) %&gt;% count(char, sort = T) %&gt;% mutate(char = fct_reorder(char, n)) %&gt;% ggplot(aes(n, char, fill = char)) + geom_col() + theme(legend.position = &quot;none&quot;) novels_ch %&gt;% filter(str_detect(char, &quot;[:alpha:]&quot;)) %&gt;% count(char, sort = T) %&gt;% # sortieren ! mutate(rank = rank(row_number(), ties.method = &quot;first&quot;), zipfs_freq = ifelse(rank == 1, n, dplyr::first(n) / rank^alpha)) %&gt;% ggplot(aes((n), (rank), color = &quot;Roman&quot;)) + geom_point() + geom_point(aes(((zipfs_freq)), ((rank)), color = &quot;theoretisch&quot;)) + labs(x = &quot;Anzahl der Buchstaben&quot;, y = &quot;Rangfolge&quot;, color = &quot;Verteilung&quot;) + scale_x_log10() + scale_y_log10() 7.6.2 Worthäufigkeiten Da wir es gewohnt sind, mit Wörtern umzugehen, machen wir lieber mit Wörtern weiter. Wie viele Wortformen wurden vom Programm unterschieden? Die folgende Wortauszählung zeigt, dass das Programm 9639 types (Wortformen) unterschieden hat, denn die Tabelle hat so viele Zeilen (0000 sind die oben umgewandelten Ziffern). Funktionswörter (z.B. und, die, nicht, sie, der, sich, er, ich, aber, ) kommen wie in allen Texten am häufigsten vor. Für die Inhaltsanalyse spielen sie kaum oder gar keine Rolle. Daher werden sie oft entfernt. Zu den Top-Ten gehört verständlicherweise auch der Name Tom, da die Hauptperson im Roman fortwährend angesprochen oder erwähnt wird (die Gebrauchsfrequenz oder Anzahl der tokens n = 738). novels_ch %&gt;% distinct(word_id, .keep_all = TRUE) %&gt;% count(word, sort = TRUE) ## # A tibble: 9,639 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 und 2641 ## 2 die 1417 ## 3 er 1218 ## 4 sie 1189 ## 5 der 1178 ## 6 zu 992 ## 7 sich 824 ## 8 nicht 802 ## 9 ich 739 ## 10 tom 738 ## # ... with 9,629 more rows Folgen die Worthäufigkeiten einer Zipfverteilung (Power-Law-Distribution)? In der folgenden Tabelle sind folgende Größen zu sehen: die Gebrauchshäufigkeit oder Tokenfrequenz eines Wortes im Text (n) die Rangfolge des Wortes (rank), die berechnete Konstante (k = frequenz \\* rang), die berechnete theoretische Tokenfrequenz eines Wortes (zips_freq). novels_ch %&gt;% distinct(word_id, .keep_all = TRUE) %&gt;% count(word, sort = TRUE) %&gt;% # sortieren !!! mutate(rank = rank(row_number(), ties.method = &quot;average&quot;), k = n * rank, zipfs_freq = ifelse(rank == 1, n, dplyr::first(n) / rank^alpha), zipfs_freq_k = ifelse(rank == 1, k, dplyr::first(k) / rank^alpha)) ## # A tibble: 9,639 x 6 ## word n rank k zipfs_freq zipfs_freq_k ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 und 2641 1 2641 2641 2641 ## 2 die 1417 2 2834 1320. 1320. ## 3 er 1218 3 3654 880. 880. ## 4 sie 1189 4 4756 660. 660. ## 5 der 1178 5 5890 528. 528. ## 6 zu 992 6 5952 440. 440. ## 7 sich 824 7 5768 377. 377. ## 8 nicht 802 8 6416 330. 330. ## 9 ich 739 9 6651 293. 293. ## 10 tom 738 10 7380 264. 264. ## # ... with 9,629 more rows Die häufigeren Wörter scheinen in Tom Sawyer häufiger vorzukommen als nach der Zipfverteilung erwartet wird. Das spricht für einen simpleren Wortschatz, einen eher umgangssprachlich formulierten Text. novels_ch %&gt;% distinct(word_id, .keep_all = TRUE) %&gt;% count(word, sort = TRUE) %&gt;% # sortieren !!! mutate(rank = rank(row_number(), ties.method = &quot;average&quot;), zipfs_freq = ifelse(rank == 1, n, dplyr::first(n) / rank^alpha)) %&gt;% ggplot(aes((n), (rank), color = &quot;Roman&quot;)) + geom_point() + geom_point(aes(((zipfs_freq)), ((rank)), color = &quot;theoretisch&quot;)) + labs(x = &quot;Worthäufigkeit&quot;, y = &quot;Rangfolge&quot;, color = &quot;Verteilung&quot;) + scale_x_log10() + scale_y_log10() 7.6.3 Wortschatzdichte Die lexikalische Diversität oder Wortschatzdichte, gemessen anhand des type-token-Verhältnisses (type-token-ratio, ttr) beträgt 0,14 Types pro Token. Ohne mit einem anderen Text vergleichen zu können, sagt uns dieser Wert nicht viel. Dieser Wert ist ansonsnten auch von der Länge eines Textes abhängig: je länger der Text, desto kleiner wird dieser Wert. novels_ch %&gt;% distinct(word_id, .keep_all = TRUE) %&gt;% count(word, sort = TRUE) %&gt;% summarise(types = length(word), tokens = sum(n), ttr = length(word)/sum(n)) ## # A tibble: 1 x 3 ## types tokens ttr ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 9639 67115 0.144 Auch die lexikalische Diversität (Wortschatzdichte) ist relativ gering, da die Anzahl der verschiedenen Wortformen (types) im Vergleich zu ihrer Gebrauchsfrequenz (tokens) verhältnismäßig klein ist. Dies spricht wiederum für die These, dass der Roman einen recht einfachen Wortschatz aufweist, was einem eher umgangssprachlichen Stil entspricht. 7.6.4 Äußerungslänge Wie viele Wörter kommen durchschnittlich in einer Äußerung vor? Da wir bei der Auszählung auch einige der als 0000 kodierten Ziffern berücksichtigen, beträgt die durchschnittliche Länge der Äußerungen im Text 14,44 Wortformen (tokens), die Standardabweichung vom Mittelwert ist fast so groß wie der Mittelwert: 14,44 +/- 11,95 Wortformen. novels_ch %&gt;% distinct(word_id, .keep_all = T) %&gt;% count(sentence) %&gt;% summarise(leng_sents = mean(n), sd_sents = sd(n)) ## # A tibble: 1 x 2 ## leng_sents sd_sents ## &lt;dbl&gt; &lt;dbl&gt; ## 1 14.4 12.0 Die Häufigkeitsverteilung der Äußerungslänge im Roman: novels_ch %&gt;% distinct(word_id, .keep_all = TRUE) %&gt;% count(sentence, sort = TRUE) %&gt;% # sortieren !!! mutate(rank = as.integer(length(sentence) - rank(n, ties.method = &quot;average&quot;) + 1), rank2 = rank(row_number(), ties.method = &quot;average&quot;), zipfs_freq = ifelse(rank == 1, n, dplyr::first(n) / rank^alpha)) %&gt;% ggplot(aes(n)) + geom_histogram(fill = &quot;darkgreen&quot;, alpha = 0.7, binwidth = 2) + labs(x = &quot;Anzahl der Wörter&quot;, y = &quot;Anzahl der Äußerungen&quot;) Logarithmiert: novels_ch %&gt;% distinct(word_id, .keep_all = TRUE) %&gt;% count(sentence, sort = TRUE) %&gt;% # sortieren !!! mutate(rank = as.integer(length(sentence) - rank(n, ties.method = &quot;average&quot;) + 1), rank2 = rank(row_number(), ties.method = &quot;average&quot;), zipfs_freq = ifelse(rank == 1, n, dplyr::first(n) / rank^alpha)) %&gt;% ggplot(aes(n)) + geom_histogram(fill = &quot;darkgreen&quot;, alpha = 0.7, binwidth = 0.05) + scale_x_log10() + labs(x = &quot;Anzahl der Wörter&quot;, y = &quot;Anzahl der Äußerungen&quot;) Tabelle mit Anzahl der Wörter pro Äußerung (n), Rangfolge (rank) und die theoretische Frequenz gemäß Zipfverteilung (zipfs_freq). novels_ch %&gt;% distinct(word_id, .keep_all = TRUE) %&gt;% count(sentence, sort = TRUE) %&gt;% # sortieren !!! mutate(rank = as.integer(length(sentence) - rank(n, ties.method = &quot;average&quot;) + 1), rank2 = rank(row_number(), ties.method = &quot;average&quot;), zipfs_freq = ifelse(rank == 1, n, dplyr::first(n) / rank^alpha)) %&gt;% arrange(rank) %&gt;% rmarkdown::paged_table() Die Verteilung der Äußerungslängen unterscheidet sich von der Zipfverteilung. alpha = 1 novels_ch %&gt;% distinct(word_id, .keep_all = TRUE) %&gt;% count(sentence, sort = F) %&gt;% # sortieren !!! mutate(rank = as.integer(length(sentence) - rank(n, ties.method = &quot;average&quot;) + 1), zipfs_freq = ifelse(rank == 1, n, dplyr::first(n) / rank^alpha)) %&gt;% ggplot(aes((n), (rank), color = &quot;Roman&quot;)) + geom_point() + geom_point(aes(((zipfs_freq)), ((rank)), color = &quot;theoretisch&quot;)) + geom_jitter() + labs(x = &quot;WAnzahl der Wörter&quot;, y = &quot;Rangfolge&quot;, color = &quot;Verteilung&quot;) + scale_x_log10() + scale_y_log10() 7.6.5 Vergleich mit quanteda library(quanteda) library(quanteda.textstats) Die Zahlenwerte, die wir mit dem Program quanteda (ohne genauere Bearbeitung) erhalten, sind höher. Das liegt daran, dass Interpunktionszeichen, Symbole und Ziffern noch nicht herausgefiltert sind. # ohne Filterung corp = corpus(novels_r) textstat_summary(corp) %&gt;% rmarkdown::paged_table() Gefiltert sind die Werte mit den Werten vergleichbar, die wir mit den tidyverse-Funktionen berechnet haben. # gefiltert tok = tokens(corp, remove_punct = T, remove_symbols = T, remove_numbers = T, remove_url = T, remove_separators = T) textstat_summary(tok) %&gt;% rmarkdown::paged_table() Zahlenwerte für die einzelnen Äußerungen: # ungefiltert corps = corpus_reshape(corp, to = &quot;sentences&quot;) textstat_summary(corps) %&gt;% rmarkdown::paged_table() # gefiltert toks = tokens(corps, remove_punct = T, remove_symbols = T, remove_numbers = T, remove_url = T, remove_separators = T) textstat_summary(toks) %&gt;% rmarkdown::paged_table() Die von quanteda berechnete durchschnittliche Äußerungslänge und ihre Standardabweichung weisen ähnliche Werte auf (Mittelwert = 14,41, Standardabweichung = 11,998) wie die zuvor berechneten mit den tidyverse-Funktionen. textstat_summary(toks) %&gt;% summarise(len_sents_q = mean(tokens), len_sents_q_sd = sd(tokens)) ## len_sents_q len_sents_q_sd ## 1 14.41251 11.99817 Lexikalische Diversität (Wortschatzdichte) des gesamten Texts liegt bei 0,1467 (ähnlich wie mit den tidyverse-Funktionen). textstat_lexdiv(tok) ## document TTR ## 1 text1 0.146718 Wortschatzdichte in einzelnen Äußerungen: textstat_lexdiv(toks) %&gt;% rmarkdown::paged_table() corp_dfm &lt;- dfm(tok, verbose = FALSE) term_count &lt;- tidy(corp_dfm) %&gt;% group_by(document) %&gt;% arrange(desc(count)) # sortieren !!! term_count ## # A tibble: 9,837 x 3 ## # Groups: document [1] ## document term count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 text1 und 2641 ## 2 text1 die 1414 ## 3 text1 er 1210 ## 4 text1 sie 1180 ## 5 text1 der 1174 ## 6 text1 zu 986 ## 7 text1 sich 824 ## 8 text1 nicht 786 ## 9 text1 tom 739 ## 10 text1 ich 730 ## # ... with 9,827 more rows term_count_rank &lt;- tidy(corp_dfm) %&gt;% group_by(document) %&gt;% arrange(desc(count)) %&gt;% mutate(rank = row_number(), total = sum(count), `term frequency` = count / total) term_count_rank %&gt;% rmarkdown::paged_table() term_count_rank %&gt;% ggplot(aes(rank, `term frequency`, color = document)) + geom_line(alpha = 0.8, show.legend = FALSE) + scale_x_log10() + scale_y_log10() alpha = 1 term_count_rank %&gt;% mutate(rank = as.integer(rank(length(term) - `term frequency` + 1, ties.method = &quot;average&quot;)), zipfs_freq = ifelse(rank == 1, `term frequency`, dplyr::first(`term frequency`) / rank^alpha)) %&gt;% ggplot(aes(rank, `term frequency`, color = document)) + geom_line(alpha = 0.8, show.legend = FALSE) + geom_point(aes((rank), (zipfs_freq), color = &quot;theoretisch&quot;)) + geom_jitter() + labs(y = &quot;Termfrequenz&quot;, x = &quot;Rangfolge&quot;, color = &quot;Verteilung&quot;) + scale_x_log10() + scale_y_log10() 7.6.6 Vergleich mit Voyant Tools Voyant Tools sind ein empfehlenwertes Werkzeug, schnell und einfach, um einen Überblick und verständliche graphische Darstellungen zu erhalten - solange man es nur mit einem Text oder wenigen zu tun hat. Die deutsche Übersetzung des Tools ist stellenweise ungenau: z.B. wird in der deutschen Übersetzung nicht zwischen types (einzigartigen Wortformen / unique word forms) und tokens (total words) unterschieden. Falsch: Dieser Korpus hat 1 Dokument mit 67,137 einzigartige Wortformen. Richtig ist: Dieses Korpus hat 67,137 tokens (Vorkommnisse von Wortformen) und 9828 types (unique word forms). Wortschatzdichte: 0.146 Durchschnittliche Wörter pro Satz: 15.5 (höher als mit tidyverse und quanteda berechnet) Die häufigsten Begriffe im Korpustom (739); s (296); huck (237); na (175); joe (150) This corpus has 1 document with 67,137 total words and 9,828 unique word forms. Vocabulary Density: 0.146 Average Words Per Sentence: 15.5 Most frequent words in the corpus: tom (739); s (296); huck (237); na (175); joe (150) "],["kurzgeschichten-im-vergleich.html", "Kapitel 8 Kurzgeschichten im Vergleich 8.1 Programme laden 8.2 Texte laden 8.3 Textzerlegung und Annotation 8.4 Datensätze vereinen 8.5 Wortklassen zählen 8.6 Nicht-parametrischer Test 8.7 Konnektoren 8.8 Äußerungslänge vergleichen", " Kapitel 8 Kurzgeschichten im Vergleich In diesem Abschnitt vergleichen wir zwei Kurzgeschichten miteinander, und zwar: - Die Küchenuhr von Wolfgang Borchert und - Das Duell von Irmtraud Morgner. Verglichen werden soll der Anteil der Pronomen und Substantive in beiden Texten. Nach der Lektüre beider Texte haben wir den Eindruck, dass der Schreibstil unterschiedlich ist. Ein hervorstechendes Merkmal in Borcherts Kurzgeschichte ist der Gebrauch von Pronomen statt Substantiven bzw. Nominalphrasen mit substantivischem Kopf. Null-Hypothese (\\(H_0\\)): Der Anteil der Pronomen ist in beiden Kurzgeschichten ähnlich groß. alternative Hypothese (\\(H_1\\)): Der Anteil der Pronomen in Borcherts Kurzgeschichte ist größer als in Morgners Kurzgeschichte. 8.1 Programme laden library(tidyverse) library(scales) library(quanteda) library(quanteda.textstats) library(quanteda.textplots) library(readtext) 8.2 Texte laden borchert &lt;- read_lines(&quot;data/borchert/borchert_kuechenuhr.txt&quot;) morgner &lt;- read_lines(&quot;data/morgner/morgner_duell.txt&quot;) 8.3 Textzerlegung und Annotation Die grammatische Analyse führen wir mit udpipe durch. Zuerst laden wir ein entsprechendes Sprachmodell. library(udpipe) destfile = &quot;german-gsd-ud-2.5-191206.udpipe&quot; if(!file.exists(destfile)){ sprachmodell &lt;- udpipe_download_model(language = &quot;german&quot;) udmodel_de &lt;- udpipe_load_model(sprachmodell$file_model) } else { file_model = destfile udmodel_de &lt;- udpipe_load_model(file_model) } Das Programm udpipe annotiert den Text mit Hilfe des Sprachmodells. x &lt;- udpipe_annotate(udmodel_de, x = borchert, trace = FALSE) b &lt;- as.data.frame(x) %&gt;% mutate(doc_id = &quot;borchert_kuechenuhr&quot;) x &lt;- udpipe_annotate(udmodel_de, x = morgner, trace = FALSE) m &lt;- as.data.frame(x) %&gt;% mutate(doc_id = &quot;morgner_duell&quot;) 8.4 Datensätze vereinen Wir vereinen beide Datensätze in einem neuen Datensatz (kurz), und zwar mit Hilfe der Funktion bind_rows(). kurz &lt;- bind_rows(b, m) 8.5 Wortklassen zählen Nun sind wir bereits in der Lage, Wortklassen zu zählen, die udpipe identifiziert hat. Dazu verwenden wir die Funktion count(). Gezählt werden die Kategorien in der Spalte xpos. kurz %&gt;% group_by(doc_id) %&gt;% count(xpos, sort = TRUE) ## # A tibble: 75 x 3 ## # Groups: doc_id [2] ## doc_id xpos n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 morgner_duell NN 318 ## 2 morgner_duell ART 185 ## 3 morgner_duell VVFIN 182 ## 4 morgner_duell PPER 121 ## 5 borchert_kuechenuhr $. 120 ## 6 borchert_kuechenuhr ADV 120 ## 7 morgner_duell $, 119 ## 8 borchert_kuechenuhr VVFIN 118 ## 9 borchert_kuechenuhr NN 117 ## 10 morgner_duell $. 112 ## # ... with 65 more rows Angesichts unserer oben angeführten Hypothese interessieren uns in der Spalte xpos nur die Kategorien NN (Nomen bzw. Substantive) und PPER (Personalpronomen). Deshalb filtern wir alle anderen Kategorien heraus. tabelle_1 &lt;- kurz %&gt;% group_by(doc_id) %&gt;% filter(xpos == &quot;NN&quot; | xpos == &quot;PPER&quot;) %&gt;% count(xpos, sort = TRUE) tabelle_1 ## # A tibble: 4 x 3 ## # Groups: doc_id [2] ## doc_id xpos n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 morgner_duell NN 318 ## 2 morgner_duell PPER 121 ## 3 borchert_kuechenuhr NN 117 ## 4 borchert_kuechenuhr PPER 87 Da Morgners Erzählung länger ist als Borcherts, fügen wir noch Prozentzahlen zur Tabelle hinzu. Dann fällt der Vergleich leichter. tabelle_1 %&gt;% mutate(pct = n/sum(n)) ## # A tibble: 4 x 4 ## # Groups: doc_id [2] ## doc_id xpos n pct ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 morgner_duell NN 318 0.724 ## 2 morgner_duell PPER 121 0.276 ## 3 borchert_kuechenuhr NN 117 0.574 ## 4 borchert_kuechenuhr PPER 87 0.426 Die Tabelle können wir umgestalten, so dass sie leichter zu lesen ist. Zu diesem Zweck setzen wir die Funktion pivot_wider() ein. tabelle_2 &lt;- tabelle_1 %&gt;% mutate(Anteil = round(100*n/sum(n), 2)) %&gt;% rename(wortklasse = xpos) %&gt;% dplyr::select(-n) %&gt;% pivot_wider(names_from = doc_id, values_from = Anteil) tabelle_2 ## # A tibble: 2 x 3 ## wortklasse morgner_duell borchert_kuechenuhr ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 NN 72.4 57.4 ## 2 PPER 27.6 42.6 Das Ergebnis können wir auch graphisch darstellen. tabelle_1 %&gt;% mutate(pct = n/sum(n)) %&gt;% ggplot(aes(doc_id, pct, fill = xpos)) + geom_col() Eine etwas schönere Graphik - zu Demonstrationszwecken (in diesem Fall ein Mordsaufwand, der an Overkill grenzt): library(ggtext) tabelle_1 %&gt;% mutate(pct = n/sum(n)) %&gt;% ggplot(aes(doc_id, pct, fill = xpos)) + geom_col(alpha = 0.7, color = &quot;black&quot;, width = 0.9, position = &quot;dodge&quot;) + # geom_label(aes(label = xpos), size = 5, hjust = 1) + scale_x_discrete(labels = c(&quot;Borchert: Die Küchenuhr&quot;, &quot;Morgner: Das Duell&quot;)) + scale_y_continuous( labels = percent_format(accuracy = 1), breaks = seq(0, 1, 0.1), sec.axis = dup_axis(name = NULL)) + scale_fill_discrete(type = c(&quot;#810080&quot;, &quot;#00aeff&quot;), labels = c(&quot;Nomen&quot;, &quot;Personalpronomen&quot;)) + coord_cartesian(expand = FALSE, clip = &quot;off&quot;) + theme(legend.position = &quot;top&quot;, legend.background = element_rect(color = &quot;black&quot;, fill = &quot;moccasin&quot;, size = 0.5, linetype = &quot;dotted&quot;), legend.key.height = unit(.5, &quot;lines&quot;), legend.key.width = unit(4, &quot;lines&quot;), panel.background = element_rect(color = &quot;black&quot;, fill = &quot;moccasin&quot;, size = 0.5, linetype = &quot;dotted&quot;), plot.margin = margin(10, 10, 5, 0), plot.title = element_markdown(color = &quot;#810080&quot;, size = 14, hjust = 0.5), plot.title.position = &quot;plot&quot;, # default: panel plot.subtitle = element_markdown(color = &quot;#00aeaf&quot;, hjust = 0.5), plot.caption = element_markdown(), plot.caption.position = &quot;plot&quot;, # default: panel plot.background = element_rect(color = &quot;black&quot;, fill = &quot;moccasin&quot;, size = 2, linetype = &quot;dotted&quot;)) + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;Anteil der **Nomen** und **Personalpronomen** in Kurzgeschichten&quot;, subtitle = &quot;**Wolfgang Borcherts _Küchenuhr_ im Vergleich zu Irmtraud Morgners _Duell_**&quot;, fill = &quot;Wortklassen: &quot;, caption = &quot;Annotation mit *udpipe* in *R&quot;) 8.6 Nicht-parametrischer Test Zum Abschluss machen wir noch den Chi-Quadrat-Test, um unsere Hypothese zu überprüfen. Die Formel für die Berechnung des \\(\\chi^2\\)-Wertes: \\[ \\tilde{\\chi}^2=\\sum_{k=1}^{n}\\frac{(Observed_k - Expected_k)^2}{Expected_k} \\] Für den \\(\\chi^2\\)-Quadrat-Test benötigen wir eine 2 x 2 - Kreuztabelle mit den beobachteten Werten in beiden Stichproben. Die erwarteten (theoretischen) Werte werden von der chisq.test()-Funktion berechnet. tabelle_3 &lt;- tabelle_1 %&gt;% rename(wortklasse = xpos) %&gt;% pivot_wider(names_from = doc_id, values_from = n) tabelle_3 ## # A tibble: 2 x 3 ## wortklasse morgner_duell borchert_kuechenuhr ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 NN 318 117 ## 2 PPER 121 87 Das Ergebnis (p = 0,0002) besagt, dass wir die Nullhypothese verwerfen und die alternative Hypothese akzeptieren können, denn p &lt; 0,05. Der Anteil der Pronomen in Borcherts Kurzgeschichte ist mit statistischer Signifikanz größer als in Morgners Kurzgeschichte. chires &lt;- chisq.test(tabelle_3[,-1]) chires ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: tabelle_3[, -1] ## X-squared = 13.8, df = 1, p-value = 0.0002033 Bei Bedarf kann man aus der oben gespeicherten Variable chires außer den beobachteten Häufigkeiten (hier: obs) auch die erwarteten Häufigkeiten (hier: ex(p)) ausgeben lassen. Letztere zeigen, welche Werte man bei Gültikgeit der Nullhypothese (also 50% Nomen- vs. 50% Pronomen-Anteil) erwarten könnte. obs &lt;- as_tibble(chires$observed) %&gt;% # shorter column names rename(morgner_obs = morgner_duell, borchert_obs = borchert_kuechenuhr) ex &lt;- as_tibble(round(chires$expected, 0)) %&gt;% # shorter column names rename(morgner_exp = morgner_duell, borchert_exp = borchert_kuechenuhr) # choose first column of tabelle_3 and bind it with the other columns freqenzen &lt;- bind_cols(tabelle_3[, 1], obs, ex) %&gt;% # rename the word classes mutate(wortklasse = ifelse(wortklasse == &quot;NN&quot;, &quot;Nomen&quot;, &quot;Pers.Pronomen&quot;)) freqenzen ## # A tibble: 2 x 5 ## wortklasse morgner_obs borchert_obs morgner_exp borchert_exp ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Nomen 318 117 297 138 ## 2 Pers.Pronomen 121 87 142 66 Wie beeinflusst die Verwendung von Personalpronomen das Textverständnis? Oder anders gefragt: Versteht der Textrezipient einen Text leichter, wenn der Textproduzent Personalpronomen statt Nomen verwendet werden? 8.7 Konnektoren Die Anzahl der Konnektoren (nebenordnende und unterordnende Konjunktionen) in den Kurzegeschichten ist in der folgenden Tabelle zu sehen. Die Junktoren (CCONJ) überwiegen zahlenmäßig wie in den meisten Texten. kurz %&gt;% group_by(doc_id) %&gt;% filter(upos == &quot;CCONJ&quot; | upos == &quot;SCONJ&quot;) %&gt;% count(upos, sort = TRUE) ## # A tibble: 4 x 3 ## # Groups: doc_id [2] ## doc_id upos n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 borchert_kuechenuhr CCONJ 48 ## 2 morgner_duell CCONJ 41 ## 3 morgner_duell SCONJ 19 ## 4 borchert_kuechenuhr SCONJ 18 Welche Konnektoren kamen in den beiden Texten vor? kurz_pivot_wid &lt;- kurz %&gt;% group_by(doc_id, lemma) %&gt;% mutate(lemma = ifelse(lemma == &quot;daß&quot;, &quot;dass&quot;, lemma)) %&gt;% filter(upos == &quot;CCONJ&quot; | upos == &quot;SCONJ&quot;) %&gt;% count(upos, sort = TRUE) %&gt;% pivot_wider(lemma:upos, names_from = doc_id, values_from = n) kurz_pivot_wid %&gt;% rmarkdown::paged_table() In beiden (ähnlich langen) Geschichten hat der Junktor und den Löwenanteil (über 30 Vorkommnisse). In Borcherts Geschichte kommt der adversative Junktor aber häufiger vor als in Morgners Kurzgeschichte. Andere Unterschiede sind marginal bzw. aufgrund der geringen Anzahl kaum statistisch bewertbar. Ein \\(\\chi^2\\)-Quadrat-Test soll bestätigen, dass der Junktor aber signifikant häufiger vorkommt als in Morgners Text. \\(H_0\\): kein signifikanter Unterschied zwischen den beiden Geschichten bezüglich der Häufigkeit von aber. \\(H_1\\): signifikanter Unterschied zwischen den beiden Geschichten bezüglich der Häufigkeit von aber. Vorbereitung: b_aber &lt;- kurz_pivot_wid$borchert_kuechenuhr[2] b_konn &lt;- sum(kurz_pivot_wid$borchert_kuechenuhr[-2], na.rm = TRUE) m_aber &lt;- kurz_pivot_wid$morgner_duell[2] m_konn &lt;- sum(kurz_pivot_wid$morgner_duell[-2], na.rm = TRUE) b_freq &lt;- rbind(b_aber, b_konn) %&gt;% as_tibble() %&gt;% rename(b = V1) m_freq &lt;- rbind(m_aber, m_konn) %&gt;% as_tibble() %&gt;% rename(m = V1) bormor &lt;- bind_cols(b_freq, m_freq) bormor ## # A tibble: 2 x 2 ## b m ## &lt;int&gt; &lt;int&gt; ## 1 12 2 ## 2 54 58 Test: chi_res &lt;- chisq.test(bormor) chi_res ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: bormor ## X-squared = 5.593, df = 1, p-value = 0.01803 Der \\(\\chi^2\\)-Test bestätigt einen signifikanten Unterschied im Gebrauch des Junktors aber. Adversativität wird in Borcherts Kurzgeschichte signifikant häufiger mit dem Junktor aber ausgedrückt als in Morgners Geschichte. 8.8 Äußerungslänge vergleichen In welcher Kurzgeschichte sind die Äußerungen im Durchschnitt länger? Das wollen wir mit dem t-Test prüfen. Zu diesem Zweck kann man alternativ quanteda- oder tidytext-Funktionen nutzen. Zuerst bereiten wir einen Datensatz mit beiden Texten vor. # Condense text, remove spaces and create dataframes bfl &lt;- borchert[-c(1, 41)] %&gt;% str_flatten(collapse = &quot; &quot;) %&gt;% str_squish() %&gt;% as_tibble() %&gt;% rename(text = value) %&gt;% mutate(doc_id = &quot;borchert&quot;) mfl &lt;- morgner[-1] %&gt;% str_flatten(collapse = &quot; &quot;) %&gt;% str_squish() %&gt;% as_tibble() %&gt;% rename(text = value) %&gt;% mutate(doc_id = &quot;morgner&quot;) # Join dataframes kurzgeschichten &lt;- bind_rows(bfl, mfl) Mit quanteda können wir unser Vorhaben folgendermaßen durchführen (in diesem Fall ohne das Programm readtext() zu verwenden): library(quanteda) library(quanteda.textstats) library(quanteda.textplots) # Create corpus from dataframe kurzcorp &lt;- corpus(kurzgeschichten) # Reshape corpus kurzkorp &lt;- corpus_reshape(kurzcorp, to = &quot;sentences&quot;) Mit der Funktion summary() erhalten wir auch die Anzahl der Wortformen (Tokens) und Äußerungen (Sentences). kurzstats &lt;- summary(kurzkorp, n = 209) %&gt;% as_tibble() kurzstats %&gt;% slice_sample(n = 10) ## # A tibble: 10 x 4 ## Text Types Tokens Sentences ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 borchert.49 7 7 1 ## 2 morgner.8 11 11 1 ## 3 morgner.98 10 10 1 ## 4 borchert.79 9 9 1 ## 5 morgner.9 18 24 1 ## 6 borchert.7 5 5 1 ## 7 morgner.68 13 14 1 ## 8 morgner.64 2 2 1 ## 9 morgner.72 70 103 1 ## 10 borchert.19 14 15 1 Daraus kann man die durchschnittliche Äußerungslänge der beiden Texte berechnen. Eine ähnliche quanteda-Funktion ist textstat_summary(). textstat_summary(kurzkorp) %&gt;% slice_sample(n = 6) ## document chars sents tokens types puncts numbers symbols urls tags emojis ## 1 morgner.98 53 1 10 10 1 0 0 0 0 0 ## 2 borchert.56 70 1 15 15 2 0 0 0 0 0 ## 3 borchert.27 46 1 11 11 2 0 0 0 0 0 ## 4 morgner.103 325 1 61 48 9 0 0 0 0 0 ## 5 morgner.71 212 1 38 32 6 0 0 0 0 0 ## 6 borchert.10 39 1 8 8 1 0 0 0 0 0 Um einen t-Test durchzuführen, berechnen wir auch die Standardabweichung (also eine Größe, die Auskunft darüber gibt, wie stark die einzelnen Äußerungslängen vom Durchschnitt abweichen). Die statistische Funktion t.test() erledigt das für uns. Vor dem t-Test spalten wir die Text-Spalte mit der Funktion separate() in zwei neue, so dass die erste Spalte doc_id nur die Namen der beiden Autoren (borchert, morgner) enthält. Das ist notwendig, weil man mit dem t-Test höchstens zwei Gruppen miteinander vergleichen kann. kurzstats &lt;- kurzstats %&gt;% separate(Text, into = c(&quot;doc_id&quot;, &quot;number&quot;), sep = &quot;\\\\.&quot;, extra = &quot;merge&quot;) kurzstats %&gt;% slice_sample(n = 6) ## # A tibble: 6 x 5 ## doc_id number Types Tokens Sentences ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 borchert 52 16 16 1 ## 2 morgner 84 23 28 1 ## 3 morgner 100 40 50 1 ## 4 morgner 91 8 8 1 ## 5 borchert 56 15 15 1 ## 6 morgner 2 6 6 1 Wir führen einen t-Test mit zwei unabhängigen Stichproben (d.h. unseren beiden Kurzgeschichten) durch. Null-Hypothese \\(H_0\\): kein Unterschied hinsichtlich der mittleren Äußerungslängen. Alternative Hypothese \\(H_1\\): die beiden Kurzgeschichten unterscheiden sich hinsichtlich ihrer durchschnittlichen Äußerungslänge. tres &lt;- t.test(Tokens ~ doc_id, data = kurzstats, var.equal = FALSE) tres ## ## Welch Two Sample t-test ## ## data: Tokens by doc_id ## t = -3.1339, df = 121.11, p-value = 0.002164 ## alternative hypothesis: true difference in means between group borchert and group morgner is not equal to 0 ## 95 percent confidence interval: ## -8.539975 -1.927510 ## sample estimates: ## mean in group borchert mean in group morgner ## 10.35849 15.59223 Das Ergebnis des t-Tests (p = 0,002164) liegt unter dem 5 % Signifikanzniveau (p = 0,05). Demnach können wir die Nullhypothese verwerfen und die alternative Hypothese annehmen. Die mittlere Äußerungslänge in Borcherts Kurzgeschichte beträgt etwa 10,36 Tokens (Wörter + Interpunktion), die in Morgners Kurzgeschichte dagegen 15,59 Tokens (Wörter + Interpunktion). Da wir das Ergebnis in der Variable tres gespeichert haben, können wir später auch andere Werte abrufen. Die Standardabweichung (sd) müssen wir allerdings auf andere Weise berechnen. tapply(kurzstats$Tokens, list(kurzstats$doc_id), sd) ## borchert morgner ## 5.048694 16.201814 Die Standardabweichung von Morgners Kurzgeschichte ist etwa 3-mal größer als die von Borcherts Kurzgeschichte. Mit anderen Worten: in Morgners Geschichte gibt es kurze, aber auch sehr lange Äußerungen. In Borcherts Geschichte sind die Schwankungen nicht so groß. Deshalb ist die Standardabweichung viel kleiner. Deshalb war es richtig, dass wir oben beim t.test() die Option var.equal = FALSE angegeben haben. Das Programm hat den statistischen Test demgemäß angepasst. Wir wandeln nun diese Werte in eine Tabelle um, weil wir eine Gesamttabelle zusammenstellen wollen. kurz_sd &lt;- tapply(kurzstats$Tokens, list(kurzstats$doc_id), sd) %&gt;% as_tibble() %&gt;% rename(sd = value) Die beiden Durchschnittswerte und die Freiheitsgrade (df) erhalten wir aus dem t-Test bzw. der oben gespeicherten Variable tres. Wir können diese Werte wiederum in eine Tabelle umwandeln. kurz_mean &lt;- tres$estimate %&gt;% as_tibble() %&gt;% rename(mean = value) Die Freiheitsgrade (auch in eine Tabelle umgewandelt): kurz_df &lt;- tres$parameter %&gt;% as_tibble() %&gt;% rename(df = value) Fügen wir noch den t-Wert und den p-Wert hinzu (wieder in Tabellen umgewandelt). kurz_t &lt;- tres$statistic %&gt;% as_tibble() %&gt;% rename(t = value) kurz_p &lt;- tres$p.value %&gt;% as_tibble() %&gt;% rename(p = value) Die einzelnen Tabellen und die erste Spalte aus der Tabelle kurzstats vereinen wir in einer Gesamttabelle. kurz_tres &lt;- bind_cols(kurzstats %&gt;% dplyr::select(doc_id) %&gt;% distinct(), kurz_mean, kurz_sd, kurz_t, kurz_p, kurz_df) kurz_tres ## # A tibble: 2 x 6 ## doc_id mean sd t p df ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 borchert 10.4 5.05 -3.13 0.00216 121. ## 2 morgner 15.6 16.2 -3.13 0.00216 121. Bei Bedarf kann man noch eine kleine Schönheitskorrektur durchführen: borchert_morgner_t_test &lt;- kurz_tres %&gt;% mutate(t = ifelse(doc_id == &quot;morgner&quot;, NA, t), p = ifelse(doc_id == &quot;morgner&quot;, NA, p), df = ifelse(doc_id == &quot;morgner&quot;, NA, df)) borchert_morgner_t_test %&gt;% rmarkdown::paged_table() Mit dem Programm report erhält man Beschreibungen eines Datensatzes oder statistischer Tests (z.B. des t.test()). library(report) kurzstats %&gt;% group_by(doc_id) %&gt;% report() %&gt;% summary() ## The data contains 209 observations, grouped by doc_id, of the following 5 variables: ## ## - borchert (n = 106): ## - number: 106 entries, such as 1 (0.94%); 10 (0.94%); 100 (0.94%) and 103 others ## - Types: Mean = 9.99, SD = 4.65, range: [2, 22] ## - Tokens: Mean = 10.36, SD = 5.05, range: [2, 25] ## - Sentences: Mean = 1.00, SD = 0.00, range: [1, 1] ## ## - morgner (n = 103): ## - number: 103 entries, such as 1 (0.97%); 10 (0.97%); 100 (0.97%) and 100 others ## - Types: Mean = 13.95, SD = 11.95, range: [2, 70] ## - Tokens: Mean = 15.59, SD = 16.20, range: [2, 103] ## - Sentences: Mean = 1.00, SD = 0.00, range: [1, 1] Bericht über den durchgeführten t.test(). report(t.test(Tokens ~ doc_id, data = kurzstats)) ## ## ## The Welch Two Sample t-test testing the difference of Tokens by doc_id (mean in group borchert = 10.36, mean in group morgner = 15.59) suggests that the effect is negative, statistically significant, and medium (difference = -5.23, 95% CI [-8.54, -1.93], t(121.11) = -3.13, p = 0.002; Cohen&#39;s d = -0.57, 95% CI [-0.93, -0.21]) library(ggstatsplot) # for reproducibility set.seed(2021) # plot kurzstats %&gt;% dplyr::select(doc_id, Tokens) %&gt;% ggbetweenstats( x = doc_id, xlab = &quot;&quot;, y = Tokens, ylab = &quot;&quot;, type = &quot;parametric&quot;, plot.type = &quot;boxviolin&quot;, pairwise.display = &quot;significant&quot;, bf.message = FALSE, conf.level = 0.95, outlier.tagging = TRUE, title = &quot;Distribution of sentence length in two short stories&quot;, caption = &quot;programmed by Teodor Petri&quot; ) "],["spracherwerbsdaten.html", "Kapitel 9 Spracherwerbsdaten 9.1 Programme 9.2 Daten lesen 9.3 Verknüpfung der einzelnen Dateien 9.4 1. Häufigkeit insgesamt 9.5 2. Welche d-Form am häufigsten? 9.6 3. Welche Form als erste? 9.7 4. Wann alle d-Formen vertreten? 9.8 5. Bevorzugtes d-Wort von MOT 9.9 6. Welche Formen hört Rahel nie / kaum?", " Kapitel 9 Spracherwerbsdaten 9.1 Programme library(tidyverse) library(scales) library(janitor) library(readxl) 9.2 Daten lesen Verwendet werden die Transkriptionsdateien der normalhörenden deutschsprachigen Kinder im Korpus von Szagun. Zuerst wird das Arbeitsverzeichnis festgelegt. Dann werden alle (relevanten) Excel-Dateien gelesen und in der Variable data (einer Liste) gespeichert, die die Erwerbsdaten von 6 Kindern und deren Müttern enthält. Die Dateien konzentrieren sich auf sechs d-Wörter, die als Demonstrativpronomen und bestimmter Artikel dienen können. Code (Version 1): Wir erhalten 12 Datensätze (6 Kinder und 6 erwachsene Bezugspersonen, meistens die Mutter). library(fs) wd &lt;- getwd() wdd &lt;- paste0(wd,&quot;/data/szagun/&quot;) file_paths &lt;- fs::dir_ls(wdd) file_paths ## D:/Users/teodo/Documents/R/raj2022-book/data/szagun/Anna.xlsx ## D:/Users/teodo/Documents/R/raj2022-book/data/szagun/Anna_M.xlsx ## D:/Users/teodo/Documents/R/raj2022-book/data/szagun/Emely.xlsx ## D:/Users/teodo/Documents/R/raj2022-book/data/szagun/Emely_M.xlsx ## D:/Users/teodo/Documents/R/raj2022-book/data/szagun/Falko.xlsx ## D:/Users/teodo/Documents/R/raj2022-book/data/szagun/Falko_M.xlsx ## D:/Users/teodo/Documents/R/raj2022-book/data/szagun/Lisa.xlsx ## D:/Users/teodo/Documents/R/raj2022-book/data/szagun/Lisa_M.xlsx ## D:/Users/teodo/Documents/R/raj2022-book/data/szagun/Rahel.xlsx ## D:/Users/teodo/Documents/R/raj2022-book/data/szagun/Rahel_M.xlsx ## D:/Users/teodo/Documents/R/raj2022-book/data/szagun/Soeren.xlsx ## D:/Users/teodo/Documents/R/raj2022-book/data/szagun/Soeren_M.xlsx data &lt;- file_paths %&gt;% map(function (path) { readxl::read_xlsx(path, skip = 3) }) # data Code (Version 2): Wir erhalten 12 Datensätze (6 Kinder und 6 erwachsene Bezugspersonen, meistens die Mutter). # rahel &lt;- read_excel(&quot;data/Rahel_Mot.xlsx&quot;) wd &lt;- getwd() wdd &lt;- paste0(wd,&quot;/data/szagun/&quot;) setwd(wdd) data.files = list.files(path = wdd, pattern = &quot;\\\\.xlsx&quot;, full.names = TRUE, recursive = FALSE) # TRUE if subdirectories included data &lt;- lapply(data.files, readxl::read_excel, skip = 3) # data # dataframes: which columns are in common? # common_cols &lt;- intersect(colnames(f), colnames(g)) 9.3 Verknüpfung der einzelnen Dateien Mit Clan wurden 12 Excel-Dateien zusammengestellt. Die Excel-Dateien werden nun zu einer einzelnen zusammengefasst. Zu diesem Zweck werden mit Hilfe einer Programm-Schleife mehrere Veränderungen vorgenommen: - die data-Liste wird in einen Datensatz umgewandelt (as.data.frame, as_tibble), - nicht relevante Variablen eliminiert (select), - Variablen umbenannt (rename), - neue Variablen aus bereits bestehenden geschaffen (separate), - fehlende Spalten (z.B. für das) hinzugefügt (if  add_column), - Variablen konvertiert (as.numeric). szagun &lt;- NULL szagun &lt;- data.frame() for(i in 1:length(data)){ f &lt;- data[i] %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% clean_names() %&gt;% dplyr::select(-c(language, corpus, sex, group, race, ses, role, education, custom_field)) %&gt;% rename(utterances = starts_with(&quot;x_&quot;)) %&gt;% # mutate(des = ifelse(&quot;des&quot; %in% names(.), des, NA)) %&gt;% # add missing column &quot;des&quot; (but this doesn&#39;t work) separate(file, into = c(&quot;id&quot;, &quot;location&quot;), sep = &quot;,&quot;) %&gt;% dplyr::select(-location) %&gt;% rename(ageof = age) %&gt;% separate(id, into = c(&quot;id&quot;, &quot;age&quot;), sep = &quot;_&quot;) if(!&#39;des&#39; %in% names(f)) { # if column &quot;des&quot; is missing, add it to dataframe, fill 0 f &lt;- f %&gt;% add_column(des = 0)} # append file to previous file szagun &lt;- rbind(szagun, f) } szagun &lt;- szagun %&gt;% mutate(ttr = as.numeric(ttr)) szagun &lt;- szagun %&gt;% rename(age1 = age, age = ageof) %&gt;% mutate(age = age1) %&gt;% mutate(age = paste0(str_sub(age, 1, 1), &quot;;&quot;, str_sub(age, 2, 3), &quot;.&quot;, str_sub(age, 4, 5))) Die gemeinsamen Spalten in Datensätzen (hier: f) kann man übrigens mit der Funktion intersect() herausfinden. common_cols &lt;- intersect(colnames(f), colnames(f)) Hier ist nun die gesamte Tabelle (mit 6 Kindern und 6 Erwachsenen) für den von Szagun ausgewählten Beobachtungszeitraum. library(DT) szagun %&gt;% DT::datatable(filter = &quot;top&quot;, fillContainer = T, extensions = c(&#39;Buttons&#39;, &quot;ColReorder&quot;, &quot;RowReorder&quot;, &#39;FixedColumns&#39;, &quot;KeyTable&quot;, &quot;Scroller&quot;), options = list(pageLength = 10, autowidth = TRUE, colReorder = TRUE, rowReorder = TRUE, order = list(c(0, &#39;asc&#39;)), keys = TRUE, deferRender = TRUE, scrollY = 600, scroller = TRUE, scrollX = TRUE, fixedColumns = list(leftColumns = 2, rightColumns = 1), dom = &#39;Bfrtip&#39;, # Bfrtip or t buttons = c(&#39;colvis&#39;,&#39;copy&#39;, &#39;csv&#39;, &#39;excel&#39;, &#39;pdf&#39;, &#39;print&#39;) )) %&gt;% formatStyle(&quot;id&quot;, target = &#39;row&#39;, backgroundColor = styleEqual(c(0, 1), c(&#39;gray30&#39;, &#39;lightblue&#39;))) writexl::write_xlsx(szagun, &quot;data/Kapitel 9 Spracherwerbsdaten Computergestütze Textanalyse mit R.xlsx&quot;) In der Gesamttabelle haben wir einen Fehler, den wir noch berichtigen müssen. Für die Mütter stehen uns nicht für jeden Monat Zahlen zur Verfügung. Dieser Zuordnungsfehler (MOT-Daten bei CHI zugeordnet) ist in der Graphik mit plotly (s.u.) bemerkbar. Außerdem gibt es auch kleinere monatliche Unterschiede bei den Kindern. Die Altersangaben müssen wir demnach vereinfachen (nur Jahr und Monat, eventuell auch bestimmte Monate zusammenfassen). Das lassen wir im Augenblick noch außer Acht. 9.4 1. Häufigkeit insgesamt children &lt;- szagun %&gt;% filter(code == &quot;CHI&quot;) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(age, code, d_wort) %&gt;% summarise(avg_freq = mean(freq/utterances)) %&gt;% ggplot(aes(age, avg_freq, group = code, fill = d_wort)) + geom_col() + geom_smooth(se = F) + scale_y_continuous(labels = label_percent(accuracy = 1)) + labs(y = &quot;mean frequency&quot;) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) ggsave(plot = children, filename =&quot;pictures/szagun_gesamtfrequenz_kinder.jpg&quot;, dpi = 300) children library(plotly) ggplotly(children) mothers &lt;- szagun %&gt;% filter(code != &quot;CHI&quot;) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(age, code, d_wort) %&gt;% summarise(avg_freq = mean(freq/utterances)) %&gt;% ggplot(aes(age, avg_freq, group = code, fill = d_wort)) + geom_col() + geom_smooth(se = F) + scale_y_continuous(labels = label_percent(accuracy = 1)) + labs(y = &quot;mean frequency&quot;) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) ggsave(plot = mothers, filename =&quot;pictures/szagun_gesamtfrequenz_muetter.jpg&quot;, dpi = 300) mothers library(plotly) ggplotly(mothers) library(patchwork) gesamt1 &lt;- (children + theme(legend.position = &quot;top&quot;, axis.ticks.x = element_blank(), axis.text.x = element_blank(), axis.title.x = element_blank()) + guides(fill = guide_legend(nrow = 1))) / (mothers + theme(legend.position = &quot;none&quot;)) ggsave(plot = gesamt1, filename =&quot;pictures/szagun_gesamtfrequenz.jpg&quot;, dpi = 300) gesamt1 9.5 2. Welche d-Form am häufigsten? topn &lt;- szagun %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(code, d_wort) %&gt;% add_count(code) %&gt;% summarise(freqsum = sum(freq)) %&gt;% mutate(pct = freqsum/sum(freqsum)) %&gt;% mutate(d_wort = fct_reorder(d_wort, pct)) %&gt;% ggplot(aes(pct, d_wort, color = code)) + # geom_col(position = &quot;dodge&quot;, color = &quot;black&quot;) + geom_segment(aes(yend = d_wort, xend = 0), size = 2) + geom_point(size = 6) + geom_point(size = 2, color = &quot;black&quot;) + scale_x_continuous(labels = label_percent()) + # guides(fill = guide_legend(nrow = 1)) + theme(legend.position = &quot;none&quot;) + # labs(fill = &quot;d-Wort: &quot;) + facet_wrap(~ code, dir = &quot;v&quot;) ggsave(plot = topn, filename =&quot;pictures/szagun_frequenzverteilung.jpg&quot;, dpi = 300) topn d &lt;- szagun %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(code, d_wort) %&gt;% summarise(freqsum = sum(freq)) %&gt;% pivot_wider(names_from = code, values_from = freqsum) d %&gt;% arrange(-CHI) %&gt;% rmarkdown::paged_table() Sowohl bei den Kindern (CHI) als auch bei den Müttern (MOT) ergibt sich fast dieselbe Reihenfolge der Häufigkeiten. Der auffälligste Unterschied ist der zwischen den Häufigkeiten von des und dem. d_dem &lt;- szagun %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(code, d_wort) %&gt;% summarise(freqsum = sum(freq)) %&gt;% mutate(d_wort = ifelse(d_wort == &quot;dem&quot;, &quot;dem&quot;, &quot;andere&quot;)) %&gt;% group_by(code, d_wort) %&gt;% summarise(freqsum = sum(freqsum)) %&gt;% pivot_wider(names_from = code, values_from = freqsum) d_dem ## # A tibble: 2 x 3 ## d_wort CHI MOT ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 andere 24312 13898 ## 2 dem 103 378 Mit dem \\(\\chi^2\\)-Quadrat-Test kann man prüfen, ob die beiden Stichproben statistisch signifikant unterschiedlich sind. chires &lt;- chisq.test(d_dem[,-1], B = 2000) # B: mit Monte Carlo test chires ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: d_dem[, -1] ## X-squared = 361.75, df = 1, p-value &lt; 2.2e-16 observed &lt;- chires$observed %&gt;% as_tibble() %&gt;% round(0) %&gt;% rename(CHI_obs = CHI, MOT_obs = MOT) expected &lt;- chires$expected %&gt;% as_tibble() %&gt;% round(0) %&gt;% rename(CHI_exp = CHI, MOT_exp = MOT) frequenztabelle &lt;- bind_cols(d_dem[,1], observed, expected) frequenztabelle %&gt;% arrange(-CHI_obs) %&gt;% rmarkdown::paged_table() Hier folgt ein \\(\\chi^2\\)-Quadrat-Test mit einem Kind, und zwar mit Rahel und ihrer MOT: szagun %&gt;% filter(id == &quot;Rahel&quot;) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(code, d_wort) %&gt;% summarise(freqsum = sum(freq)) %&gt;% mutate(d_wort = ifelse(d_wort == &quot;dem&quot;, &quot;dem&quot;, &quot;andere&quot;)) %&gt;% group_by(code, d_wort) %&gt;% summarise(freqsum = sum(freqsum)) %&gt;% pivot_wider(names_from = code, values_from = freqsum) %&gt;% select(-d_wort) %&gt;% chisq.test(.) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: . ## X-squared = 96.562, df = 1, p-value &lt; 2.2e-16 Eine Frequenztabelle mit den d-Wörtern von Soren und seiner MOT (aber ohne \\(\\chi^2\\)-Quadrat-Test): szagun %&gt;% filter(id == &quot;Soeren&quot;) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(code, d_wort) %&gt;% summarise(freqsum = sum(freq)) %&gt;% pivot_wider(names_from = code, values_from = freqsum) ## # A tibble: 6 x 3 ## d_wort CHI MOT ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 das 2552 1136 ## 2 dem 9 24 ## 3 den 432 198 ## 4 der 1101 300 ## 5 des 24 1 ## 6 die 1556 519 9.6 3. Welche Form als erste? Zur Beantwortung dieser Frage filtern wir zuerst die Spalten id und code. Dann sortieren wir die jeweilige Tabellen für das ausgewählte Kind nach Alter age. Mit der Funktion pivot_longer() bilden wir eine lange Tabellenform. szagun %&gt;% filter(id == &quot;Rahel&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(freq &gt; 0) ## # A tibble: 92 x 10 ## # Groups: age [22] ## id age1 code age utterances types token ttr d_wort freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Rahel 10400 CHI 1;04.00 428 1 4 0.25 die 4 ## 2 Rahel 10507 CHI 1;05.07 630 3 4 0.75 das 1 ## 3 Rahel 10507 CHI 1;05.07 630 3 4 0.75 der 1 ## 4 Rahel 10507 CHI 1;05.07 630 3 4 0.75 des 2 ## 5 Rahel 10614 CHI 1;06.14 632 2 16 0.125 das 1 ## 6 Rahel 10614 CHI 1;06.14 632 2 16 0.125 die 15 ## 7 Rahel 10800 CHI 1;08.00 909 3 11 0.273 das 8 ## 8 Rahel 10800 CHI 1;08.00 909 3 11 0.273 des 1 ## 9 Rahel 10800 CHI 1;08.00 909 3 11 0.273 die 2 ## 10 Rahel 10907 CHI 1;09.07 424 3 6 0.5 das 4 ## # ... with 82 more rows szagun %&gt;% filter(id == &quot;Anna&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(freq &gt; 0) ## # A tibble: 116 x 10 ## # Groups: age [22] ## id age1 code age utterances types token ttr d_wort freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Anna 10400 CHI 1;04.00 230 2 12 0.167 das 1 ## 2 Anna 10400 CHI 1;04.00 230 2 12 0.167 die 11 ## 3 Anna 10507 CHI 1;05.07 633 4 23 0.174 das 7 ## 4 Anna 10507 CHI 1;05.07 633 4 23 0.174 den 1 ## 5 Anna 10507 CHI 1;05.07 633 4 23 0.174 des 5 ## 6 Anna 10507 CHI 1;05.07 633 4 23 0.174 die 10 ## 7 Anna 10614 CHI 1;06.14 845 4 76 0.053 das 68 ## 8 Anna 10614 CHI 1;06.14 845 4 76 0.053 der 1 ## 9 Anna 10614 CHI 1;06.14 845 4 76 0.053 des 1 ## 10 Anna 10614 CHI 1;06.14 845 4 76 0.053 die 6 ## # ... with 106 more rows szagun %&gt;% filter(id == &quot;Emely&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(freq &gt; 0) ## # A tibble: 78 x 10 ## # Groups: age [21] ## id age1 code age utterances types token ttr d_wort freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Emely 10407 CHI 1;04.07 374 1 1 1 des 1 ## 2 Emely 10507 CHI 1;05.07 235 2 2 1 das 1 ## 3 Emely 10507 CHI 1;05.07 235 2 2 1 des 1 ## 4 Emely 10813 CHI 1;08.13 321 2 2 1 das 1 ## 5 Emely 10813 CHI 1;08.13 321 2 2 1 des 1 ## 6 Emely 10907 CHI 1;09.07 720 3 40 0.075 der 3 ## 7 Emely 10907 CHI 1;09.07 720 3 40 0.075 des 1 ## 8 Emely 10907 CHI 1;09.07 720 3 40 0.075 die 36 ## 9 Emely 11014 CHI 1;10.14 875 4 76 0.053 das 1 ## 10 Emely 11014 CHI 1;10.14 875 4 76 0.053 der 1 ## # ... with 68 more rows szagun %&gt;% filter(id == &quot;Falko&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(freq &gt; 0) ## # A tibble: 93 x 10 ## # Groups: age [19] ## id age1 code age utterances types token ttr d_wort freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Falko 10800 CHI 1;08.00 311 1 2 0.5 die 2 ## 2 Falko 10907 CHI 1;09.07 238 2 3 0.667 das 1 ## 3 Falko 10907 CHI 1;09.07 238 2 3 0.667 den 2 ## 4 Falko 11014 CHI 1;10.14 378 2 28 0.071 das 27 ## 5 Falko 11014 CHI 1;10.14 378 2 28 0.071 der 1 ## 6 Falko 20000 CHI 2;00.00 817 6 55 0.109 das 37 ## 7 Falko 20000 CHI 2;00.00 817 6 55 0.109 dem 1 ## 8 Falko 20000 CHI 2;00.00 817 6 55 0.109 den 7 ## 9 Falko 20000 CHI 2;00.00 817 6 55 0.109 der 4 ## 10 Falko 20000 CHI 2;00.00 817 6 55 0.109 des 1 ## # ... with 83 more rows szagun %&gt;% filter(id == &quot;Lisa&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(freq &gt; 0) ## # A tibble: 94 x 10 ## # Groups: age [21] ## id age1 code age utterances types token ttr d_wort freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Lisa 10507 CHI 1;05.07 694 2 3 0.667 das 1 ## 2 Lisa 10507 CHI 1;05.07 694 2 3 0.667 die 2 ## 3 Lisa 10614 CHI 1;06.14 555 2 24 0.083 den 1 ## 4 Lisa 10614 CHI 1;06.14 555 2 24 0.083 die 23 ## 5 Lisa 10800 CHI 1;08.00 410 3 11 0.273 den 3 ## 6 Lisa 10800 CHI 1;08.00 410 3 11 0.273 der 3 ## 7 Lisa 10800 CHI 1;08.00 410 3 11 0.273 die 5 ## 8 Lisa 10907 CHI 1;09.07 580 4 20 0.2 das 2 ## 9 Lisa 10907 CHI 1;09.07 580 4 20 0.2 den 3 ## 10 Lisa 10907 CHI 1;09.07 580 4 20 0.2 der 5 ## # ... with 84 more rows szagun %&gt;% filter(id == &quot;Soeren&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(freq &gt; 0) ## # A tibble: 95 x 10 ## # Groups: age [22] ## id age1 code age utterances types token ttr d_wort freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Soeren 10400 CHI 1;04.00 487 1 1 1 das 1 ## 2 Soeren 10507 CHI 1;05.07 663 2 21 0.095 das 19 ## 3 Soeren 10507 CHI 1;05.07 663 2 21 0.095 der 2 ## 4 Soeren 10614 CHI 1;06.14 765 4 68 0.059 das 23 ## 5 Soeren 10614 CHI 1;06.14 765 4 68 0.059 den 39 ## 6 Soeren 10614 CHI 1;06.14 765 4 68 0.059 der 1 ## 7 Soeren 10614 CHI 1;06.14 765 4 68 0.059 die 5 ## 8 Soeren 10800 CHI 1;08.00 753 5 84 0.06 das 42 ## 9 Soeren 10800 CHI 1;08.00 753 5 84 0.06 dem 1 ## 10 Soeren 10800 CHI 1;08.00 753 5 84 0.06 den 35 ## # ... with 85 more rows 9.7 4. Wann alle d-Formen vertreten? Die Vorgangsweise ist fast dieselbe wie bei der vorherigen Frage, aber wir filtern dieses Mal zusätzlich die Anzahl der types. szagun %&gt;% filter(id == &quot;Rahel&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(types &gt;= 5) ## # A tibble: 66 x 10 ## # Groups: age [11] ## id age1 code age utterances types token ttr d_wort freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Rahel 20107 CHI 2;01.07 556 5 25 0.2 das 18 ## 2 Rahel 20107 CHI 2;01.07 556 5 25 0.2 dem 0 ## 3 Rahel 20107 CHI 2;01.07 556 5 25 0.2 den 1 ## 4 Rahel 20107 CHI 2;01.07 556 5 25 0.2 der 1 ## 5 Rahel 20107 CHI 2;01.07 556 5 25 0.2 des 3 ## 6 Rahel 20107 CHI 2;01.07 556 5 25 0.2 die 2 ## 7 Rahel 20400 CHI 2;04.00 1115 5 122 0.041 das 100 ## 8 Rahel 20400 CHI 2;04.00 1115 5 122 0.041 dem 0 ## 9 Rahel 20400 CHI 2;04.00 1115 5 122 0.041 den 2 ## 10 Rahel 20400 CHI 2;04.00 1115 5 122 0.041 der 5 ## # ... with 56 more rows szagun %&gt;% filter(id == &quot;Rahel&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(types == 6) ## # A tibble: 12 x 10 ## # Groups: age [2] ## id age1 code age utterances types token ttr d_wort freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Rahel 21014 CHI 2;10.14 741 6 362 0.017 das 159 ## 2 Rahel 21014 CHI 2;10.14 741 6 362 0.017 dem 3 ## 3 Rahel 21014 CHI 2;10.14 741 6 362 0.017 den 5 ## 4 Rahel 21014 CHI 2;10.14 741 6 362 0.017 der 57 ## 5 Rahel 21014 CHI 2;10.14 741 6 362 0.017 des 1 ## 6 Rahel 21014 CHI 2;10.14 741 6 362 0.017 die 137 ## 7 Rahel 30107 CHI 3;01.07 1035 6 420 0.014 das 233 ## 8 Rahel 30107 CHI 3;01.07 1035 6 420 0.014 dem 4 ## 9 Rahel 30107 CHI 3;01.07 1035 6 420 0.014 den 16 ## 10 Rahel 30107 CHI 3;01.07 1035 6 420 0.014 der 57 ## 11 Rahel 30107 CHI 3;01.07 1035 6 420 0.014 des 1 ## 12 Rahel 30107 CHI 3;01.07 1035 6 420 0.014 die 109 szagun %&gt;% filter(id == &quot;Anna&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(types &gt;= 5) ## # A tibble: 114 x 10 ## # Groups: age [19] ## id age1 code age utterances types token ttr d_wort freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Anna 10800 CHI 1;08.00 1247 5 87 0.057 das 66 ## 2 Anna 10800 CHI 1;08.00 1247 5 87 0.057 dem 0 ## 3 Anna 10800 CHI 1;08.00 1247 5 87 0.057 den 5 ## 4 Anna 10800 CHI 1;08.00 1247 5 87 0.057 der 1 ## 5 Anna 10800 CHI 1;08.00 1247 5 87 0.057 des 2 ## 6 Anna 10800 CHI 1;08.00 1247 5 87 0.057 die 13 ## 7 Anna 10907 CHI 1;09.07 1076 5 149 0.034 das 127 ## 8 Anna 10907 CHI 1;09.07 1076 5 149 0.034 dem 0 ## 9 Anna 10907 CHI 1;09.07 1076 5 149 0.034 den 5 ## 10 Anna 10907 CHI 1;09.07 1076 5 149 0.034 der 7 ## # ... with 104 more rows szagun %&gt;% filter(id == &quot;Anna&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(types == 6) ## # A tibble: 66 x 10 ## # Groups: age [11] ## id age1 code age utterances types token ttr d_wort freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Anna 20000 CHI 2;00.00 957 6 226 0.027 das 93 ## 2 Anna 20000 CHI 2;00.00 957 6 226 0.027 dem 1 ## 3 Anna 20000 CHI 2;00.00 957 6 226 0.027 den 4 ## 4 Anna 20000 CHI 2;00.00 957 6 226 0.027 der 32 ## 5 Anna 20000 CHI 2;00.00 957 6 226 0.027 des 9 ## 6 Anna 20000 CHI 2;00.00 957 6 226 0.027 die 87 ## 7 Anna 20214 CHI 2;02.14 929 6 251 0.024 das 55 ## 8 Anna 20214 CHI 2;02.14 929 6 251 0.024 dem 3 ## 9 Anna 20214 CHI 2;02.14 929 6 251 0.024 den 14 ## 10 Anna 20214 CHI 2;02.14 929 6 251 0.024 der 57 ## # ... with 56 more rows szagun %&gt;% filter(id == &quot;Emely&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(types &gt;= 5) ## # A tibble: 54 x 10 ## # Groups: age [9] ## id age1 code age utterances types token ttr d_wort freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Emely 20907 CHI 2;09.07 1103 5 161 0.031 das 107 ## 2 Emely 20907 CHI 2;09.07 1103 5 161 0.031 dem 0 ## 3 Emely 20907 CHI 2;09.07 1103 5 161 0.031 den 4 ## 4 Emely 20907 CHI 2;09.07 1103 5 161 0.031 der 28 ## 5 Emely 20907 CHI 2;09.07 1103 5 161 0.031 des 3 ## 6 Emely 20907 CHI 2;09.07 1103 5 161 0.031 die 19 ## 7 Emely 21014 CHI 2;10.14 1004 5 217 0.023 das 141 ## 8 Emely 21014 CHI 2;10.14 1004 5 217 0.023 dem 0 ## 9 Emely 21014 CHI 2;10.14 1004 5 217 0.023 den 7 ## 10 Emely 21014 CHI 2;10.14 1004 5 217 0.023 der 28 ## # ... with 44 more rows szagun %&gt;% filter(id == &quot;Emely&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(types == 6) ## # A tibble: 0 x 10 ## # Groups: age [0] ## # ... with 10 variables: id &lt;chr&gt;, age1 &lt;chr&gt;, code &lt;chr&gt;, age &lt;chr&gt;, ## # utterances &lt;dbl&gt;, types &lt;dbl&gt;, token &lt;dbl&gt;, ttr &lt;dbl&gt;, d_wort &lt;chr&gt;, ## # freq &lt;dbl&gt; szagun %&gt;% filter(id == &quot;Falko&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(types &gt;= 5) ## # A tibble: 90 x 10 ## # Groups: age [15] ## id age1 code age utterances types token ttr d_wort freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Falko 20000 CHI 2;00.00 817 6 55 0.109 das 37 ## 2 Falko 20000 CHI 2;00.00 817 6 55 0.109 dem 1 ## 3 Falko 20000 CHI 2;00.00 817 6 55 0.109 den 7 ## 4 Falko 20000 CHI 2;00.00 817 6 55 0.109 der 4 ## 5 Falko 20000 CHI 2;00.00 817 6 55 0.109 des 1 ## 6 Falko 20000 CHI 2;00.00 817 6 55 0.109 die 5 ## 7 Falko 20107 CHI 2;01.07 734 5 251 0.02 das 195 ## 8 Falko 20107 CHI 2;01.07 734 5 251 0.02 dem 0 ## 9 Falko 20107 CHI 2;01.07 734 5 251 0.02 den 5 ## 10 Falko 20107 CHI 2;01.07 734 5 251 0.02 der 6 ## # ... with 80 more rows szagun %&gt;% filter(id == &quot;Falko&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(types == 6) ## # A tibble: 54 x 10 ## # Groups: age [9] ## id age1 code age utterances types token ttr d_wort freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Falko 20000 CHI 2;00.00 817 6 55 0.109 das 37 ## 2 Falko 20000 CHI 2;00.00 817 6 55 0.109 dem 1 ## 3 Falko 20000 CHI 2;00.00 817 6 55 0.109 den 7 ## 4 Falko 20000 CHI 2;00.00 817 6 55 0.109 der 4 ## 5 Falko 20000 CHI 2;00.00 817 6 55 0.109 des 1 ## 6 Falko 20000 CHI 2;00.00 817 6 55 0.109 die 5 ## 7 Falko 20614 CHI 2;06.14 806 6 290 0.021 das 149 ## 8 Falko 20614 CHI 2;06.14 806 6 290 0.021 dem 1 ## 9 Falko 20614 CHI 2;06.14 806 6 290 0.021 den 17 ## 10 Falko 20614 CHI 2;06.14 806 6 290 0.021 der 42 ## # ... with 44 more rows szagun %&gt;% filter(id == &quot;Lisa&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(types &gt;= 5) ## # A tibble: 66 x 10 ## # Groups: age [11] ## id age1 code age utterances types token ttr d_wort freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Lisa 20400 CHI 2;04.00 693 5 140 0.036 das 94 ## 2 Lisa 20400 CHI 2;04.00 693 5 140 0.036 dem 0 ## 3 Lisa 20400 CHI 2;04.00 693 5 140 0.036 den 20 ## 4 Lisa 20400 CHI 2;04.00 693 5 140 0.036 der 4 ## 5 Lisa 20400 CHI 2;04.00 693 5 140 0.036 des 6 ## 6 Lisa 20400 CHI 2;04.00 693 5 140 0.036 die 16 ## 7 Lisa 20507 CHI 2;05.07 735 6 126 0.048 das 46 ## 8 Lisa 20507 CHI 2;05.07 735 6 126 0.048 dem 1 ## 9 Lisa 20507 CHI 2;05.07 735 6 126 0.048 den 35 ## 10 Lisa 20507 CHI 2;05.07 735 6 126 0.048 der 9 ## # ... with 56 more rows szagun %&gt;% filter(id == &quot;Lisa&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(types == 6) ## # A tibble: 30 x 10 ## # Groups: age [5] ## id age1 code age utterances types token ttr d_wort freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Lisa 20507 CHI 2;05.07 735 6 126 0.048 das 46 ## 2 Lisa 20507 CHI 2;05.07 735 6 126 0.048 dem 1 ## 3 Lisa 20507 CHI 2;05.07 735 6 126 0.048 den 35 ## 4 Lisa 20507 CHI 2;05.07 735 6 126 0.048 der 9 ## 5 Lisa 20507 CHI 2;05.07 735 6 126 0.048 des 2 ## 6 Lisa 20507 CHI 2;05.07 735 6 126 0.048 die 33 ## 7 Lisa 30000 CHI 3;00.00 602 6 197 0.03 das 94 ## 8 Lisa 30000 CHI 3;00.00 602 6 197 0.03 dem 2 ## 9 Lisa 30000 CHI 3;00.00 602 6 197 0.03 den 12 ## 10 Lisa 30000 CHI 3;00.00 602 6 197 0.03 der 13 ## # ... with 20 more rows szagun %&gt;% filter(id == &quot;Soeren&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(types &gt;= 5) ## # A tibble: 72 x 10 ## # Groups: age [12] ## id age1 code age utterances types token ttr d_wort freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Soeren 10800 CHI 1;08.00 753 5 84 0.06 das 42 ## 2 Soeren 10800 CHI 1;08.00 753 5 84 0.06 dem 1 ## 3 Soeren 10800 CHI 1;08.00 753 5 84 0.06 den 35 ## 4 Soeren 10800 CHI 1;08.00 753 5 84 0.06 der 3 ## 5 Soeren 10800 CHI 1;08.00 753 5 84 0.06 des 0 ## 6 Soeren 10800 CHI 1;08.00 753 5 84 0.06 die 3 ## 7 Soeren 20000 CHI 2;00.00 679 5 101 0.05 das 60 ## 8 Soeren 20000 CHI 2;00.00 679 5 101 0.05 dem 1 ## 9 Soeren 20000 CHI 2;00.00 679 5 101 0.05 den 14 ## 10 Soeren 20000 CHI 2;00.00 679 5 101 0.05 der 13 ## # ... with 62 more rows szagun %&gt;% filter(id == &quot;Soeren&quot;, code == &quot;CHI&quot;) %&gt;% group_by(age) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% filter(types == 6) ## # A tibble: 6 x 10 ## # Groups: age [1] ## id age1 code age utterances types token ttr d_wort freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Soeren 30214 CHI 3;02.14 1119 6 361 0.017 das 169 ## 2 Soeren 30214 CHI 3;02.14 1119 6 361 0.017 dem 2 ## 3 Soeren 30214 CHI 3;02.14 1119 6 361 0.017 den 25 ## 4 Soeren 30214 CHI 3;02.14 1119 6 361 0.017 der 67 ## 5 Soeren 30214 CHI 3;02.14 1119 6 361 0.017 des 2 ## 6 Soeren 30214 CHI 3;02.14 1119 6 361 0.017 die 96 9.8 5. Bevorzugtes d-Wort von MOT szagun %&gt;% filter(id == &quot;Rahel&quot;, code == &quot;MOT&quot;) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(d_wort) %&gt;% summarise(sum = sum(freq)) %&gt;% arrange(-sum) ## # A tibble: 6 x 2 ## d_wort sum ## &lt;chr&gt; &lt;dbl&gt; ## 1 das 1051 ## 2 die 602 ## 3 der 382 ## 4 den 217 ## 5 dem 73 ## 6 des 1 szagun %&gt;% filter(id == &quot;Anna&quot;, code == &quot;MOT&quot;) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(d_wort) %&gt;% summarise(sum = sum(freq)) %&gt;% arrange(-sum) ## # A tibble: 6 x 2 ## d_wort sum ## &lt;chr&gt; &lt;dbl&gt; ## 1 das 1315 ## 2 die 1052 ## 3 der 557 ## 4 den 252 ## 5 dem 132 ## 6 des 129 szagun %&gt;% filter(id == &quot;Emely&quot;, code == &quot;MOT&quot;) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(d_wort) %&gt;% summarise(sum = sum(freq)) %&gt;% arrange(-sum) ## # A tibble: 6 x 2 ## d_wort sum ## &lt;chr&gt; &lt;dbl&gt; ## 1 das 831 ## 2 die 376 ## 3 der 287 ## 4 den 106 ## 5 des 66 ## 6 dem 24 szagun %&gt;% filter(id == &quot;Falko&quot;, code == &quot;MOT&quot;) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(d_wort) %&gt;% summarise(sum = sum(freq)) %&gt;% arrange(-sum) ## # A tibble: 6 x 2 ## d_wort sum ## &lt;chr&gt; &lt;dbl&gt; ## 1 das 951 ## 2 die 594 ## 3 der 468 ## 4 den 175 ## 5 dem 44 ## 6 des 0 szagun %&gt;% filter(id == &quot;Lisa&quot;, code == &quot;MOT&quot;) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(d_wort) %&gt;% summarise(sum = sum(freq)) %&gt;% arrange(-sum) ## # A tibble: 6 x 2 ## d_wort sum ## &lt;chr&gt; &lt;dbl&gt; ## 1 das 1235 ## 2 die 635 ## 3 der 275 ## 4 den 186 ## 5 dem 81 ## 6 des 1 szagun %&gt;% filter(id == &quot;Soeren&quot;, code == &quot;MOT&quot;) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(d_wort) %&gt;% summarise(sum = sum(freq)) %&gt;% arrange(-sum) ## # A tibble: 6 x 2 ## d_wort sum ## &lt;chr&gt; &lt;dbl&gt; ## 1 das 1136 ## 2 die 519 ## 3 der 300 ## 4 den 198 ## 5 dem 24 ## 6 des 1 9.9 6. Welche Formen hört Rahel nie / kaum? szagun %&gt;% filter(id == &quot;Rahel&quot;, code == &quot;MOT&quot;) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(d_wort) %&gt;% summarise(sum = sum(freq)) %&gt;% arrange(sum) ## # A tibble: 6 x 2 ## d_wort sum ## &lt;chr&gt; &lt;dbl&gt; ## 1 des 1 ## 2 dem 73 ## 3 den 217 ## 4 der 382 ## 5 die 602 ## 6 das 1051 szagun %&gt;% filter(id == &quot;Anna&quot;, code == &quot;MOT&quot;) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(d_wort) %&gt;% summarise(sum = sum(freq)) %&gt;% arrange(sum) ## # A tibble: 6 x 2 ## d_wort sum ## &lt;chr&gt; &lt;dbl&gt; ## 1 des 129 ## 2 dem 132 ## 3 den 252 ## 4 der 557 ## 5 die 1052 ## 6 das 1315 szagun %&gt;% filter(id == &quot;Emely&quot;, code == &quot;MOT&quot;) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(d_wort) %&gt;% summarise(sum = sum(freq)) %&gt;% arrange(sum) ## # A tibble: 6 x 2 ## d_wort sum ## &lt;chr&gt; &lt;dbl&gt; ## 1 dem 24 ## 2 des 66 ## 3 den 106 ## 4 der 287 ## 5 die 376 ## 6 das 831 szagun %&gt;% filter(id == &quot;Falko&quot;, code == &quot;MOT&quot;) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(d_wort) %&gt;% summarise(sum = sum(freq)) %&gt;% arrange(sum) ## # A tibble: 6 x 2 ## d_wort sum ## &lt;chr&gt; &lt;dbl&gt; ## 1 des 0 ## 2 dem 44 ## 3 den 175 ## 4 der 468 ## 5 die 594 ## 6 das 951 szagun %&gt;% filter(id == &quot;Lisa&quot;, code == &quot;MOT&quot;) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(d_wort) %&gt;% summarise(sum = sum(freq)) %&gt;% arrange(sum) ## # A tibble: 6 x 2 ## d_wort sum ## &lt;chr&gt; &lt;dbl&gt; ## 1 des 1 ## 2 dem 81 ## 3 den 186 ## 4 der 275 ## 5 die 635 ## 6 das 1235 szagun %&gt;% filter(id == &quot;Soeren&quot;, code == &quot;MOT&quot;) %&gt;% pivot_longer(das:die, names_to = &quot;d_wort&quot;, values_to = &quot;freq&quot;) %&gt;% group_by(d_wort) %&gt;% summarise(sum = sum(freq)) %&gt;% arrange(sum) ## # A tibble: 6 x 2 ## d_wort sum ## &lt;chr&gt; &lt;dbl&gt; ## 1 des 1 ## 2 dem 24 ## 3 den 198 ## 4 der 300 ## 5 die 519 ## 6 das 1136 "],["textvergleich.html", "Kapitel 10 Textvergleich 10.1 Programme installieren 10.2 Programme laden 10.3 Texte öffnen 10.4 Korpus anlegen 10.5 Tokenisierung 10.6 Tokenliste säubern 10.7 Kwic 10.8 Häufigkeit 10.9 Kollokationen 10.10 Lemmatisierung 10.11 Wortwolken 10.12 Position im Text (xray) 10.13 Lexikalische Vielfalt 10.14 Textähnlichkeit 10.15 Schlüsselwörter (keywords) 10.16 Lesbarkeit des Textes 10.17 Kookurrenz-Netzwerk (FCM) 10.18 Grammatische Analyse 10.19 Sentiment", " Kapitel 10 Textvergleich 10.1 Programme installieren Installation von Programmen (Paketen): Wenn Sie die Programme bereits installiert haben, können Sie diesen Schritt überspringen. Das Zeichen # im Programmblock (chunk) bedeutet, dass diese Zeile nicht ausgeführt wird. Entfernen Sie # vor dem Installationsbefehl, falls Sie ein Programm installieren möchten. Der folgende Programmblock automatisiert die Installation von erwünschten, aber noch nicht installierten Programmen. # install.packages(&quot;readtext&quot;) ## First specify the packages of interest packages = c(&quot;tidyverse&quot;, &quot;quanteda&quot;, &quot;quanteda.textplots&quot;, &quot;quanteda.textstats&quot;, &quot;wordcloud2&quot;, &quot;tidytext&quot;, &quot;udpipe&quot;, &quot;janitor&quot;, &quot;scales&quot;, &quot;widyr&quot;, &quot;syuzhet&quot;, &quot;corpustools&quot;, &quot;readtext&quot;) # Install packages not yet installed installed_packages &lt;- packages %in% rownames(installed.packages()) if (any(installed_packages == FALSE)) { install.packages(packages[!installed_packages]) } # Packages loading invisible(lapply(packages, library, character.only = TRUE)) 10.2 Programme laden Zuerst müssen wir die Programme ausführen, die wir für die geplante Arbeit benötigen. library(readtext) library(quanteda) library(quanteda.textstats) library(quanteda.textplots) library(tidyverse) library(tidytext) library(wordcloud2) library(udpipe) library(janitor) library(scales) library(widyr) library(syuzhet) library(corpustools) 10.3 Texte öffnen txt = readtext(&quot;data/books/*.txt&quot;, encoding = &quot;UTF-8&quot;) txt ## readtext object consisting of 2 documents and 0 docvars. ## # Description: df [2 x 2] ## doc_id text ## &lt;chr&gt; &lt;chr&gt; ## 1 prozess.txt &quot;\\&quot;Der Prozes\\&quot;...&quot; ## 2 tom.txt &quot;\\&quot;Tom Sawyer\\&quot;...&quot; Alternativ können Sie die Texte auch aus dem Internet auf Ihren Computer laden und öffnen: txt1 = readtext(&quot;https://raw.githubusercontent.com/tpetric7/tpetric7.github.io/main/data/books/prozess.txt&quot;, encoding = &quot;UTF-8&quot;) txt2 = readtext(&quot;https://raw.githubusercontent.com/tpetric7/tpetric7.github.io/main/data/books/tom.txt&quot;, encoding = &quot;UTF-8&quot;) # Datoteki zdruimo txt = rbind(txt1,txt2) 10.4 Korpus anlegen Wir erstellen ein Korpus, d.h. eine Textsammlung. Der Befehl im Programmbündel quanteda lautet corpus(). romane = corpus(txt) Zusammenfassung einiger grundlegender quantitativer Merkmale des sprachlichen Materials mit Hilfe von zwei quanteda-Funktionen: - summary() - textstat_summary(). (romanstatistik = textstat_summary(romane) ) ## document chars sents tokens types puncts numbers symbols urls tags emojis ## 1 prozess.txt 482722 3845 88010 7907 16380 10 0 0 0 0 ## 2 tom.txt 460249 4652 85841 9860 18785 9 0 0 0 0 povzetek = summary(romane) povzetek ## Corpus consisting of 2 documents, showing 2 documents: ## ## Text Types Tokens Sentences ## prozess.txt 8507 88010 3845 ## tom.txt 10551 85841 4652 Anhand der zusammengefassten Korpusdaten können Sie z.B. die durchschnittliche Satzlänge in den Texten des Korpus berechnen: povzetek %&gt;% group_by(Text) %&gt;% mutate(dolzina_povedi = Tokens/Sentences) ## # A tibble: 2 x 5 ## # Groups: Text [2] ## Text Types Tokens Sentences dolzina_povedi ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 prozess.txt 8507 88010 3845 22.9 ## 2 tom.txt 10551 85841 4652 18.5 Wir könnten auch einen Indikator für die lexikalische Vielfalt in Texten berechnen, d. h. das Verhältnis zwischen Types und Tokens, was im Englischen als type token ratio (ttr) bezeichnet wird. Es wird unterschieden zwischen Wörterbucheinheiten (Lemmata), Wortformtypen (Types) und Wortformen (Tokens). So ist z. B. das deutsche Verb gehen eine Lexikoneinheit, die mehrere verschiedene Formen (Types) aufweist: z.B. gehe, gehst, geht, gehen, geht, ging, gingst,  gegangen. Wortformen (Tokens): Einige Formen des Verbs (Types) kommen häufiger vor als andere, und erscheinen im ausgewählten Text nicht. povzetek %&gt;% group_by(Text) %&gt;% mutate(ttr = Types/Tokens) ## # A tibble: 2 x 5 ## # Groups: Text [2] ## Text Types Tokens Sentences ttr ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 prozess.txt 8507 88010 3845 0.0967 ## 2 tom.txt 10551 85841 4652 0.123 Das Programm quanteda verfügt über mehrere Optionen zur Ermittlung der lexikalischen Vielfalt, die aber eine Zerlegung der Texte in kleinere Einheiten, d. h. Tokens (Wörter, Satzzeichen usw.), erfordert. Für einige Merkmale müssen wir eine Dokumenthäufigkeitsmatrix (dfm, document frequency matrix) erstellen, in der festgehalten wird, wie oft eine Wortform in jedem Text der Textsammlung vorkommt. 10.5 Tokenisierung Um mehr über die Texte herauszufinden, z.B. welche Wörter in den Texten vorkommen, müssen wir zunächst eine Liste von Texteinheiten (d.h. Wörter, Satzzeichen usw.) erstellen. Wir zerlegen die Texte in Wortformen (z. B. mit Hilfe von Leerzeichen zwischen den Wortformen als Trennungszeichen). Für die Tokenisierung gibt es in quanteda den Befehl tokens(). besede = tokens(romane) head(besede) ## Tokens consisting of 2 documents. ## prozess.txt : ## [1] &quot;Der&quot; &quot;Prozess&quot; &quot;by&quot; ## [4] &quot;Franz&quot; &quot;Kafka&quot; &quot;Aligned&quot; ## [7] &quot;by&quot; &quot;:&quot; &quot;bilingual-texts.com&quot; ## [10] &quot;(&quot; &quot;fully&quot; &quot;reviewed&quot; ## [ ... and 87,998 more ] ## ## tom.txt : ## [1] &quot;Tom&quot; &quot;Sawyer&quot; &quot;by&quot; &quot;Mark&quot; ## [5] &quot;Twain&quot; &quot;Aligned&quot; &quot;by&quot; &quot;:&quot; ## [9] &quot;András&quot; &quot;Farkas&quot; &quot;(&quot; &quot;autoalignment&quot; ## [ ... and 85,829 more ] 10.6 Tokenliste säubern Wir können Nicht-Wörter aus der Wortliste entfernen: besede = tokens(romane, remove_punct = T, remove_symbols = T, remove_numbers = T, remove_url = T) head(besede) ## Tokens consisting of 2 documents. ## prozess.txt : ## [1] &quot;Der&quot; &quot;Prozess&quot; &quot;by&quot; ## [4] &quot;Franz&quot; &quot;Kafka&quot; &quot;Aligned&quot; ## [7] &quot;by&quot; &quot;bilingual-texts.com&quot; &quot;fully&quot; ## [10] &quot;reviewed&quot; &quot;Der&quot; &quot;Prozess&quot; ## [ ... and 71,608 more ] ## ## tom.txt : ## [1] &quot;Tom&quot; &quot;Sawyer&quot; &quot;by&quot; &quot;Mark&quot; ## [5] &quot;Twain&quot; &quot;Aligned&quot; &quot;by&quot; &quot;András&quot; ## [9] &quot;Farkas&quot; &quot;autoalignment&quot; &quot;Source&quot; &quot;Project&quot; ## [ ... and 67,035 more ] Wir können auch Wortformen ausschließen, die für die Inhaltsanalyse nicht erwünscht sind, sogenannte Stoppwörter. Auch englische Wörter, die in den ausgewählten deutschen Texten vorkommen, können entfernt werden. Wir verketten mehrere Einheiten mit Hilfe der c()-Funktion (engl. concatenate = verketten). stoplist_de = c(stopwords(&quot;de&quot;), &quot;dass&quot;, &quot;Aligned&quot;, &quot;by&quot;, &quot;autoalignment&quot;, &quot;Source&quot;, &quot;Project&quot;, &quot;bilingual-texts.com&quot;, &quot;fully&quot;, &quot;reviewed&quot;) besede = tokens_select(besede, pattern = stoplist_de, selection = &quot;remove&quot;) Die folgende Wortiste, mit Hilfe der tokens()-Funktion angelegt, wird verwendet, um eine Konkordanz zu erstellen, d.h. eine Liste von Kontexten, in denen ein bestimmter Suchbegriff (z.B. ein Wort oder eine Wortgruppe) vorkommt. stoplist_en = c(&quot;Aligned&quot;, &quot;by&quot;, &quot;autoalignment&quot;, &quot;Source&quot;, &quot;Project&quot;, &quot;bilingual-texts.com&quot;, &quot;fully&quot;, &quot;reviewed&quot;) # Obdrali bomo loila woerter = tokens(romane, remove_symbols = T, remove_numbers = T, remove_url = T) # Odstranili bomo angleke besede na zaetku besedil woerter = tokens_select(woerter, pattern = stoplist_en, selection = &quot;remove&quot;, padding = TRUE) 10.7 Kwic Um Konkordanzen zu erstellen, verfügt das Programm quanteda über die kwic()-Funktion (Schlüsselwort im Kontext-Ansichten). Es ist möglich, nach einzelnen Wörtern, Phrasen und beliebigen anderen Zeichen (z.B. Wildcards wie * ) zu suchen. kwic(woerter, pattern = c(&quot;Frau&quot;, &quot;Herr&quot;)) %&gt;% head(3) ## Keyword-in-context with 3 matches. ## [prozess.txt, 22] Kafka Verhaftung, Gespräch mit | Frau | ## [prozess.txt, 54] verhaftet. Die Köchin der | Frau | ## [prozess.txt, 96] seinem Kopfkissen aus die alte | Frau | ## ## Grubach, dann Fräulein Brüstner ## Grubach, seiner Zimmervermieterin, ## , die ihm gegenüber wohnte Wir werden die Konkordanz in eine Tabelle (oder Datensatz) umwandeln, d.h. in ein data.frame oder tibble(). Dies hat z.B. den Vorteil, dass Spaltennamen (d.h. Variablen) angegeben werden. Die kwic()-Funktion hat mehrere Optionen, z.B. case_insensitive = FALSE unterscheidet zwischen Groß- und Kleinschreibung. Der Standardwert ist TRUE, d.h. dass diese Unterscheidung (wie in Excel) nicht getroffen wird. konkordanca = kwic(woerter, pattern = c(&quot;Frau&quot;, &quot;Herr&quot;), case_insensitive = FALSE) %&gt;% as_tibble() konkordanca %&gt;% rmarkdown::paged_table() Mit dem Befehl count() kann man die Anzahl der im Korpus vorgefundenen Wortformen auszählen. konkordanca %&gt;% count(keyword) ## # A tibble: 2 x 2 ## keyword n ## &lt;chr&gt; &lt;int&gt; ## 1 Frau 132 ## 2 Herr 94 Im nächsten Beispiel werden Wörter mit der Endung in für Substantive gesucht, die weibliche Personennamen bezeichnen (z.B. Ärztin, Köchin, ). (konkordanca2 = kwic(woerter, pattern = c(&quot;*in&quot;), case_insensitive = FALSE) %&gt;% as_tibble() ) ## # A tibble: 4,100 x 7 ## docname from to pre keyword post pattern ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 prozess.txt 26 26 mit Frau Grubach , dann Fräulein Brüs~ *in ## 2 prozess.txt 52 52 eines Morgens verhaftet . Die Köchin der ~ *in ## 3 prozess.txt 58 58 der Frau Grubach , seiner Zimmerve~ , di~ *in ## 4 prozess.txt 86 86 . K . wartete noch ein Weil~ *in ## 5 prozess.txt 129 129 . Sofort klopfte es und ein Mann~ *in ## 6 prozess.txt 134 134 ein Mann , den er in dies~ *in ## 7 prozess.txt 143 143 niemals gesehen hatte , trat ein . Er~ *in ## 8 prozess.txt 155 155 fest gebaut , er trug ein anli~ *in ## 9 prozess.txt 292 292 zur Tür , die er ein weni~ *in ## 10 prozess.txt 322 322 das Frühstück bringt . « Ein klei~ *in ## # ... with 4,090 more rows Leider enthält die obige Liste von Kontexten viele Wortformen, die keine weiblichen Personennamen darstellen (z. B. ein, und, ). Wenn wir eine genauere Liste haben wollen, müssen wir auf eine geeignetere Weise suchen, z.B. mit einer Reihe von Platzhaltern, den sogenannten regulären Ausdrücken (regular expressions, regex). Sie können reguläre Ausdrücke auf dem Portal https://regex101.com/ ausprobieren und lernen. In dem folgenden Recherchebeispiel arbeiten wir mit regulären Ausdrücken, um möglichst wenige falsche Treffer zu erhalten. konkordanca2 = kwic(woerter, pattern = &quot;\\\\A[A-Z][a-z]+[^Eae]in\\\\b&quot;, valuetype = &quot;regex&quot;, case_insensitive = FALSE) %&gt;% as_tibble() %&gt;% filter(keyword != &quot;Immerhin&quot;, keyword != &quot;Darin&quot;, keyword != &quot;Termin&quot;, keyword != &quot;Worin&quot;, keyword != &quot;Robin&quot;, keyword != &quot;Medizin&quot;, keyword != &quot;Austin&quot;, keyword != &quot;Musselin&quot;, keyword != &quot;Benjamin&quot;, keyword != &quot;Franklin&quot;) konkordanca2 %&gt;% rmarkdown::paged_table() Ein weiteres Beispiel für die Verwendung regulärer Ausdrücke bei der Textrecherche: Welches Diminutivsuffix ist in dem Korpus vorherrschend: lein oder chen ? (konkordanca3a = kwic(woerter, &quot;*lein&quot;, valuetype = &quot;glob&quot;, case_insensitive = FALSE) %&gt;% as_tibble() %&gt;% count(keyword, sort = TRUE) ) ## # A tibble: 6 x 2 ## keyword n ## &lt;chr&gt; &lt;int&gt; ## 1 Fräulein 112 ## 2 allein 49 ## 3 klein 10 ## 4 Allein 2 ## 5 Äuglein 1 ## 6 Schreibmaschinenfräulein 1 (konkordanca3b &lt;- kwic(woerter, &quot;*chen&quot;, valuetype = &quot;glob&quot;, case_insensitive = FALSE) %&gt;% as_tibble() %&gt;% count(keyword, sort = T) ) ## # A tibble: 415 x 2 ## keyword n ## &lt;chr&gt; &lt;int&gt; ## 1 machen 125 ## 2 Mädchen 100 ## 3 sprechen 57 ## 4 bißchen 44 ## 5 zwischen 43 ## 6 solchen 38 ## 7 Weilchen 33 ## 8 Zeichen 31 ## 9 Menschen 30 ## 10 Burschen 28 ## # ... with 405 more rows (konkordanca3 &lt;- kwic(woerter, pattern = c(&quot;\\\\A[A-Z][a-z]*[^aäeiouürs]chen\\\\b&quot;, &quot;[A-Z]*[^kl]lein\\\\b&quot;), valuetype = &quot;regex&quot;, case_insensitive = FALSE) %&gt;% as_tibble() %&gt;% filter(keyword != &quot;Welchen&quot;, keyword != &quot;Manchen&quot;, keyword != &quot;Solchen&quot;, keyword != &quot;Fräulein&quot;) ) ## # A tibble: 74 x 7 ## docname from to pre keyword post pattern ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 prozess.txt 87 87 K . wartete noch ein Weilchen , sa~ &quot;\\\\A[A~ ## 2 prozess.txt 750 750 warf das Buch auf ein Tischchen und ~ &quot;\\\\A[A~ ## 3 prozess.txt 1740 1740 aufgeschreckt , die bei dem Tischchen am o~ &quot;\\\\A[A~ ## 4 prozess.txt 2617 2617 , stand K . ein Weilchen lang~ &quot;\\\\A[A~ ## 5 prozess.txt 3323 3323 Stuhl und hielt ihn ein Weilchen mit ~ &quot;\\\\A[A~ ## 6 prozess.txt 3624 3624 hatte . Jetzt war das Nachttisch~ von ~ &quot;\\\\A[A~ ## 7 prozess.txt 3799 3799 Gegenstände , die auf dem Nachttisch~ lage~ &quot;\\\\A[A~ ## 8 prozess.txt 5805 5805 sagte K . nach einem Weilchen und ~ &quot;\\\\A[A~ ## 9 prozess.txt 6952 6952 , das früh auf dem Tischchen beim~ &quot;\\\\A[A~ ## 10 prozess.txt 10539 10539 . » Darf ich das Nachttisch~ von ~ &quot;\\\\A[A~ ## # ... with 64 more rows Im folgenden Beispiel suchen wir mit Hilfe regulärer Ausdrücke das Wort Frau + Nachname / Vorname. Hierbei ist zwingend erforderlich, case_insensitive = FALSE zu setzen, da das Programm zwischen Groß- und Kleinbuchstaben unterscheiden soll. (konkordanca4 &lt;- kwic(woerter, pattern = phrase(&quot;\\\\bFrau\\\\b ^[A-Z][^[:punct:]]&quot;), valuetype = &quot;regex&quot;, case_insensitive = FALSE) %&gt;% as_tibble() ) ## # A tibble: 61 x 7 ## docname from to pre keyword post pattern ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 prozess.txt 22 23 Kafka Verhaftung , Gespräch mit Frau G~ , da~ &quot;\\\\bFr~ ## 2 prozess.txt 54 55 verhaftet . Die Köchin der Frau G~ , se~ &quot;\\\\bFr~ ## 3 prozess.txt 416 417 im Nebenzimmer sind und wie Frau G~ dies~ &quot;\\\\bFr~ ## 4 prozess.txt 551 552 Es war das Wohnzimmer der Frau G~ , vi~ &quot;\\\\bFr~ ## 5 prozess.txt 700 701 . » Ich will doch Frau G~ - « ~ &quot;\\\\bFr~ ## 6 prozess.txt 1647 1648 gerade die gegenüberliegende T~ Frau G~ woll~ &quot;\\\\bFr~ ## 7 prozess.txt 2868 2869 war , so konnte er Frau G~ als ~ &quot;\\\\bFr~ ## 8 prozess.txt 5960 5961 . Im Vorzimmer öffnete dann Frau G~ , di~ &quot;\\\\bFr~ ## 9 prozess.txt 6557 6558 in der ganzen Wohnung der Frau G~ veru~ &quot;\\\\bFr~ ## 10 prozess.txt 6852 6853 , aber da er mit Frau G~ spre~ &quot;\\\\bFr~ ## # ... with 51 more rows 10.8 Häufigkeit Die Dokument-Frequenz-Matrix (dfm) ist der Ausgangspunkt für die Berechnung und grafische Darstellung verschiedener statistischer Größen, z.B. auch der Häufigkeit von Wortformen in den Texten des Korpus: matrika = dfm(besede, tolower = FALSE) # za zdaj obdrimo velike zaetnice # Odstranimo besede, ki jih v vsebinski analizi ne potrebujemo (stopwords) matrika = dfm_select(matrika, selection = &quot;remove&quot;, pattern = stoplist_de) matrika ## Document-feature matrix of: 2 documents, 15,185 features (39.73% sparse) and 0 docvars. ## features ## docs Prozess Franz Kafka Verhaftung Gespräch Frau Grubach Fräulein ## prozess.txt 2 24 2 18 16 114 50 112 ## tom.txt 0 0 0 0 4 18 0 0 ## features ## docs Brüstner Jemand ## prozess.txt 1 2 ## tom.txt 0 1 ## [ reached max_nfeat ... 15,175 more features ] Das Programm quanteda verfügt über eine spezielle Funktion, die eine Liste von Wortformen und deren Häufigkeit erstellt, und zwar textstat_frequency(). library(quanteda.textstats) library(quanteda.textplots) pogostnost = textstat_frequency(matrika, groups = c(&quot;prozess.txt&quot;, &quot;tom.txt&quot;)) pogostnost %&gt;% rmarkdown::paged_table() Ein Diagramm mit den gebräuchlichsten Wortformen im Korpus: pogostnost %&gt;% slice_max(order_by = frequency, n = 20) %&gt;% mutate(feature = reorder_within(feature, frequency, frequency, sep = &quot;: &quot;)) %&gt;% # ggplot(aes(frequency, reorder(feature, frequency))) + ggplot(aes(frequency, feature)) + geom_col(fill=&quot;steelblue&quot;) + labs(x = &quot;Frequency&quot;, y = &quot;&quot;) + facet_wrap(~ group, scales = &quot;free&quot;) Falls erforderlich, kann die Liste der Wortformhäufigkeiten mit der filter()-Funktion in zwei separate Listen aufgeteilt werden. pogost_tom = textstat_frequency(matrika, groups = c(&quot;prozess.txt&quot;, &quot;tom.txt&quot;)) %&gt;% filter(group == &quot;tom.txt&quot;) pogost_tom %&gt;% rmarkdown::paged_table() pogost_prozess = textstat_frequency(matrika, groups = c(&quot;prozess.txt&quot;, &quot;tom.txt&quot;)) %&gt;% filter(group == &quot;prozess.txt&quot;) pogost_prozess %&gt;% rmarkdown::paged_table() Verben des Sagens und des Denkens: Welche kommen in den ausgewählten Texten häufiger vor? sagen = pogostnost %&gt;% filter(str_detect(feature, &quot;^(ge)?sag*&quot;)) sagen %&gt;% rmarkdown::paged_table() reden = pogostnost %&gt;% filter(str_detect(feature, &quot;^(ge)?rede*&quot;)) reden %&gt;% rmarkdown::paged_table() fragen = pogostnost %&gt;% filter(str_detect(feature, &quot;^(ge)?frag*&quot;)) fragen %&gt;% rmarkdown::paged_table() antworten = pogostnost %&gt;% filter(str_detect(feature, &quot;^(ge)?antwort*&quot;)) antworten %&gt;% rmarkdown::paged_table() rufen = pogostnost %&gt;% filter(str_detect(feature, pattern = &quot;^(ge)?ruf*&quot;, negate = FALSE)) %&gt;% filter(!str_detect(feature, &quot;ruh|run|rum|rui|ruch&quot;)) rufen %&gt;% rmarkdown::paged_table() verb1 = sagen %&gt;% group_by(group) %&gt;% summarise(freq = sum(frequency)) %&gt;% mutate(verb = &quot;sagen&quot;) verb2 = reden %&gt;% group_by(group) %&gt;% summarise(freq = sum(frequency)) %&gt;% mutate(verb = &quot;reden&quot;) verb3 = fragen %&gt;% group_by(group) %&gt;% summarise(freq = sum(frequency)) %&gt;% mutate(verb = &quot;fragen&quot;) verb4 = antworten %&gt;% group_by(group) %&gt;% summarise(freq = sum(frequency)) %&gt;% mutate(verb = &quot;antworten&quot;) verb5 = rufen %&gt;% group_by(group) %&gt;% summarise(freq = sum(frequency)) %&gt;% mutate(verb = &quot;rufen&quot;) Die fünf kleinen Tabellen können zu einer größeren zusammengefügt werden, z. B. mit der Funktion rbind() oder mit bind_rows(). glagoli = rbind(verb1, verb2, verb3, verb4, verb5) glagoli %&gt;% rmarkdown::paged_table() Noch ein Diagramm: glagoli %&gt;% ggplot(aes(freq, verb, fill = verb)) + geom_col() + facet_wrap(~ group) + theme(legend.position = &quot;none&quot;) Um den Vergleich von Texten zu erleichtern, kann die Tabelle auch umgestellt werden, und zwar mit der pivot_wider()-Funktion: glagoli %&gt;% pivot_wider(id_cols = verb, names_from = group, values_from = freq) %&gt;% rmarkdown::paged_table() 10.9 Kollokationen Kollexeme sind Lexikoneinheiten, die zusammen verwendet werden. Kollokationen sind sprachliche Elemente, die gemeinsam vorkommen. Statistische Definition: Wenn zwei Ausdrücke (z. B. Guten Tag) im Vergleich zu ihren unmittelbaren Nachbarn signifikant häufiger vorkommen, als man nach dem Zufall erwarten könnte, dann können sie als Kollokation betrachtet werden. Linguistische Definition: Eine Kollokation ist eine semantisch verwandte Folge von Wörtern. In quanteda steht uns die Funktion textstat_collocations() zur Auffindung von Kollokationen (im statistischen Sinne) zur Verfügung. woerter ist die Liste der Wortformen (padding = TRUE ist hier notwendigerweise zu setzen!), die wir oben erstellt haben. Kollokationen mit zwei Gliedern: coll_2 = textstat_collocations(woerter, size = 2, tolower = TRUE) # naredi male rke ! coll_2 %&gt;% rmarkdown::paged_table() Dreigliedrige Kollokationen: coll_3 = textstat_collocations(woerter, size = 3, tolower = FALSE) coll_3 %&gt;% rmarkdown::paged_table() Viergliedrige Kollokationen: coll_4 = textstat_collocations(woerter, size = 4, tolower = FALSE) coll_4 %&gt;% rmarkdown::paged_table() Mit welchen Wortformen kommen die synonymen Fragewörter warum und wieso in unserem Korpus häufiger gemeinsam vor? warum &lt;- coll_2 %&gt;% filter(str_detect(collocation, &quot;^warum&quot;)) warum %&gt;% rmarkdown::paged_table() wieso &lt;- coll_2 %&gt;% filter(str_detect(collocation, &quot;^wieso&quot;)) wieso %&gt;% rmarkdown::paged_table() Kollokation von Nominalphrasen (NP). Im Deutschen werden substantivische Wörter groß geschrieben. Daher erstellen wir zunächst eine Liste der großgeschriebenen Wortformen im Korpus (woerter_caps). Daraus können wir eine Liste von Kollokationen erhalten (coll_caps2). woerter_caps = tokens_select(woerter, pattern = &quot;^[A-Z]&quot;, valuetype = &quot;regex&quot;, case_insensitive = FALSE, padding = TRUE) coll_caps2 = textstat_collocations(woerter_caps, size = 2, tolower = FALSE, min_count = 5) coll_caps2 %&gt;% rmarkdown::paged_table() Es macht keinen Sinn, die Wortverbindung Der/Die/Das + Substantiv als Kollokation zu betrachten, da die überwiegende Mehrheit der Substantive im Deutschen mit dem Artikel auftritt. Deshalb werden wir den großen Anfangsbuchstaben von einigen Funktionswörtern am Satzanfang (z.B. die Artikel Der, Die, Das und einige andere Wortformen), in Kleinbuchstaben umwandeln. woerter_small = tokens_replace( woerter, pattern = c(&quot;Der&quot;,&quot;Die&quot;,&quot;Das&quot;,&quot;Des&quot;, &quot;Wollen&quot;,&quot;Im&quot;,&quot;Zum&quot;, &quot;Kein&quot;,&quot;Jeden&quot;,&quot;Wenn&quot;, &quot;Als&quot;, &quot;Da&quot;,&quot;Aber&quot;, &quot;Und&quot;,&quot;Sehen&quot;), replacement = c(&quot;der&quot;,&quot;die&quot;,&quot;das&quot;, &quot;des&quot;,&quot;wollen&quot;,&quot;im&quot;, &quot;zum&quot;, &quot;kein&quot;,&quot;jeden&quot;, &quot;wenn&quot;, &quot;als&quot;,&quot;da&quot;, &quot;aber&quot;, &quot;und&quot;,&quot;sehen&quot;)) woerter_caps = tokens_select(woerter_small, pattern = &quot;^[A-Z]&quot;, valuetype = &quot;regex&quot;, case_insensitive = FALSE, padding = TRUE) coll_caps2 = textstat_collocations(woerter_caps, size = 2, tolower = FALSE, min_count = 5) coll_caps2 %&gt;% rmarkdown::paged_table() 10.10 Lemmatisierung Ein Lemma ist eine Lexikoneinheit. Für Substantive wird gewöhnlich der Nominativ Singular, für Verben der Infinitiv und für Adjektive der Positiv im Nominativ Singular als Lemmaform verwendet. Listen deutscher Lexikoneinheiten (Lemmata) oder anderer Sprachen kann man aus dem Internet auf die eigene Festplatte herunterladen. Im folgenden Programmblock setzen wir solch eine Lemmaliste aus dem Internet zur Lemmatisierung der Wortformen in unserem Korpus ein. Das bewerkstelligen wir mit der quanteda-Funktion tokens_replace(). # Preberi seznam slovarskih enot in pojavnic z diska lemdict = read.delim2(&quot;data/lemmatization_de.txt&quot;, sep = &quot;\\t&quot;, encoding = &quot;UTF-8&quot;, col.names = c(&quot;lemma&quot;, &quot;word&quot;), stringsAsFactors = F) # Pretvori podatkovna niza v znakovna niza lemma = as.character(lemdict$lemma) word = as.character(lemdict$word) # Lematiziraj pojavnice v naih besedilih lemmas &lt;- tokens_replace(besede, pattern = word, replacement = lemma, case_insensitive = TRUE, valuetype = &quot;fixed&quot;) Anschließen erstellen wir eine Matrix mit Lemmas (anstelle von Wortformen). matrika_lem = dfm(lemmas, tolower = FALSE) # za zdaj obdrimo velike zaetnice # Odstranimo besede, ki jih v vsebinski analizi ne potrebujemo (stopwords) matrika_lem = dfm_select(matrika_lem, selection = &quot;remove&quot;, pattern = stoplist_de) matrika_lem ## Document-feature matrix of: 2 documents, 10,072 features (38.04% sparse) and 0 docvars. ## features ## docs Prozess franzen Kafka Verhaftung Gespräch Frau Grubach Fräulein ## prozess.txt 2 24 2 19 18 121 50 112 ## tom.txt 0 0 0 0 5 27 0 0 ## features ## docs Brüstner Jemand ## prozess.txt 1 2 ## tom.txt 0 1 ## [ reached max_nfeat ... 10,062 more features ] 10.11 Wortwolken Mit der quanteda-Funktion texplot_wordcloud() erstellen wir eine einfache Wortwolke aus der zuvor gespeicherten Dokument-Häufigkeits-Matrix, in der die Wortformen durch Lemmas ersetzt wurden. textplot_wordcloud(matrika_lem, comparison = TRUE, adjust = 0.3, color = c(&quot;darkblue&quot;,&quot;darkgreen&quot;), max_size = 4, min_size = 0.75, rotation = 0.5, min_count = 30, max_words = 250) Ästhetisch ansprechendere Wortwolken bilden wir mit dem Programm wortcloud2. Die erste Wortwolke zeigt Wörter aus dem ersten Text, die andere für den zweiten. In beiden Fällen verwenden wir die zuvor gespeicherte Dokument-Häufigkeits-Matrix, filtern sie jedoch durch die Hinzufügung eines Wertes in eckigen Klammern: matrika_lem[1,] bzw. matrika_lem[2,]. Diese Schreibweise in R besagt, dass alle Spalten der Matrix verwendet werden sollen, aber nur die erste bzw. zweite Zeile. In den Spalten der Matrix stehen nämlich die Lemmas, die erste Zeile bezieht sich auf den ersten Text und die zweite auf den zweiten Text. # install.packages(&quot;wordcloud2) matrika_lem_prozess = matrika_lem[1,] set.seed(1320) library(wordcloud2) topfeat &lt;- as.data.frame(topfeatures(matrika_lem_prozess, 100)) topfeat &lt;- rownames_to_column(topfeat, var = &quot;word&quot;) wordcloud2(topfeat) matrika_lem_tom = matrika_lem[2,] set.seed(1321) library(wordcloud2) topfeat2 &lt;- as.data.frame(topfeatures(matrika_lem_tom, 100)) topfeat2 &lt;- rownames_to_column(topfeat2, var = &quot;word&quot;) wordcloud2(topfeat2) 10.12 Position im Text (xray) Ein xray-Diagramm zeigt die Verteilung einer Zeichenkette in einem oder mehreren Texten des Korpus an. Im folgenden Beispiel mit der quanteda-Funktion textplot_xray() sieht man schematisch, an welchen Textstellen das deutsche Wort frau in den Texten erscheint. Derartige Diagramme sind auch unter anderen Bezeichnungen geläufig: z.B. Barcode- oder Strichcode-Diagramm, Dispersionsdiagramm. Eine ähnliche graphische Funktion, genannt MicroSearch, begegnet uns auch in Voyant Tools. kwic_frau = kwic(lemmas, pattern = &quot;frau&quot;) textplot_xray(kwic_frau) Die folgende Funktion dispersion_plot() haben wir selber zusammengestellt und soll wie die Python-Bibliothek NLTK möglichst benutzerfreundlich sein. Angegeben werden muss lediglich der Name eines beliebigen Textes und ein beliebiges Wort, dass im Text ausfindig gemacht werden soll. Unter der Haube der selbstgebastelten Funktion wird der Text in Wortformen zerlegt, eine KWIC()-Recherche durchgeführt und letztendlich ein xray-Diagramm geplottet. Vorausgesetzt werden die beiden Programme quanteda und quanteda.textplots. dispersion_plot &lt;- function(text, word){ library(quanteda); library(quanteda.textplots) tokens({text}) %&gt;% kwic({word}) %&gt;% textplot_xray() } Im folgenden Beispiel möchten wir wissen, wo in unseren beiden Texten die Phrase die frau vorkommt. So wie in der kWIC()-Funktion der quanteda-Bibliothek kommt die (phrase()-Funktion zum Einsatz, da wir nicht nach einem einzelnen Wort suchen sondern nach einer Wortverbindung. dispersion_plot(txt$text, phrase(&quot;die frau&quot;)) 10.13 Lexikalische Vielfalt Mit Hilfe der quanteda-Funktion textstat_lexdiv(), die als Eingabe eine Dokument-Frequenz-Matrix verlangt, können verschiedene Indices für lexikalische Diversität (Wortvielfalt, Wortreichtum) berechnet werden, d.h. wie viele verschiedene Wortformen in einem Text vorkommen. Der bekannteste Quotient zur Einschätzung des Formenreichtums ist das Type-Token-Verhältnis. Da aber letztgenanntes Verhältnis von der Länge des Textes bzw. der Größe des Textkorpus abhängt, sind auch andere Indices im Einsatz, um den eben genannten Nachteil zu kompensieren. textstat_lexdiv(matrika, measure = &quot;all&quot;) ## document TTR C R CTTR U S K ## 1 prozess.txt 0.2457355 0.8651285 44.68334 31.59590 33.50861 0.9039511 22.33655 ## 2 tom.txt 0.2939104 0.8828536 54.69657 38.67631 38.75057 0.9176397 11.48121 ## I D Vm Maas lgV0 lgeV0 ## 1 26.76129 0.002233722 0.04626902 0.1727515 7.795478 17.94975 ## 2 73.92612 0.001148154 0.03284439 0.1606427 8.533417 19.64892 10.14 Textähnlichkeit Mit der quanteda-Funktion textsat_simil() kann die Ähnlichkeit von Texten berechnet werden. Dieses Verfahren ist vor allem dann interessant, wenn wir mehr als zwei Texte vergleichen wollen. Deshalb fügen wir unserem bisherigen Textkorpus noch eine Novelle von Kafka hinzu. # odpremo datoteko verwandl = readtext(&quot;data/books/verwandlung/verwandlung.txt&quot;, encoding = &quot;UTF-8&quot;) # ustvarimo nov korpus verw_corp = corpus(verwandl) # zdruimo novi korpus s prrejnjim romane3 = romane + verw_corp # tokenizacija romane3_toks = tokens(romane3) # ustvarimo matriko (dfm) romane3_dfm = dfm(romane3_toks) Das Ergebnis der Berechnung: Kafkas Novelle Die Verwandlung hat etwas mehr Ähnlichkeit mit Kafkas Roman Der Prozess als mit Twains Roman Tom Sawyer. textstat_simil(romane3_dfm, method = &quot;cosine&quot;, margin = &quot;documents&quot;) ## textstat_simil object; method = &quot;cosine&quot; ## prozess.txt tom.txt verwandlung.txt ## prozess.txt 1.000 0.930 0.948 ## tom.txt 0.930 1.000 0.933 ## verwandlung.txt 0.948 0.933 1.000 Statt die textstat_simil()-Funktion mit ganzen Texten zu konfrontieren, kann man auch die Ähnlichkeit von ausgewählten Wortformen (margin = features) berechnen lassen. # compute some term similarities simil1 = textstat_simil(matrika, matrika[, c(&quot;Josef&quot;, &quot;Tom&quot;, &quot;Sawyer&quot;, &quot;Huck&quot;, &quot;Finn&quot;)], method = &quot;cosine&quot;, margin = &quot;features&quot;) head(as.matrix(simil1), 10) ## Josef Tom Sawyer Huck Finn ## Prozess 0.9991331 0.0000000 0.0000000 0.0000000 0.0000000 ## Franz 0.9991331 0.0000000 0.0000000 0.0000000 0.0000000 ## Kafka 0.9991331 0.0000000 0.0000000 0.0000000 0.0000000 ## Verhaftung 0.9991331 0.0000000 0.0000000 0.0000000 0.0000000 ## Gespräch 0.9793983 0.2425356 0.2425356 0.2425356 0.2425356 ## Frau 0.9933995 0.1559626 0.1559626 0.1559626 0.1559626 ## Grubach 0.9991331 0.0000000 0.0000000 0.0000000 0.0000000 ## Fräulein 0.9991331 0.0000000 0.0000000 0.0000000 0.0000000 ## Brüstner 0.9991331 0.0000000 0.0000000 0.0000000 0.0000000 ## Jemand 0.9122695 0.4472136 0.4472136 0.4472136 0.4472136 Die Unterschiedlichkeit oder Distanz von Texten kann man mit Hilfe der textstat_dist() berechnen lassen. # plot a dendrogram after converting the object into distances dist1 = textstat_dist(romane3_dfm, method = &quot;euclidean&quot;, margin = &quot;documents&quot;) plot(hclust(as.dist(dist1))) 10.15 Schlüsselwörter (keywords) Welche Wortformen können als Schlüsselwörter für einen Text angesehen werden, d.h. als Begriffe, die für einen bestimmten Text charakteristisch sind und ihn von anderen unterscheiden? Mit der quanteda-Funktion textstat_keyness() vergleichen wir einen Zieltext (target) mit einem Referenztext (reference). key_tom &lt;- textstat_keyness(matrika, target = &quot;tom.txt&quot;) key_tom %&gt;% rmarkdown::paged_table() key_prozess &lt;- textstat_keyness(matrika, target = &quot;prozess.txt&quot;) key_prozess %&gt;% rmarkdown::paged_table() textplot_keyness(key_tom, key_tom$n_target == 1) textplot_keyness(key_tom, key_prozess$n_target == 1) textplot_keyness(key_tom) textplot_keyness(key_prozess) 10.16 Lesbarkeit des Textes Ein Lesbarkeitsindex (readibility index) gibt Auskunft darüber, wie schwer ein Lesetext zu verstehen ist. Es gibt eine ganze Reihe von Lesbarkeitsindices, eine ganze Palette davon macht auch quanteda verfügbar. Die meisten sind an die englische Sprache angepasst und haben daher für andere Sprachen eingeschränkte Aussagekraft. Grundlage für die verschiedenen Lesbarkeitsindices sind meist Größen, die sich auf die Satz- und Wortlänge beziehen. Sätze, die aus vielen Wörtern bestehen, und Wörter, die aus vielen Silben oder vielen Buchstaben bestehen, sind meist nicht so leicht und schnell zu verarbeiten wie kürzere Sätze und Wörter. Der Flesch-Index ist einer bekanntesten Lesbarkeitsindices. Es gibt sogar eine Version, die an deutschsprachige Texte angepasst ist. In unserem Textkorpus zeigt sich, dass der Roman Der Prozess einen etwas niedrigeren Wert (52) hat als Tom Sawyer (61). Der niedrigere Indexwert bedeutet, dass Kafkas Prozess - wohl wegen der im Durchschnitt etwas längeren Sätze und Wörter - schwieriger zu lesen bzw. zu verstehen ist als Tom Sawyer. textstat_readability(romane, measure = c(&quot;Flesch&quot;, &quot;Flesch.Kincaid&quot;, &quot;FOG&quot;, &quot;FOG.PSK&quot;, &quot;FOG.NRI&quot;)) ## document Flesch Flesch.Kincaid FOG FOG.PSK FOG.NRI ## 1 prozess.txt 51.94715 10.644645 13.04497 6.390374 8545.508 ## 2 tom.txt 60.58142 8.395483 10.61185 5.074038 6016.218 10.17 Kookurrenz-Netzwerk (FCM) Die Feature-Kookurrenz-Matrix (FCM) gibt Auskunft darüber, welche Wörter in in einem Text oder Textkorpus häufiger miteinander verknüpft werden. Eine FCM wird in zwei Schritten ermittelt: - Zunächst wird eine Liste von Ausdrücken (pattern, Muster) aus einer vorher gespeicherten Matrix (dfm) ausgewählt, und zwar mit der dfm_select()-Funktion, - dann wird die Feature-Kookurrenz-Matrix mit Hilfe der fcm()-Funktion erstellt. Hier folgt ein Beispiel für den zweiten Text (Tom Sawyer). dfm_tags &lt;- dfm_select( matrika[2,], pattern = (c(&quot;tom&quot;, &quot;huck&quot;, &quot;*joe&quot;, &quot;becky&quot;, &quot;tante&quot;, &quot;witwe&quot;, &quot;polly&quot;, &quot;sid&quot;, &quot;mary&quot;, &quot;thatcher&quot;, &quot;höhle&quot;, &quot;herz&quot;, &quot;*schule&quot;, &quot;katze&quot;, &quot;geld&quot;, &quot;zaun&quot;, &quot;piraten&quot;, &quot;schatz&quot;))) toptag &lt;- names(topfeatures(dfm_tags, 50)) head(toptag) ## [1] &quot;Tom&quot; &quot;Huck&quot; &quot;Joe&quot; &quot;Becky&quot; &quot;Tante&quot; &quot;Sid&quot; Die FCM kann mit Hilfe der quanteda-Funktion textplot_network() graphisch dargestellt werden. Als Eingabe dient eine FCM, die aber vorher gefiltert werden muss, um Übersichtlichkeit oder Interpretierbarkeit zu gewährleisten. # Construct feature-cooccurrence matrix (fcm) of tags fcm_tom &lt;- fcm(matrika[2,]) # besedilo 2 je tom.txt head(fcm_tom) ## Feature co-occurrence matrix of: 6 by 15,185 features. ## features ## features Prozess Franz Kafka Verhaftung Gespräch Frau Grubach Fräulein ## Prozess 0 0 0 0 0 0 0 0 ## Franz 0 0 0 0 0 0 0 0 ## Kafka 0 0 0 0 0 0 0 0 ## Verhaftung 0 0 0 0 0 0 0 0 ## Gespräch 0 0 0 0 6 72 0 0 ## Frau 0 0 0 0 0 153 0 0 ## features ## features Brüstner Jemand ## Prozess 0 0 ## Franz 0 0 ## Kafka 0 0 ## Verhaftung 0 0 ## Gespräch 0 4 ## Frau 0 18 ## [ reached max_nfeat ... 15,175 more features ] top_fcm &lt;- fcm_select(fcm_tom, pattern = toptag) textplot_network(top_fcm, min_freq = 0.6, edge_alpha = 0.8, edge_size = 5) 10.18 Grammatische Analyse Spezielle Programme (z.B. spacyr oder udpipe) können zur grammatikalischen Analyse und Lemmatisierung von Wortformen eingesetzt werden. Das Programm spacyr verlangt eine zusätzliche Python-Umgebung. Es ist für verbreitete europäische Sprachen wie Englisch, Französich, Deutsch und einige weitere einsetzbar, aber leider nicht für Slowenisch. Das Programm udpipe hat zwei Vorteile: (a) es verlangt lediglich eine R-Installation und (b) es ist zur Zeit für mehr als sechzig Sprachen verfügbar, neben Englisch und Deutsch auch für Slowenisch. 10.18.1 Vorbereitung Vor der ersten Benutzung müssen wir das deutsche Sprachmodell aus dem Internet herunterladen. Im nächsten Schritt laden wir das Sprachmodell in den Arbeitsspeicher unseres Computers, und zwar mit udpipe_load_model(). Der nachfolgende Programmblock überprüft zuerst, ob das gewünschte Modell für eine bestimmte Sprache bereits im Arbeitsverzeichnis auf der Festplatte unseres Computers gespeichert ist. Ist es noch nicht auf der Festplatte, wird das Sprachmodell heruntergeladen und anschließend in den Arbeitsspeicher geladen. Ist das Sprachmodell bereits im Arbeitsverzeichnis, wird es sofort in den Arbeitsspeicher geladen und nicht noch einmal aus dem Internet heruntergeladen. library(udpipe) destfile = &quot;german-gsd-ud-2.5-191206.udpipe&quot; if(!file.exists(destfile)){ sprachmodell &lt;- udpipe_download_model(language = &quot;german&quot;) udmodel_de &lt;- udpipe_load_model(sprachmodell$file_model) } else { file_model = destfile udmodel_de &lt;- udpipe_load_model(file_model) } Wenn sich das Sprachmodell bereits im Arbeitsverzeichnis Ihres Computers befindet, können Sie es auch auf folgende Weise ausführen: Der nächste Programmschritt ist die Annotation der Texte, und zwar mit der Funktion udpipe_annotate(). Die Wortformen in den Texten werden nach nun verschiedenen grammatischen Kriterien bestimmt. # Na zaetku je readtext prebral besedila, shranili smo jih v spremenljivki &quot;txt&quot;. x &lt;- udpipe_annotate(udmodel_de, x = txt$text, trace = TRUE) ## 2022-03-20 21:06:46 Annotating text fragment 1/2 ## 2022-03-20 21:08:46 Annotating text fragment 2/2 # # samo prvo besedilo: # x &lt;- udpipe_annotate(udmodel_de, x = txt$text[1], trace = TRUE) x &lt;- as.data.frame(x) Die Struktur des Datensatzes: str(x) ## &#39;data.frame&#39;: 174925 obs. of 14 variables: ## $ doc_id : chr &quot;doc1&quot; &quot;doc1&quot; &quot;doc1&quot; &quot;doc1&quot; ... ## $ paragraph_id : int 1 1 1 1 1 1 1 1 1 1 ... ## $ sentence_id : int 1 1 1 1 1 1 1 1 1 1 ... ## $ sentence : chr &quot;Der Prozess by Franz Kafka Aligned by : bilingual-texts.com ( fully reviewed ) Der Prozess Franz Kafka 1 Verhaf&quot;| __truncated__ &quot;Der Prozess by Franz Kafka Aligned by : bilingual-texts.com ( fully reviewed ) Der Prozess Franz Kafka 1 Verhaf&quot;| __truncated__ &quot;Der Prozess by Franz Kafka Aligned by : bilingual-texts.com ( fully reviewed ) Der Prozess Franz Kafka 1 Verhaf&quot;| __truncated__ &quot;Der Prozess by Franz Kafka Aligned by : bilingual-texts.com ( fully reviewed ) Der Prozess Franz Kafka 1 Verhaf&quot;| __truncated__ ... ## $ token_id : chr &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ token : chr &quot;Der&quot; &quot;Prozess&quot; &quot;by&quot; &quot;Franz&quot; ... ## $ lemma : chr &quot;der&quot; &quot;Prozeß&quot; &quot;by&quot; &quot;Franz&quot; ... ## $ upos : chr &quot;DET&quot; &quot;NOUN&quot; &quot;PROPN&quot; &quot;PROPN&quot; ... ## $ xpos : chr &quot;ART&quot; &quot;NN&quot; &quot;NE&quot; &quot;NE&quot; ... ## $ feats : chr &quot;Case=Nom|Definite=Def|Gender=Masc|Number=Sing|PronType=Art&quot; &quot;Case=Nom|Gender=Masc|Number=Sing&quot; &quot;Case=Nom|Gender=Masc|Number=Sing&quot; &quot;Case=Nom|Gender=Masc|Number=Sing&quot; ... ## $ head_token_id: chr &quot;2&quot; &quot;72&quot; &quot;2&quot; &quot;3&quot; ... ## $ dep_rel : chr &quot;det&quot; &quot;nsubj&quot; &quot;appos&quot; &quot;flat&quot; ... ## $ deps : chr NA NA NA NA ... ## $ misc : chr NA NA NA NA ... So sieht der Datensatz mit annotatierten Wortformen und Lemmatisierung aus: head(x, 10) %&gt;% rmarkdown::paged_table() 10.18.2 Vergleich Nomen : Pronomen Nach der automatischen, mit Überschallgeschwindigkeit durchgeführten Annotation können wir uns mit der Analyse grammatischer Kategorien von Wortformen und Lemmas beschäftigen. Wie häufig kommen universalen Wortklassen (upos) in den Texten vor? (tabela = x %&gt;% group_by(doc_id) %&gt;% count(upos) %&gt;% filter(!is.na(upos), upos != &quot;PUNCT&quot;) ) ## # A tibble: 28 x 3 ## # Groups: doc_id [2] ## doc_id upos n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 doc1 ADJ 5284 ## 2 doc1 ADP 6350 ## 3 doc1 ADV 8387 ## 4 doc1 AUX 4390 ## 5 doc1 CCONJ 2425 ## 6 doc1 DET 8050 ## 7 doc1 NOUN 10705 ## 8 doc1 NUM 155 ## 9 doc1 PART 1984 ## 10 doc1 PRON 11280 ## # ... with 18 more rows tabela %&gt;% mutate(upos = reorder_within(upos, n, n, sep = &quot;: &quot;)) %&gt;% ggplot(aes(n, upos, fill = upos)) + geom_col() + facet_wrap(~ doc_id, scales = &quot;free&quot;) + theme(legend.position = &quot;none&quot;) + labs(x = &quot;tevilo pojavnic&quot;, y = &quot;&quot;) Zur besseren Vergleichbarkeit fügen wir noch die entsprechenden Prozentwerte hinzu. (delezi = tabela %&gt;% mutate(prozent = n/sum(n)) %&gt;% pivot_wider(id_cols = upos, names_from = doc_id, values_from = n:prozent) ) ## # A tibble: 14 x 5 ## upos n_doc1 n_doc2 prozent_doc1 prozent_doc2 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ADJ 5284 5539 0.0729 0.0818 ## 2 ADP 6350 5524 0.0877 0.0816 ## 3 ADV 8387 6706 0.116 0.0990 ## 4 AUX 4390 3386 0.0606 0.0500 ## 5 CCONJ 2425 3270 0.0335 0.0483 ## 6 DET 8050 6888 0.111 0.102 ## 7 NOUN 10705 10871 0.148 0.160 ## 8 NUM 155 306 0.00214 0.00452 ## 9 PART 1984 1658 0.0274 0.0245 ## 10 PRON 11280 9027 0.156 0.133 ## 11 PROPN 2317 3919 0.0320 0.0579 ## 12 SCONJ 1687 1296 0.0233 0.0191 ## 13 VERB 9401 8669 0.130 0.128 ## 14 X 20 678 0.000276 0.0100 Welche Anteile haben die beiden Wortklassen Nomen und Pronomen in den beiden Texten? delezi %&gt;% filter(upos %in% c(&quot;NOUN&quot;, &quot;PRON&quot;)) ## # A tibble: 2 x 5 ## upos n_doc1 n_doc2 prozent_doc1 prozent_doc2 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 NOUN 10705 10871 0.148 0.160 ## 2 PRON 11280 9027 0.156 0.133 Sind die Anteile der Nomina und Pronomina in beiden Texten ähnlich oder verschieden? Ein \\(\\chi^2\\)-Test könnte uns eine erste Antwort auf diese Frage geben. # za hi kvadrat test potrebujemo le drugi in tretji stolpec nominal = delezi %&gt;% filter(upos %in% c(&quot;NOUN&quot;, &quot;PRON&quot;)) %&gt;% dplyr::select(n_doc1, n_doc2) chisq.test(nominal) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: nominal ## X-squared = 147.38, df = 1, p-value &lt; 2.2e-16 Die beiden Texte unterscheiden sich mit statistischer Signifikanz voneinander: \\(\\chi^2\\) (1) = 147,38; p &lt; 0,001. Die obige Häufigkeitstabelle zeigt, dass der Anteil der Pronomina im Prozess vergleichsweise höher ist als im Tom Sawyer. Für eine verlässliche sprachwissenschaftliche Interpretation, müsste man sich genauer anschauen, welche Pronomina und welche Nomina einen starken Einfluss auf dieses Zahlenverhältnis haben. Semantisch gesehen lassen sich Pronomina nicht so eindeutig auf ein Antezedens (vorangegangenes Bezugsobjekt) beziehen wie Nomina und daher weniger zuverlässige sprachliche Mittel als Nomina, insbesondere wenn die Distanz zwischen Antezedens und Pronomen größer ist oder aufgrund von konkurrierenden Bezugsobjekten schwierig ist. Formell betrachtet sind Pronomina allerdings weniger komplex als Nomina. Wenn wir eine Wortart mit allen anderen im Datensatz vergleichen wollen, ist die Umrechnung komplizierter, denn wie in Excel müssen wir - zuerst die Summe aller Wortarten berechnen, - dann die Anzahl der Pronomina bzw. Nomina von der Gesamtsumme subtrahieren, - und letztendlich die Differenz in der 2x2-Tabelle für den \\(\\chi^2\\)-Test berücksichtigen. (zaimki = x %&gt;% group_by(doc_id) %&gt;% count(upos) %&gt;% filter(!is.na(upos), upos != &quot;PUNCT&quot;) %&gt;% mutate(vsota = sum(n), no_noun = vsota - n[upos == &quot;NOUN&quot;], no_pron = vsota - n[upos == &quot;PRON&quot;]) %&gt;% filter(upos == &quot;PRON&quot;) %&gt;% dplyr::select(doc_id, n, no_pron) %&gt;% pivot_longer(-doc_id, names_to = &#39;kategorija&#39;, values_to = &#39;vrednost&#39;) %&gt;% pivot_wider(id_cols = kategorija, names_from = doc_id, values_from = vrednost) ) ## # A tibble: 2 x 3 ## kategorija doc1 doc2 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 n 11280 9027 ## 2 no_pron 61155 58710 (samostalniki = x %&gt;% group_by(doc_id) %&gt;% count(upos) %&gt;% filter(!is.na(upos), upos != &quot;PUNCT&quot;) %&gt;% mutate(vsota = sum(n), no_noun = vsota - n[upos == &quot;NOUN&quot;], no_pron = vsota - n[upos == &quot;PRON&quot;]) %&gt;% filter(upos == &quot;NOUN&quot;) %&gt;% dplyr::select(doc_id, n, no_noun) %&gt;% pivot_longer(-doc_id, names_to = &#39;kategorija&#39;, values_to = &#39;vrednost&#39;) %&gt;% pivot_wider(id_cols = kategorija, names_from = doc_id, values_from = vrednost) ) ## # A tibble: 2 x 3 ## kategorija doc1 doc2 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 n 10705 10871 ## 2 no_noun 61730 56866 Wir führen zwei \\(\\chi^2\\)-Tests durch, und zwar: - einen, um die Anzahl der Pronomina mit der Anzahl anderer Wortarten zu vergleichen, - und einen, um die Anzahl der Nomina mit anderen Wortarten zu vergleichen. # izloimo prvi stolpec [, -1], za hi kvadrat test potrebujemo le drugi in tretji stolpec chisq.test(zaimki[,-1]) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: zaimki[, -1] ## X-squared = 142.36, df = 1, p-value &lt; 2.2e-16 chisq.test(samostalniki[,-1]) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: samostalniki[, -1] ## X-squared = 43.259, df = 1, p-value = 4.796e-11 Die beiden statistischen Tests zeigen einen statistisch signifikanten Unterschied zwischen den beiden Texten an. Allerdings ist statitische Signifikanz bei so großen Stichproben wahrscheinlicher als bei kleinen. 10.18.3 Konjunktionen im Vergleich Als nächstes haben wir vor, die Anzahl der Sätze mit koordinierender oder subordinierender Konjunktion miteinander zu vergleichen. Die Grundannahme ist, dass Parataxe leichter zu verstehen ist als Hypotaxe. Parataxe bedeutet, dass zwei oder mehrere Sätze in einer sprachlichen Äußerungen nebengeordnet (koordiniert, gleichrangig) sind, Hypotaxe dagegen, das ein oder mehrere Sätze in einer sprachlichen Äußerung einem anderen Satz untergeordnet (subordiniert) ist. In der folgenden Auszählung berücksichtigen wir lediglich Sätze, die mit einem Junktor (koordinierender Konjunktion, CCONJ) oder einem Subjunktor (subordinierender Konjunktion, SCONJ) eingeleitet sind. Das bedeutet, dass beispielweise Konjunktionaladverbien, die nebengeordnete Sätze miteinander verbinden können, oder Relativpronomen, die untergeordnete Sätze einleiten können, in der Auszählung nicht berücksichtigt werden. Die Hypothesen, die beim statistischen Test geprüft werden sollen, lauten folgendermaßen: - \\(H_0\\): Das zahlenmäßige Verhältnis zwischen Junktoren und Subjunktoren in den beiden Romanen ist gleich. - \\(H_1\\): Das zahlenmäßige Verhältnis zwischen Junktoren und Subjunktoren in den beiden Romanen unterscheidet sich. (vezniki = tabela %&gt;% filter(upos %in% c(&quot;CCONJ&quot;, &quot;SCONJ&quot;)) %&gt;% mutate(prozent = n/sum(n)) %&gt;% pivot_wider(id_cols = upos, names_from = doc_id, values_from = n:prozent) ) ## # A tibble: 2 x 5 ## upos n_doc1 n_doc2 prozent_doc1 prozent_doc2 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 CCONJ 2425 3270 0.590 0.716 ## 2 SCONJ 1687 1296 0.410 0.284 Die Prozentzahlen weisen darauf hin, dass der Anteil der koordinierenden Konjunktionen im Roman Prozess (doc1: ca. 59%) kleiner ist als im Roman Tom Sawyer (doc2: ca. 72%). Das könnte bedeuten, dass im Tom Sawyer mehr Parataxe verwendet wird als im Prozess. Da es sich aber um Stichproben beider Romane handelt, müssen wir einen geeigneten statistischen Test durchführen, um den beobachteten Prozentunterschied (Häufigkeitsunterschied) zu bestätigen. Mit dem \\(\\chi^2\\)-Test prüfen wir, ob der Häufigkeitsunterschied zwischen den beiden Romanstichproben signifikant ist, also nach statistischen Kriterien groß genug ist, um die die statistische Hypothese \\(H_1\\) zu bestätigen oder zu verwerfen. chisq.test(vezniki[,c(2:3)]) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: vezniki[, c(2:3)] ## X-squared = 152.74, df = 1, p-value &lt; 2.2e-16 Der statistische Test bestätigt, dass der Unterschied zwischen den beiden Romanstichproben statistisch signifikant ist und dass damit die Hypothese \\(H_1\\) angenommen werden kann. In dem soeben durchgeführten Test haben wir nur die Anzahl der Junktoren und Subjunktoren berücksichtigt. Ändert sich das statistische Ergebnis, wenn wir in der Tabelle auch die übrigen Wortarten einbeziehen? Das soll im folgenden Programmblock überprüft werden. Zuerst müssen wir die beiden relevanten Tabellen erstellen. Die erste enthält die Häufigkeiten von Junktoren im Vergleich zu Nicht-Junktoren, die zweite dagegen die Häufigkeiten von Subjunktoren im Vergleich zu Nicht-Junktoren. (koord = tabela %&gt;% mutate(vsota = sum(n), no_cconj = vsota - n[upos == &quot;CCONJ&quot;], no_sconj = vsota - n[upos == &quot;SCONJ&quot;]) %&gt;% filter(upos == &quot;CCONJ&quot;) %&gt;% dplyr::select(doc_id, n, no_cconj) %&gt;% pivot_longer(-doc_id, names_to = &#39;kategorija&#39;, values_to = &#39;vrednost&#39;) %&gt;% pivot_wider(id_cols = kategorija, names_from = doc_id, values_from = vrednost) ) ## # A tibble: 2 x 3 ## kategorija doc1 doc2 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 n 2425 3270 ## 2 no_cconj 70010 64467 (subord = tabela %&gt;% mutate(vsota = sum(n), no_cconj = vsota - n[upos == &quot;CCONJ&quot;], no_sconj = vsota - n[upos == &quot;SCONJ&quot;]) %&gt;% filter(upos == &quot;SCONJ&quot;) %&gt;% dplyr::select(doc_id, n, no_sconj) %&gt;% pivot_longer(-doc_id, names_to = &#39;kategorija&#39;, values_to = &#39;vrednost&#39;) %&gt;% pivot_wider(id_cols = kategorija, names_from = doc_id, values_from = vrednost) ) ## # A tibble: 2 x 3 ## kategorija doc1 doc2 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 n 1687 1296 ## 2 no_sconj 70748 66441 Beide \\(\\chi^2\\)-Tests bestätigen einen statistisch signifikanten Unterschied zwischen den beiden Romanen. Die Häufigkeits- bzw. Prozentzahlen deuten darauf hin, dass im Prozess mehr Subjunktoren verwendet werden als im Tom Sawyer und weniger Junktoren. chisq.test(koord[,-1]) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: koord[, -1] ## X-squared = 196.24, df = 1, p-value &lt; 2.2e-16 chisq.test(subord[,-1]) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: subord[, -1] ## X-squared = 28.843, df = 1, p-value = 7.849e-08 Das könnte man so verstehen, dass im Tom Sawyer Parataxe dominanter ist als im Prozess und dass der erste Roman leichter zu verstehen sein könnte als der letztere. Allerdings dürfen wir an dieser Stelle nicht vergessen, dass wir lediglich Stichproben erhoben haben. Wir haben ja lediglich Sätze berücksichtigt, die Junktoren oder Subjunktoren enthalten. Uneingeleitete Sätze oder Sätze, eingeleitet durch Konjunktionaladverbien, Relativpronomina u.a., haben wir in unserer Stichprobenerhebung nicht berücksichtigt. Bei Einbezug solcher Sätze könnte sich das Ergebnis wesentlich ändern. 10.18.4 Lexikalische Einheiten Das Program udpipe hat jede Wortform einem Lemma (einer Lexikoneinheit) zugeordnet. Wie viele Lemmas enthalten die Texte? Zu welchen Wortarten gehören sie am häufigsten? Wir stellen die Verhältnisse tabellarisch und graphisch dar. Die allgegenwärtigen Interpunktionszeichen (Komma, Punkt usw.) werden herausgefiltert. (tabela2 = x %&gt;% group_by(doc_id, upos) %&gt;% filter(!is.na(upos), upos != &quot;PUNCT&quot;, upos != &quot;X&quot;) %&gt;% distinct(lemma) %&gt;% count(lemma) %&gt;% summarise(lemmas = sum(n)) %&gt;% mutate(prozent = round(lemmas/sum(lemmas), 4)) %&gt;% arrange(-prozent) ) ## # A tibble: 26 x 4 ## # Groups: doc_id [2] ## doc_id upos lemmas prozent ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 doc2 NOUN 3401 0.361 ## 2 doc1 NOUN 2519 0.352 ## 3 doc1 VERB 1696 0.237 ## 4 doc1 ADJ 1528 0.213 ## 5 doc2 VERB 1934 0.206 ## 6 doc2 ADJ 1875 0.199 ## 7 doc2 PROPN 973 0.103 ## 8 doc1 ADV 605 0.0845 ## 9 doc2 ADV 671 0.0713 ## 10 doc1 PROPN 387 0.054 ## # ... with 16 more rows tabela2 %&gt;% # slice_max(order_by = prozent, n=6) %&gt;% mutate(upos = reorder_within(upos, lemmas, paste(&quot;(&quot;,100*prozent,&quot;%)&quot;), sep = &quot; &quot;)) %&gt;% ggplot(aes(prozent, upos, fill = upos)) + geom_col() + facet_wrap(~ doc_id, scales = &quot;free&quot;) + theme(legend.position = &quot;none&quot;) + scale_x_continuous(labels = percent_format()) + labs(x = &quot;Anteil&quot;, y = &quot;Wortklasse&quot;) Aus unseren beiden Darstellungen der Lemmahäufigkeit ist ersichtlich, dass in beiden Texten Nomina (noun) am häufigsten vertreten sind (mehr als ein Drittel aller Lemmas), gefolgt von Verben und Adjektiven. Das ist auch für andere Texte das typische Bild, da es sich bei diesen drei Klassen um offene Wortklassen handelt. Funktionswörter gehören zu den geschlossenen Wortklassen, die nur aus relativ wenigen (oft unflektierten) Einheiten bestehen und im nur sehr geringen Maße erweiterbar sind. So machen die in fast allen Texten häufig auftretenden Pronomina (Personalpronomen, demonstrativpronomen, Possessivpronomen usw.) weniger als zwei Prozent aller Lemmas in den beiden Romanen aus. Entsprechendes ist auch bei den anderen Funktionswortklassen zu beobachten: sie haben eine hohe Vorkommenshäufigkeit (Tokenfrequenz), aber geringe Lemmahäufigkeit. 10.18.5 Wortkorrelationen Bei der Inhaltsanalyse kann es von Nutzen sein, mehr darüber zu erfahren, welche Wörter im Text miteinander häufig verknüpft werden. Oben haben wir bereits nach Kollokationen in den beiden Romanen recherchiert. Eine ähnliche Methode ist die Korrelationsanalyse, die sich aber nicht auf das direkte Nacheinanderauftreten von Wörtern beschränkt. Unsere nächste Untersuchungsfrage lautet daher: Welche Worthäufigkeiten nehmen parallel zu oder ab? Können wir also paarweise Korrelationen von Wörtern nachweisen? Zu diesem Zweck setzen wir das Programm widyr ein. Ein ähnliches Analysewerkzeug ist übrigens auch bei Voyant Tools zu finden. Im folgenden Programmchunk erstellen wir eine Tabelle, die miteinander auftretende Lemmas und ihre Korrelation anführt. library(widyr) # pairwise correlation (correlations = x %&gt;% filter(dep_rel != &quot;punct&quot;, dep_rel != &quot;nummod&quot;) %&gt;% mutate(lemma = tolower(lemma), token = tolower(token), lemma = str_trim(lemma), token = str_trim(token)) %&gt;% janitor::clean_names() %&gt;% group_by(doc_id, lemma, token, sentence_id) %&gt;% # add_count(token) %&gt;% summarize(Freq = n()) %&gt;% arrange(-Freq) %&gt;% filter(Freq &gt; 2) %&gt;% pairwise_cor(lemma, sentence_id, sort = TRUE) %&gt;% filter(correlation &lt; 1 &amp; correlation &gt; 0.3) ) ## # A tibble: 2,592 x 3 ## item1 item2 correlation ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 verteidigung natürlich 0.865 ## 2 natürlich verteidigung 0.865 ## 3 stellvertreter direktor 0.812 ## 4 direktor stellvertreter 0.812 ## 5 bürstner fräulein 0.741 ## 6 fräulein bürstner 0.741 ## 7 master jim 0.706 ## 8 depot jim 0.706 ## 9 eimer jim 0.706 ## 10 glaskugel jim 0.706 ## # ... with 2,582 more rows Die Korrelationswerte liegen zwischen 1 und -1. Je stärker die Verbindung zwischen zwei Wortitems (Lemmas), umso höher ist der Korrelationswert. Positive Korrelatonswerte bedeuten, dass zwei Lemmas einander anziehen (d.h. häufiger miteinander auftreten), negative Korrelationswerte dagegen, dass sie einander abstoßen (d.h. seltener miteinander auftreten). Als erstes Beispiel wählen wir das Lemma Zaun aus dem Roman Tom Sawyer. Welche Lemmas sind positiv oder negativ damit korreliert (d.h. treten häufger miteinander auf oder umgekehrt)? correlations %&gt;% filter(item1 == &quot;zaun&quot;) %&gt;% mutate(item2 = fct_reorder(item2, correlation)) %&gt;% ggplot(aes(item2, correlation, fill = item2)) + geom_col(show.legend = F) + coord_flip() + labs(title = &quot;What tends to appear with &#39;Zaun&#39;?&quot;, subtitle = &quot;Among elements that appeared in at least 2 sentences&quot;) Die beiden Lemmas Spaß und Arbeit sind stärker mit Zaun korreliert. Das macht Sinn, wenn man das entsprechende Kapitel im Roman gelesen hat: Tom Sawyer hat von seiner Tante die Aufgabe erhalten, den Gartenzaun anzustreichen. Das macht Tom überhaupt keinen Spaß. Ein Junge kommt vorbei und Tom kommt eine Idee. Er tut so, als würde die Arbeit Spaß machen. Das war nur ein Trick von Tom. Tom gelang es, den anderen Jungen zu überzeugen, dass Zaunanstreichen Spaß macht. Der andere Junge übernahm Toms Arbeit. Ein weiteres Beispiel aus Kafkas Roman Der Prozess wäre das Lemma Gericht. correlations %&gt;% filter(item1 == &quot;gericht&quot;) %&gt;% mutate(item2 = fct_reorder(item2, correlation)) %&gt;% ggplot(aes(item2, correlation, fill = item2)) + geom_col(show.legend = F) + coord_flip() + labs(title = &quot;What tends to appear with &#39;Gericht&#39;?&quot;, subtitle = &quot;Among elements that appeared in at least 2 sentences&quot;) Im Diagramm sind mehrere Lemmas zu sehen, die mittelstark mit dem Lemma Gericht korrelieren. Am meisten Sinn ergeben in diesem Zusammenhang die Lemmas Anklage, unschuldig, frei und zwingen. 10.19 Sentiment Stopnjo ustvenosti ali emocionalnosti besedila je mogoe doloiti s sentimentnim slovarjem. 10.19.1 Version 1 Zuerst soll das nrc-Sentimentlexikon für Deutsch zum Einsatz kommen, das im Programmpaket von syuzhet enthalten ist. Für die Sentimentanalyse mit syuzhet wird ein Text erst einmal mit der Funktion get_sentences() in Sätze zerlegt. library(syuzhet) tom_v = get_sentences(txt$text[2]) # izberemo drugo besedilo: tom.txt tom_v = (tom_v[-1]) # tako lahko izloimo prvo vrstico (uredniko pripombo) head(tom_v[-1]) ## [1] &quot;Das eine oder das andere habe ich selbst erlebt , die anderen meine Schulkameraden .&quot; ## [2] &quot;Huck Finn ist nach dem Leben gezeichnet , nicht weniger Tom Sawyer , doch entspricht dieser nicht einer bestimmten Persönlichkeit , sondern wurde mit charakteristischen Zügen mehrerer meiner Altersgenossen ausgestattet und darf daher jenem gegenüber als einigermaßen kompliziertes psychologisches Problem gelten .&quot; ## [3] &quot;Ich muß hier bemerken , daß zur Zeit meiner Erzählung -- vor dreißig bis vierzig Jahren -- unter den Unmündigen und Unwissenden des Westens noch die seltsamsten , unwahrscheinlichsten Vorurteile und Aberglauben herrschten .&quot; ## [4] &quot;Obwohl dies Buch vor allem zur Unterhaltung der kleinen Welt geschrieben wurde , so darf ich doch wohl hoffen , daß es auch von Erwachsenen nicht ganz unbeachtet gelassen werde , habe ich doch darin versucht , ihnen auf angenehme Weise zu zeigen , was sie einst selbst waren , wie sie fühlten , dachten , sprachen , und welcher Art ihr Ehrgeiz und ihre Unternehmungen waren .&quot; ## [5] &quot;Erstes Kapitel .&quot; ## [6] &quot;, ,Tom !&quot; Die Funktion get_sentiment() weist den Wörtern in den Äußerungen einen positiven (+1), negativen (-1) oder neutralen (0) Stimmungswert (Sentiment, Emotionswert) zu. Anschließend werden diese Sentimentwerte summiert. tom_values &lt;- get_sentiment(tom_v, method = &quot;nrc&quot;, language = &quot;german&quot;) length(tom_values) ## [1] 5047 tom_values[100:110] ## [1] 0 -2 0 1 0 1 0 0 0 0 0 Wir binden die Äußerungen, die Emotionswerte und die Satzlänge in einen Datensatz ein. So lässt sich besser beurteilen, wie erfolgreich der Einsatz des Sentiment-Wörterbuchs in unserem Text war. Außerdem wollen wir auch einige Spalten umbenennen. sentiment1 = cbind(tom_v, tom_values, ntoken(tom_v)) %&gt;% as.data.frame() %&gt;% rename(words = V3, text = tom_v, values = tom_values) %&gt;% mutate(doc_id = &quot;tom.txt&quot;) %&gt;% rowid_to_column(var = &quot;sentence&quot;) # View(sentiment1) sentiment1 %&gt;% rmarkdown::paged_table() Wiederholen Sie die obigen Programschritte für den anderen Text, den wir mit dem ersten vergleichen möchten, und zwar mit Kafaks Prozess. prozess_v = get_sentences(txt$text[1]) # izberemo prvo besedilo: prozess.txt prozess_v = (prozess_v[-1]) # tako lahko izloimo prvo vrstico (uredniko pripombo) prozess_values &lt;- get_sentiment(prozess_v, method = &quot;nrc&quot;, language = &quot;german&quot;) sentiment2 = cbind(prozess_v, prozess_values, ntoken(prozess_v)) %&gt;% as.data.frame() %&gt;% rename(words = V3, text = prozess_v, values = prozess_values) %&gt;% mutate(doc_id = &quot;prozess.txt&quot;) %&gt;% rowid_to_column(var = &quot;sentence&quot;) # View(sentiment2) sentiment2 %&gt;% rmarkdown::paged_table() Durch Addition der Stimmungswerte lässt sich abschätzen, welcher Text einen größeren Anteil positiv bewerteter Wörter enthält. Zu diesem Zweck wollen wir die beiden Datensätze zusammenführen und außerdem das Format der Spalten Wörter und Werte anpassen. sentiment = rbind(sentiment1, sentiment2) %&gt;% as_tibble() %&gt;% mutate(values = parse_number(values), words = parse_number(words)) %&gt;% dplyr::select(doc_id, sentence, words, values, text) sentiment %&gt;% rmarkdown::paged_table() Das Ergebnis: Nach der obigen Methode ist der Durchschnitt der emotionalen Werte im Roman Prozess etwas höher (0,055) als im Roman Tom Sawyer (). Dieses Ergebnis war unerwartet, denn Tom Sawyer enthält viele heitere Geschichten. Aber möglicherweise dominieren bestimmte Kapitel mit negativ konnotierten Wörtern (z.B. Flüche, heutzutage als rassistisch angesehene Wörter wie z.B. Nigger u.a.) und vielleicht auch die unheimlichen oder gefährlichen Begegnungen mit bestimmten Personen das Gesamtergebnis beeinflusst haben. Natürlich kann es durchaus sein, dass unser Sentimentlexikon zu viele Wortformen nicht erfasst hat und daher das Ergebnis verfälscht ist. sentiment %&gt;% group_by(doc_id) %&gt;% summarise(polarnost = mean(values)) ## # A tibble: 2 x 2 ## doc_id polarnost ## &lt;chr&gt; &lt;dbl&gt; ## 1 prozess.txt 0.0550 ## 2 tom.txt -0.0109 Da wir die beiden Romane nicht so schnell (erneut) durchlesen können, sollten wir es noch auf andere Weise versuchen: Behandeln wir doch positive, neutrale und negative Werte getrennt und berücksichtigen wir dabei auch die Satzlänge! sentiment1 = sentiment %&gt;% group_by(doc_id) %&gt;% mutate(positive = ifelse(values &gt; 0, abs(values), 0), neutral = ifelse(values == 0, 1, 0), negative = ifelse(values &lt; 0, abs(values), 0)) sentiment1 %&gt;% summarise(pos = mean(100*positive/words), neut = mean(100*neutral/words), neg = mean(100*negative/words)) ## # A tibble: 2 x 4 ## doc_id pos neut neg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 prozess.txt 2.30 4.34 2.13 ## 2 tom.txt 2.63 6.77 2.81 Alle durchschnittlichen Sentimentwerte (egal, ob positiv, negativ oder neutral) sind in Tom Sawyer etwas höher als im Prozess. Am deutlichsten ist der Unterschied zwischen den als neutral bewerteten Wörtern. Versuchen wir mit einem t-Test herauszufinden, welche Unterschiede statistisch signifikant sind! Da wir die Varianzen der Sentimentwerte nicht kennen, wählen wir den Welch-t-Test, der bei ungleichen Varianzen zum Einsatz kommt (var.equal = FALSE). t.test(positive ~ doc_id, data = sentiment1, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: positive by doc_id ## t = 3.8797, df = 7776.3, p-value = 0.0001054 ## alternative hypothesis: true difference in means between group prozess.txt and group tom.txt is not equal to 0 ## 95 percent confidence interval: ## 0.03729845 0.11348152 ## sample estimates: ## mean in group prozess.txt mean in group tom.txt ## 0.4797886 0.4043987 t.test(negative ~ doc_id, data = sentiment1, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: negative by doc_id ## t = 0.40793, df = 8554.9, p-value = 0.6833 ## alternative hypothesis: true difference in means between group prozess.txt and group tom.txt is not equal to 0 ## 95 percent confidence interval: ## -0.03629760 0.05537492 ## sample estimates: ## mean in group prozess.txt mean in group tom.txt ## 0.4248349 0.4152962 t.test(neutral ~ doc_id, data = sentiment1, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: neutral by doc_id ## t = -5.1647, df = 8142.5, p-value = 2.465e-07 ## alternative hypothesis: true difference in means between group prozess.txt and group tom.txt is not equal to 0 ## 95 percent confidence interval: ## -0.07649156 -0.03440200 ## sample estimates: ## mean in group prozess.txt mean in group tom.txt ## 0.4803170 0.5357638 Zwei t-Tests weisen einen signifikanten Unterschied aus, und zwar bei positiven und neutralen Sentimentwerten. Bei negativen Sentimentwerten ist der Unterschied zwischen den beiden Romanen nicht signifikant. Dieses Ergebnis entspricht eher unseren oben beschriebenen Erwartungen: Tom Sawyer scheint im Durchschnitt positivere und neutralere Sentimentwerte aufzuweisen als der Prozess, bei negativen Sentimentwerten konnte dagegen kein signifikanter Unterschied zwischen den beiden Romanen festgestellt werden. Schauen wir uns nun einige der Sätze an, die negativ bewertet wurden: sentiment1 %&gt;% filter(negative &gt; 0) %&gt;% rmarkdown::paged_table() 10.19.2 Variante 2 Im zweiten Versuch wählen wir das nrc-Sentimentlexikon, und zwar mit Hilfe der Funktion get_nrc_sentiment(). tom_v = get_sentences(txt$text[2]) tom_nrc_values = get_nrc_sentiment(tom_v) tom_joy_items = which(tom_nrc_values$joy &gt; 0) head(tom_v[tom_joy_items], 4) ## [1] &quot;Obwohl dies Buch vor allem zur Unterhaltung der kleinen Welt geschrieben wurde , so darf ich doch wohl hoffen , daß es auch von Erwachsenen nicht ganz unbeachtet gelassen werde , habe ich doch darin versucht , ihnen auf angenehme Weise zu zeigen , was sie einst selbst waren , wie sie fühlten , dachten , sprachen , und welcher Art ihr Ehrgeiz und ihre Unternehmungen waren .&quot; ## [2] &quot;, Spare die Rute , und du verdirbst dein Kind &#39; , heißt es .&quot; ## [3] &quot;Er ist meiner toten Schwester Kind , ein armes Kind , und ich habe nicht das Herz , ihn irgendwie am Gängelband zu führen .&quot; ## [4] &quot;Es ist wohl hart für ihn , am Samstag stillzusitzen , wenn alle anderen Knaben Feiertag haben , aber er haßt Arbeit mehr als irgend sonst was , und ich will meine Pflicht an ihm tun , oder ich würde das Kind zu Grunde richten .&quot; nrc_sentiment = as.data.frame(cbind(tom_v, tom_nrc_values)) nrc_sentiment %&gt;% rmarkdown::paged_table() Die beiden folgenden linearen Regressionsanalysen zeigen, dass die positiven Sentimentwerte vor allem von den emotionalen Werten joy, trust, anticipation getragen werden, die negativen Sentimentwerte dagegen von den emotionalen Werten anger, disgust, fear, sadness. Der emotionale Wert surprise (Überraschung) scheint negativ mit dem positiven Sentiment korreliert zu sein. Überraschungen können natürlich nicht nur angenehm (positiv) sein, sondern auch unangenehm (negativ). library(jtools) mpos &lt;- lm(positive ~ joy + trust + anticipation + surprise, data = nrc_sentiment) summ(mpos) Observations 5048 Dependent variable positive Type OLS linear regression F(4,5043) 1302.59 R2 0.51 Adj. R2 0.51 Est. S.E. t val. p (Intercept) 0.01 0.00 5.51 0.00 joy 0.95 0.02 46.99 0.00 trust 0.19 0.01 20.47 0.00 anticipation 0.09 0.02 3.74 0.00 surprise -0.06 0.03 -2.32 0.02 Standard errors: OLS mneg &lt;- lm(negative ~ anger + disgust + fear + sadness + surprise, data = nrc_sentiment) summ(mneg) Observations 5048 Dependent variable negative Type OLS linear regression F(5,5042) 7713.63 R2 0.88 Adj. R2 0.88 Est. S.E. t val. p (Intercept) 0.03 0.00 9.49 0.00 anger 0.41 0.02 18.79 0.00 disgust 0.17 0.02 9.13 0.00 fear 0.93 0.01 113.82 0.00 sadness 0.10 0.01 9.35 0.00 surprise 0.01 0.03 0.29 0.77 Standard errors: OLS 10.19.3 Variante 3 Im dritten Versuch soll ein Wortliste mit emotionalen Werten zum Einsatz kommen, und zwar das BAWLR. # This lexicons contains values of Emotional valence and arousal ranging from 1 to 5. # But this extended version contains also binary Emo_Val values (1, -1). bawlr &lt;- read.delim2(&quot;data/BAWLR_utf8.txt&quot;, sep = &quot;\\t&quot;, dec = &quot;,&quot;, fileEncoding = &quot;UTF-8&quot;, header = T, stringsAsFactors = T) # # bawlr$EmoVal &lt;- as.character(bawlr$EmoVal) # # str(EmoVal) # bawlr$EmoVal &lt;- gsub(&#39;NEG&#39;, &#39;-1&#39;, bawlr$EmoVal) # bawlr$EmoVal &lt;- gsub(&#39;POS&#39;, &#39;1&#39;, bawlr$EmoVal) # bawlr$EmoVal &lt;- as.numeric(bawlr$EmoVal) bawlr %&gt;% rmarkdown::paged_table() Machen wir zwei Listen, und zwar eine Liste mit positiv bewerteten Wörtern und eine mit negativ bewerteten Wörtern! positive.words = bawlr %&gt;% mutate(WORD_LOWER = as.character(WORD_LOWER)) %&gt;% dplyr::select(EmoVal, WORD_LOWER) %&gt;% filter(EmoVal == &quot;POS&quot;) %&gt;% dplyr::select(WORD_LOWER) %&gt;% filter(str_detect(WORD_LOWER, &quot;[a-zA-Z]&quot;)) negative.words = bawlr %&gt;% mutate(WORD_LOWER = as.character(WORD_LOWER)) %&gt;% dplyr::select(EmoVal, WORD_LOWER) %&gt;% filter(EmoVal == &quot;NEG&quot;) %&gt;% dplyr::select(WORD_LOWER) %&gt;% filter(str_detect(WORD_LOWER, &quot;[a-zA-Z]&quot;)) Daraus erstellen wir ein quanteda Lexikon, und zwar mit der Funktion dictionary(): bawlr_dict = dictionary(list(positive = list(positive.words), negative = list(negative.words))) Wir verwenden eine in vorherigen Abschnitten gebildete Matrix (dfm) mit Lexikoneinheiten (Lemmas), da das bawlr_dict-Wörterbuch nur die Grundform der Lemmata enthält. matrika_lemmas = dfm(matrika_lem, tolower = TRUE) result = matrika_lemmas %&gt;% dfm_lookup(bawlr_dict) %&gt;% convert(to = &quot;data.frame&quot;) %&gt;% as_tibble result ## # A tibble: 2 x 3 ## doc_id positive negative ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 prozess.txt 7330 4000 ## 2 tom.txt 6386 3706 Wir können die Gesamtwortlänge hinzufügen, wenn das Ergebnis in Bezug auf die Länge der Texte normalisiert werden soll. result = result %&gt;% mutate(length=ntoken(matrika_lemmas)) result ## # A tibble: 2 x 4 ## doc_id positive negative length ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 prozess.txt 7330 4000 32058 ## 2 tom.txt 6386 3706 33520 Normalerweise wollen wir den Gesamtstimmungswert berechnen. Dafür gibt es mehrere Möglichkeiten: z.B. - die negativen Werte von den positiven zu subtrahieren und dann die Differenz durch die Summe der beiden Kategorien zu dividieren, - die negativen Werte von den positiven Werten zu subtrahieren und dann die Differenz durch die Länge der Texte zu dividieren. Wir können auch den Grad der Subjektivität berechnen, d.h. wie viele emotionale Werte insgesamt ausgedrückt werden: result = result %&gt;% mutate(sentiment1=(positive - negative) / (positive + negative)) result = result %&gt;% mutate(sentiment2=(positive - negative) / length) result = result %&gt;% mutate(subjektivnost=(positive + negative) / length) result %&gt;% rmarkdown::paged_table() Der Subjektivitätsgrad scheint demnach im Prozess etwas höher zu sein als im Tom Sawyer. 10.19.3.1 Farbliche Sentimentmarkierung Das Programm corpustools ermöglicht die farbliche Kodierung der Textwörter gemäß den Stimmungswerten, die im Sentimentlexikon verzeichnet sind. Der erste Schritt besteht darin, einen tcorpus anzulegen. library(corpustools) t = create_tcorpus(txt, doc_column=&quot;doc_id&quot;) Der zweite Schritt ist eine Wortrecherche im tcorpus: t$code_dictionary(bawlr_dict, column = &#39;bawlr&#39;) t$set(&#39;sentiment&#39;, 1, subset = bawlr %in% c(&#39;positive&#39;,&#39;neg_negative&#39;)) t$set(&#39;sentiment&#39;, -1, subset = bawlr %in% c(&#39;negative&#39;,&#39;neg_positive&#39;)) Dies ermöglicht die Anzeige der farbkodierten Texte im Viewer-Fenster: browse_texts(t, scale=&#39;sentiment&#39;) Der farbkodierte Text kann in einem Webbrowser angezeigt und als HTML-Datei gespeichert werden: browse_texts(t, scale=&#39;sentiment&#39;, filename = &quot;sentiment_prozess_tom.html&quot;, header = &quot;Sentiment in Kafkas Prozess und Twains Tom Sawyer&quot;) "],["suffixe-und-stemming.html", "Kapitel 11 Suffixe und Stemming 11.1 Packages 11.2 Dateien einlesen 11.3 Lemmatisierung 11.4 Stemming 11.5 Wortbildungsanalyse", " Kapitel 11 Suffixe und Stemming 11.1 Packages library(tidyverse) library(tidytext) library(SnowballC) library(readtext) library(rmarkdown) library(scales) library(udpipe) library(vecsets) 11.2 Dateien einlesen novels_txt = readtext(&quot;data/books/*.txt&quot;, docvarsfrom = &quot;filenames&quot;, encoding = &quot;UTF-8&quot;) %&gt;% rename(title = docvar1) novels_txt ## readtext object consisting of 2 documents and 1 docvar. ## # Description: df [2 x 3] ## doc_id text title ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 prozess.txt &quot;\\&quot;Der Prozes\\&quot;...&quot; prozess ## 2 tom.txt &quot;\\&quot;Tom Sawyer\\&quot;...&quot; tom Wir wandeln die Datei in eine Tabelle um. novels = as.data.frame(novels_txt) 11.3 Lemmatisierung Unser Ziel ist die Extrahierung von Wortbildungsmorphemen. Daher führen wir mit dem Program udpipe zuerst eine Analyse der Wortformen durch, die auch eine Lemmatisierung der Wortformen einschließt. Durch die Lemmatisierung können wir auf die Grundformen von Wörtern zurückgreifen und Flexionsmorpheme ausschließen. library(udpipe) destfile = &quot;german-gsd-ud-2.5-191206.udpipe&quot; if(!file.exists(destfile)){ model &lt;- udpipe_download_model(language = &quot;german&quot;) demodel &lt;- udpipe_load_model(model$file_model) } else { file_model = destfile demodel &lt;- udpipe_load_model(file_model) } Die Annotation der Wortformen kann mehrere Minuten dauern, falls es sich um längere Texte handelt. x = udpipe_annotate(demodel, novels_txt$text, trace = TRUE) ## 2022-03-20 21:19:49 Annotating text fragment 1/2 ## 2022-03-20 21:21:46 Annotating text fragment 2/2 x = as.data.frame(x) Wir filtern Interpunktionszeichen heraus, die für unser Ziel nicht relevant sind. Die Spalte upos enthält die dafür relevante Kategorie (PUNCT). In einem weiteren Schritt fügen wir mit mutate eine neue Spalte (word) hinzu, welche die tokens der Texte enthält, aber kleingeschrieben gewerden sollen, was mit tolower() erreicht werden kann. udpipe = x %&gt;% filter(upos != &quot;PUNCT&quot;) %&gt;% mutate(word = tolower(token)) Für die Entfernung von Ziffern und Symbolen aus der Tabelle verwenden wir einen regulären Ausdruck (regex). Außerdem sollen alle Lemmas mit Kleinbuchstaben beginnen. novels_words = udpipe %&gt;% filter(str_detect(lemma, &quot;[:alpha:]&quot;)) %&gt;% # keine Ziffern oder Interpunktionszeichen oder Symbole mutate(lemma = tolower(lemma)) Da wir vor allem an Wörtern interessiert sind, die aus mehr als einem Morphem bestehen, filtern wir u.a. Funktionswörter heraus, und zwar mit der Funktion anti_join() und einer Stoppwortliste aus dem Programm stopwords. Da die Stoppwortliste im Format einer Liste vorliegt, müssen wir sie in eine Tabelle umwandeln, und zwar mit der Funktion as_tibble(). Der Name der Tabellenspalte muss mit dem Namen der entsprechende Spalte in novels_words (also: word) übereinstimmen, damit wir die beiden Tabellen entsprechend vereinen können. Den Namen von Tabellenspalten verändern wir mit rename(). Außerdem wollen wir gleichzeitig auch einige Wörter herausfiltern, die nicht zu den Romantexten gehören: englische Wörter, die Namen der Autoren, eventuell noch nicht entfernte Ziffern, Interpunktionszeichen und Symbole. Zur Vereinigung der Wortformen in einen Vektor bzw. Tabellenspalte verwenden wir die concatenate-Funktion c(). In einem weiteren Schritt sollen die Kategorien der doc_id bessere Namen erhalten: die allgemeineren Namen doc1 und doc2 ersetzen wir mit den eindeutigeren Namen prozess und tom. stoplist_de = c(stopwords::stopwords(language = &quot;german&quot;), &quot;franz&quot;,&quot;kafka&quot;,&quot;mark&quot;,&quot;twain&quot;, &quot;by&quot;,&quot;aligned&quot;,&quot;Aligned&quot;,&quot;&quot;,&quot;bilingual-texts.com&quot;,&quot;fully&quot;,&quot;reviewed&quot;) %&gt;% as_tibble() %&gt;% rename(word = value) novels_words = novels_words %&gt;% mutate(doc_id = str_replace(doc_id, &quot;doc1&quot;, &quot;prozess&quot;), doc_id = str_replace(doc_id, &quot;doc2&quot;, &quot;tom&quot;)) %&gt;% anti_join(stoplist_de, novels_words, by = &quot;word&quot;) # möglichst keine Funktionswörter head(novels_words, 10) %&gt;% paged_table() tail(novels_words, 10) %&gt;% paged_table() 11.4 Stemming Beim Stemming werden die Stämme von Wortformen extrahiert. In flexionsarmen Sprachen (z.B. Englisch) sind die Ergebnisse gewöhnlich nützlicher als in morphologisch reichen Sprachen (z.B. Deutsch, Slowenisch). Hier verwenden wir die Stemming-Funktion wordStem() des Programms SnowballC, um potentielle Suffixe und Suffixoide komplexer Wörter zu extrahieren. Das Ziel ist eine morphologische Vergleichsanalyse, und zwar von Wortbildungsmorphemen in den Romaen. Mit mutate() und der wordStem()-Funktion fügen wir der Tabelle eine weitere Spalte hinzu, der wir den Namen stamm geben. novels_words = novels_words %&gt;% mutate(stamm = wordStem(lemma, language = &quot;de&quot;)) head(novels_words) %&gt;% paged_table() Nun stehen uns die Lemma- und Stammformen zur Verfügung. Der Unterschied zwischen den jeweiligen Formen sollte (meist) Wortbildungsmorpheme (Suffixe) ergeben. Um Unterschiede zwischen den in den Spalten lemma und stamm gespeicherten Wortformen zu bestimmen, wollen wir ein spezielles Programm verwenden: library(vectsets). Dann folgen einige Korrekturen mit str_remove(), str_remove_all(), str_replace() und str_replace_all(), damit in der Spalte diffs möglichst nur Wortbildungssuffixe vorkommen. Mit str_remove() beseitigen wir ein Zeichen einmal, mit str_remove_all() so oft, wie es in einer Tabellenspalte vorkommt. Mit str_replace() wandeln wir ein Zeichen einmal in ein anderes um, mit str_replace_all() so oft, wie es in einer Tabellenspalte vorkommt. library(vecsets) novels_full_words = novels_words %&gt;% mutate(diffs = as.character(mapply(vsetdiff, strsplit(lemma, split = &quot;&quot;), strsplit(stamm, split = &quot;&quot;)))) %&gt;% mutate(diffs = str_remove(diffs, &quot;c\\\\(&quot;), diffs = str_remove(diffs, &quot;\\\\)&quot;), diffs = str_remove_all(diffs, &#39;\\\\&quot;&#39;), diffs = str_remove_all(diffs, &quot;, &quot;), diffs = str_replace(diffs, &quot;character\\\\(0&quot;, &quot;&quot;), diffs = str_replace(diffs, &quot;ß&quot;, &quot;&quot;), diffs = str_replace_all(diffs, &quot;ä&quot;, &quot;&quot;), diffs = str_replace_all(diffs, &quot;ö&quot;, &quot;&quot;), diffs = str_replace_all(diffs, &quot;ü&quot;, &quot;&quot;)) Das Ergebnis ist nicht perfekt, aber für bestimmte Wortbildungssuffixe brauchbar. novels_full_words %&gt;% dplyr::select(doc_id, lemma, stamm, diffs) %&gt;% head(10) %&gt;% paged_table() Wir speichern die Tabelle für spätere Analysen. Möglich sind verschiedene Formate, z.B. rds-Dateien, die man mit R/Rstudio öffnen kann, und csv-Dateien, die man mit beliebigen Programmen öffnen kann. Aber wir speichern die Tabelle hier nur als Excel-Datei ab, weil die Tabellenzeilen nicht zu lang sind. # write_rds(novels_full_words, &quot;data/novels_full_words.rds&quot;) # write_csv(novels_full_words, &quot;data/novels_full_words.csv&quot;) writexl::write_xlsx(novels_full_words, &quot;data/novels_full_words.xlsx&quot;) 11.5 Wortbildungsanalyse Um am nächsten Tag nicht alle vorherigen Schritte noch einmal machen zu müssen, können wir an dieser Stelle die zuvor gespeicherte Tabelle öffnen. novels_full_words = readxl::read_xlsx(&quot;data/novels_full_words.xlsx&quot;) Wir beginnen unsere Wortbildungsanalyse mit dem Abzählen von verschiedenen Endungen, die von unserem Programm identifiziert wurden. Wir wählen mit dplyr::select() nur ein paar Tabellenspalten aus, damit wir die Übersicht behalten. Die Ergebnisse sollen nach dem Romantitel gruppiert werden, was man mit group_by() bewerkstelligt. Mit filter() werden leere Zeilen in der Tabellenspalte diffs herausgefiltert. Dann zählen wir die verschiedenen Kategorien in der Tabellenspalte, und zwar mit count(). Zuletzt verändern wir mit pivot_wider() das Tabellenformat, so dass die Romantitel als Spaltennamen erscheinen und die Endungen als Tabellenzeilen. Die Spalten prozess und tom enthalten nun die Häufigkeitswerte für die einzelnen Endungen. novels_full_words %&gt;% dplyr::select(doc_id, lemma, stamm, diffs) %&gt;% group_by(doc_id) %&gt;% filter(diffs != &quot;&quot;) %&gt;% count(diffs) %&gt;% pivot_wider(names_from = doc_id, values_from = n) %&gt;% paged_table() Wir erweitern unsere Häufigkeitstabelle mit Prozentzahlen und geben ihr einen Namen (novels_diffs). novels_diffs = novels_full_words %&gt;% dplyr::select(doc_id, lemma, stamm, diffs) %&gt;% group_by(doc_id) %&gt;% filter(diffs != &quot;&quot;) %&gt;% count(diffs) %&gt;% pivot_wider(names_from = doc_id, values_from = n) %&gt;% mutate(prozess_total = sum(prozess, na.rm = TRUE), tom_total = sum(tom, na.rm = TRUE)) %&gt;% mutate(prozess_pct = prozess/prozess_total, tom_pct = tom/tom_total,) %&gt;% dplyr::select(-prozess_total, -tom_total) head(novels_diffs) %&gt;% paged_table() Noch eine graphische Darstellung der Häufigkeitswerte, für die wir die Tabelle umformen, und zwar mit pivot_longer() und verkürzen (durch Filtervorgänge): wir wollen nur Endungen mit einer Häufigkeit von mehr als 0,5% (0.005) beibehalten. Mit fct_lump() kann man die Anzahl der Kategorien reduzieren (die Restkategorie heißt hier Other). Mit fct_reorder() sorgen wir dafür, dass die häufigeren Endungen im Diagramm oben erscheinen. Die Funktion facet_wrap() ermöglicht die getrennte Darstellung der Romane. library(scales) novels_diffs %&gt;% pivot_longer(cols = prozess_pct:tom_pct, names_to = &quot;title&quot;, values_to = &quot;prozent&quot;) %&gt;% filter(!is.na(prozent)) %&gt;% filter(prozent &gt; 0.005) %&gt;% mutate(diffs = fct_lump(diffs, 10)) %&gt;% mutate(diffs = fct_reorder(diffs, prozent)) %&gt;% ggplot(aes(prozent, diffs, fill = title)) + geom_col() + theme(legend.position = &quot;none&quot;) + scale_x_continuous(labels = percent) + labs(x = &quot;&quot;, y = &quot;Endungen&quot;) + facet_wrap(~ title, scales = &quot;free&quot;) 11.5.1 Suffix -lich Vergleichen wir mal die Häufigkeit der Endung -lich in den Romanen! Da dieses Wortbildungssuffix mit adjektivischen Stämmen verknüpft wird, filtern die entsprechende Wortklasse heraus. (lich_tab = novels_full_words %&gt;% group_by(doc_id) %&gt;% filter(upos == &quot;ADJ&quot;) %&gt;% count(diffs == &quot;lich&quot;) %&gt;% rename(lich = `diffs == &quot;lich&quot;`) %&gt;% filter(!is.na(lich)) %&gt;% pivot_wider(names_from = doc_id, values_from = n) ) ## # A tibble: 2 x 3 ## lich prozess tom ## &lt;lgl&gt; &lt;int&gt; &lt;int&gt; ## 1 FALSE 1403 1184 ## 2 TRUE 386 213 Ist der Unterschied zwischen den Romanen statistisch signifikant? Das überprüfen wir mit dem Chi-Quadrat-Test. Die erste Spalte enthält keine Zahlen, daher müssen wir sie beim Testen entfernen, und zwar mit [, -1]: alle Zeilen übernehmen, aber die este Tabellenspalte nicht. chisq.test(lich_tab[,-1]) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: lich_tab[, -1] ## X-squared = 20.172, df = 1, p-value = 7.077e-06 Der Chi-Quadrat-Test hat lediglich einen signifikanten Unterschied zwischen den beiden Stichproben prozess und tom bestätigt, sagt uns aber nicht, in welcher Stichprobe, das Suffix -lich verhältnismäßig häufiger vorkommt. Bei dieser Beurteilung helfen uns Prozentzahlen. lichtab2 = novels_full_words %&gt;% group_by(doc_id) %&gt;% filter(upos == &quot;ADJ&quot;) %&gt;% count(diffs == &quot;lich&quot;) %&gt;% rename(lich = `diffs == &quot;lich&quot;`) %&gt;% filter(!is.na(lich)) %&gt;% pivot_wider(names_from = doc_id, values_from = n) %&gt;% mutate(prozess_total = sum(prozess, na.rm = TRUE), tom_total = sum(tom, na.rm = TRUE)) %&gt;% mutate(prozess_pct = prozess/prozess_total, tom_pct = tom/tom_total,) %&gt;% dplyr::select(-prozess_total, -tom_total) lichtab2 %&gt;% paged_table() Etwa 21,5% der als Adjektiv identifizierten Lemmas im Roman prozess enden mit dem Suffix -lich, im Roman tom sawyer sind es etwa 15,2%. Der Unterschied ist auch in der graphischen Darstellung zu sehen. lichtab2 %&gt;% pivot_longer(prozess_pct:tom_pct, names_to = &quot;title&quot;, values_to = &quot;pct&quot;) %&gt;% ggplot(aes(title, pct, fill = lich)) + geom_col() Das Suffix -lich gehört zu den produktiven Wortbildungsmitteln im Deutschen. Warum sind im prozess mehr davon zu finden als im anderen Roman? Zur Klärung dieser Frage müssten wir zuerst mehr über die semantischen Eigenschaften und Verknüpfungsmöglichkeiten (oder -einschränkungen) mit verschiedenen Wortstämmen erfahren. novels_full_words %&gt;% dplyr::select(doc_id, lemma, word, upos, diffs) %&gt;% group_by(doc_id, lemma, word) %&gt;% filter(upos == &quot;ADJ&quot;) %&gt;% filter(diffs == &quot;lich&quot;) %&gt;% paged_table() 11.5.2 Mehrere ADJ Suffixe Vergleichen wir die Häufigkeit von mehreren adjektivischen Suffixen in unserem Romankorpus! novels_full_words %&gt;% dplyr::select(doc_id, lemma, word, upos, diffs) %&gt;% # Auswahl von hier relevanten Spalten group_by(doc_id, lemma, word) %&gt;% # Gruppierung nach diesen Merkmalen (Spalten) filter(upos == &quot;ADJ&quot;) %&gt;% # Auswahl der Wortklasse filter(diffs == &quot;lich&quot; | diffs == &quot;erlich&quot; | diffs == &quot;isch&quot; | diffs == &quot;ig&quot;) %&gt;% # Suffixauswahl paged_table() Von den insgesamt 1370 als (suffigiertes) Adjektiv identifizierten Wortformen (tokens) kommen 847 im prozess und 523 in tom sawyer vor. Der Anteil der Zustandsbeschreibungen mit Hilfe von suffigierten Adjektiven scheint im ersten Werk größer zu sein als im zuletzt genannten (was wir aber an dieser Stelle nicht mit einem Chi-Quadrat-Test überprüfen wollen). adj_tab = novels_full_words %&gt;% dplyr::select(doc_id, lemma, upos, diffs) %&gt;% group_by(doc_id) %&gt;% filter(upos == &quot;ADJ&quot;) %&gt;% filter(diffs == &quot;lich&quot; | diffs == &quot;erlich&quot; | diffs == &quot;isch&quot; | diffs == &quot;ig&quot;) %&gt;% count(diffs) %&gt;% pivot_wider(names_from = doc_id, values_from = n) %&gt;% mutate(prozess_total = sum(prozess, na.rm = TRUE), tom_total = sum(tom, na.rm = TRUE)) %&gt;% mutate(prozess_pct = prozess/prozess_total, tom_pct = tom/tom_total) adj_tab %&gt;% paged_table() library(scales) adj_tab %&gt;% pivot_longer(prozess_pct:tom_pct, names_to = &quot;title&quot;, values_to = &quot;pct&quot;) %&gt;% mutate(diffs = fct_reorder(diffs, pct)) %&gt;% ggplot(aes(pct, diffs, fill = title)) + geom_col(position = &quot;dodge&quot;) + scale_x_continuous(labels = percent) + theme(legend.position = &quot;top&quot;) 11.5.3 Nicht verwendete Tabelle novels_full_words %&gt;% dplyr::select(doc_id, lemma, stamm, diffs) %&gt;% group_by(doc_id) %&gt;% filter(diffs != &quot;&quot;) %&gt;% add_count(doc_id, name = &quot;total&quot;) %&gt;% add_count(diffs) %&gt;% mutate(pct = n/total) %&gt;% pivot_wider(names_from = doc_id, values_from = n, names_repair = &quot;unique&quot;) %&gt;% unnest(c(prozess, tom)) %&gt;% paged_table() "],["buchstaben-in-romanen.html", "Kapitel 12 Buchstaben in Romanen 12.1 Packages 12.2 Datensatz lesen 12.3 Buchstaben extrahieren 12.4 Buchstaben zählen 12.5 Vokale 12.6 Konsonanten 12.7 Vokal-Konsonant-Verhältnis 12.8 Anzahl der Silben 12.9 Mittlere Wortlänge 12.10 Testen von Mittelwertunterschieden 12.11 Quanteda-Funktionen 12.12 Konsonantenverbindungen 12.13 Datensatz-Variante", " Kapitel 12 Buchstaben in Romanen 12.1 Packages library(tidyverse) library(tidytext) library(scales) library(readtext) library(rmarkdown) # library(qdap) # syllable_count and syllable_sum # library(quanteda) # nsyllable(tokens(txt)) Im Wikipedia-Artikel zum Thema Buchstabenhäufigkeit gibt die folgende Tabelle Auskunft über die Häufigkeit von Buchstaben in einer Stichprobe von deutschen Texten. Die Umlaute werden in dieser Tabelle als jeweils zwei Monophthonge gezählt. library(readxl) buchstabenhaeufigkeit &lt;- read_xlsx(&quot;data/wikipedia_buchstabenhaeufigkeit_deutsch.xlsx&quot;) buchstabenhaeufigkeit %&gt;% rmarkdown::paged_table() Die ersten fünf Buchstaben haben einen Anteil von etwa der Hälfte, die häufigsten zehn Buchstaben decken etwa drei Viertel der relativen Buchstabenhäufigkeit in deutschen Texten ab. Eine weitere Tabelle zeigt die Häufigkeit der Buchstaben in Texten aus einem Briefkorpus (Briefe aus den Jahren 1996-2004). In diesem Fall sind auch die Frequenzen der Umlaute erhoben worden. Die zehn häufigsten Buchstaben im Briefkorpus decken sich zum großen Teil mit denen im vorher gezeigten. library(readxl) buchstabenhaeufigkeit_briefe &lt;- read_xlsx(&quot;data/wikipedia_buchstabenhaeufigkeit_briefkorpus.xlsx&quot;) buchstabenhaeufigkeit_briefe %&gt;% rmarkdown::paged_table() In einem anderen Wikipedia-Artikel mit dem Titel Frekvence rk werden die relativen Häufigkeiten der Buchstaben in slowenischen belletristischen Texten tabellarisch dargestellt und einigen anderen Sprachen gegenübergestellt. In dieser Tabelle fällt auf, dass die Graphme der Vollvokale a und o einen deutliche höheren Rang einnehmen als in den beiden Tabellen für deutsche Texte. Ähnlich wie in den Tabellen für die deutschen Texte ist wiederum, dass die Vokalgrapheme e und i zu den häufigsten gehören. Unter den Konsonantgraphemen sind auch hier n, s, r und t stark vertreten. library(readxl) buchstabenhaeufigkeit_slov &lt;- read_xlsx(&quot;data/wikipedia_frekvence_crk.xlsx&quot;) buchstabenhaeufigkeit_slov %&gt;% rmarkdown::paged_table() Wir stellen uns die Aufgabe, die Buchstabenhäufigkeit in von uns ausgewählten Texten literarischer Prosa tabellarisch zusammenzustellen und mit denen im Wikipedia-Artikel zu vergleichen. In den folgenden Abschnitten beschäftigen wir uns mit der Häufigkeit von Vokalgraphemen, Konsonantengraphemen, Konsonantenverbindungen und Silben in tabellarischer und graphischer Form. 12.2 Datensatz lesen Die readtext()-Funktion erlaubt Einlesen von mehreren Dateien auf einfache Art und Weise. Mit docvarsfrom erhalten wird eine neue Spalte in der Tabelle, die wir mit der Funktion rename() umbenennen. Mit encoding = UTF-8 teilen wir dem Programm mit, wie der Text kodiert ist (Code Page). novels_txt = readtext(&quot;data/books/*.txt&quot;, docvarsfrom = &quot;filenames&quot;, encoding = &quot;UTF-8&quot;) %&gt;% rename(title = docvar1) novels_txt ## readtext object consisting of 2 documents and 1 docvar. ## # Description: df [2 x 3] ## doc_id text title ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 prozess.txt &quot;\\&quot;Der Prozes\\&quot;...&quot; prozess ## 2 tom.txt &quot;\\&quot;Tom Sawyer\\&quot;...&quot; tom 12.3 Buchstaben extrahieren 12.3.1 aus Liste Der reguläre Ausdruck [a-zA-Z] extrahiert nur Buchstaben des englischen Alphabets, [:alpha:] extrahiert dagegen auch nicht-englische Buchstaben, z.B. deutsche oder slowenische Sonderzeichen. Zahlen und andere spezielle Zeichen (z.B. Interpunktion) werden auf diese Weise nicht extrahiert. Regex {1} (= default) extrahiert Einzelbuchstaben. Bei Verwendung von {2} werden jeweils zwei aufeinander folgende Buchstaben extrahiert. Die Funktion tolower() sorgt dafür, dass Großbuchstaben in Kleinbuchstaben umgewandelt werden. Falls zwischen großen und kleinen Buchstaben unterschieden werden soll, entfernen wir diese Funktion aus dem Programmkode. letters = tolower(novels_txt$text) %&gt;% str_extract_all(pattern = &quot;[:alpha:]{1}&quot;) letters[[1]][1:10] ## [1] &quot;d&quot; &quot;e&quot; &quot;r&quot; &quot;p&quot; &quot;r&quot; &quot;o&quot; &quot;z&quot; &quot;e&quot; &quot;s&quot; &quot;s&quot; letters[[2]][1:9] ## [1] &quot;t&quot; &quot;o&quot; &quot;m&quot; &quot;s&quot; &quot;a&quot; &quot;w&quot; &quot;y&quot; &quot;e&quot; &quot;r&quot; 12.3.2 aus Datensatz Tabellen und Graphiken erstellen ist leichter, wenn wir die Texte in Datensätze umwandeln, und zwar mit der Funktion as.data.frame(). novels = as.data.frame(novels_txt) Mit der Funktion unnest_tokens() können wir auch Buchstaben isolieren und anschließend auszählen. library(tidytext) novels_character &lt;- novels %&gt;% unnest_tokens(character, text, token = &quot;characters&quot;, to_lower = TRUE, drop = T) head(novels_character) ## doc_id title character ## 1 prozess.txt prozess d ## 2 prozess.txt prozess e ## 3 prozess.txt prozess r ## 4 prozess.txt prozess p ## 5 prozess.txt prozess r ## 6 prozess.txt prozess o 12.4 Buchstaben zählen Mit count() können wir die Häufigkeit einer Variable (hier: der Buchstaben) auszählen. novels_character %&gt;% count(character, sort = TRUE) %&gt;% head(3) ## character n ## 1 e 114769 ## 2 n 70151 ## 3 i 52767 Der tidytext-Tokenizer hat nicht nur Buchstaben, sondern auch Zahlen extrahiert. Da wir nur an der Häufigkeit von Buchstaben interessiert sind, filtern wir die Zahlen und andere Zeichen heraus. Dazu verwenden wir die Funktionen filter() und zusätzlich str_detect(), da wir für diese Aufgabe einen regulären Ausdruck nutzen wollen. novels_character %&gt;% filter(str_detect(character, &quot;[:alpha:]&quot;)) %&gt;% count(character, sort = T) %&gt;% head(3) ## character n ## 1 e 114769 ## 2 n 70151 ## 3 i 52767 Ein paar Zeichen, die nicht zum deutschen Alphabet gehören und mit dem vorherigen Programm-Schritt nicht herausfiltern konnten, werden im nächsten Schritt ebenfalls herausgefiltert. Wir speichern das Ergebnis als neue Tabelle mit dem Namen char_freq. Die zehn häufigsten Buchstaben in dieser Tabelle decken sich mit denen in den beiden eingangs gezeigten Tabellen aus dem Wikipedia-Artikel über Buchstabenhäufigkeit, insbesondere mit der, die auf einem Briefkorpus beruhte. char_freq = novels_character %&gt;% filter(str_detect(character, &quot;[:alpha:]&quot;)) %&gt;% filter(!str_detect(character, &quot;é|á&quot;)) %&gt;% count(character, sort = T) library(DT) char_freq %&gt;% DT::datatable(fillContainer = FALSE, filter = &quot;top&quot;, options = list(pageLength = 10)) Insgesamt haben wir 30 Buchstaben des deutschen Alphabets in den Romanen unterschieden. Aus wie vielen Buchstaben des deutschen Alphabets bestehen die Romane? Die Summe erhalten wir mit der Funktion summarise() - fast 700 Tausend. char_freq %&gt;% summarise(total = sum(n)) ## total ## 1 694556 Es ist nun wirklich Zeit, mal ein Bild zu malen! Dazu verwenden wir das Programm (library) ggplot2, das im Programmbündel tidyverse enthalten ist. Das Diagramm zeigt sehr deutlich, dass gewaltige Häufigkeitsunterschiede im deutschen Alphabet bestehen. char_freq %&gt;% mutate(character = fct_reorder(character, n)) %&gt;% # Sortieren nach Frequenz ggplot(aes(n, character, fill = character)) + geom_col() + theme(legend.position = &quot;none&quot;) Eine bessere Vorstellung von den Zahlenverhältnissen erhalten wir, wenn wir die mehrstelligen Zahlenwerte in Prozente umwandeln. library(scales) char_freq %&gt;% mutate(Prozent = n / sum(n)) %&gt;% # Umwandlung in Prozente ungroup() %&gt;% mutate(character = fct_reorder(character, Prozent)) %&gt;% # Sortieren nach Prozenten ggplot(aes(Prozent, character, fill = character)) + geom_col() + theme(legend.position = &quot;none&quot;) + scale_x_continuous(labels = percent_format( decimal.mark = &quot;,&quot;, accuracy = 1)) # Prozent-Format Getrennte tabellarische Darstellung für die Texte: novels_character %&gt;% group_by(doc_id) %&gt;% count(character, sort = TRUE) %&gt;% pivot_wider(names_from = doc_id, values_from = n) %&gt;% DT::datatable(fillContainer = FALSE, filter = &quot;top&quot;, options = list(pageLength = 10)) Getrennte graphische Darstellung für die Texte: library(scales) novels_character %&gt;% group_by(doc_id) %&gt;% count(character, sort = TRUE) %&gt;% mutate(Prozent = n / sum(n)) %&gt;% # Umwandlung in Prozente ungroup() %&gt;% mutate(character = fct_reorder(character, Prozent)) %&gt;% # Sortieren nach Prozenten filter(Prozent &gt; 0.0001) %&gt;% ggplot(aes(Prozent, character, fill = character)) + geom_col() + theme(legend.position = &quot;none&quot;) + facet_wrap(~ doc_id, scales = &quot;free&quot;) scale_x_continuous(labels = percent) # Prozent-Format ## &lt;ScaleContinuousPosition&gt; ## Range: ## Limits: 0 -- 1 12.5 Vokale Betrachten wir zunächst nur die Buchstaben, die Vokale symbolisieren! Zu diesem Zweck bilden wir eine Vokalliste. Zwischen den Vokalen setzen wir das oder-Zeichen ein: den logischen Operator |. vokale = &quot;a|e|i|o|u|ä|ö|ü|y&quot; Die Vokalliste vokale verwenden wir mit den Funktionen filter() und str_detect(). library(scales) char_freq %&gt;% filter(str_detect(character, vokale)) %&gt;% mutate(Prozent = n / sum(n)) %&gt;% # Umwandlung in Prozente ungroup() %&gt;% mutate(character = fct_reorder(character, Prozent)) %&gt;% # Sortieren nach Prozenten ggplot(aes(Prozent, character, fill = character)) + geom_col() + theme(legend.position = &quot;none&quot;) + labs(y = &quot;Vokale&quot;, x = &quot;Häufigkeit in Romanen&quot;) + scale_x_continuous(labels = percent_format(accuracy = 1), breaks = seq(0, 0.50, 0.05)) # Prozent-Format Am häufigsten kommt der Buchstabe e in den Romanen vor (fast 45%-iger Anteil unter den Vokalen!), am seltensten y, welches im Wesentlichen in Fremd- und Lehnwörtern auftritt. 12.6 Konsonanten Welche Buchstaben, die Konsonanten symbolisieren, kommen am häufigsten vor? Zum Filtern verwenden wir wiederum die Vokalliste, dieses Mal allerdings mit Negationszeichen !. library(scales) char_freq %&gt;% filter(!str_detect(character, vokale)) %&gt;% # Negationszeichen, daher Konsonanten beibehalten mutate(Prozent = n / sum(n)) %&gt;% # Umwandlung in Prozente ungroup() %&gt;% mutate(character = fct_reorder(character, Prozent)) %&gt;% # Sortieren nach Prozenten ggplot(aes(Prozent, character, fill = character)) + geom_col() + theme(legend.position = &quot;none&quot;) + labs(y = &quot;Konsonanten&quot;, x = &quot;Häufigkeit in Romanen&quot;) + scale_x_continuous(labels = percent_format(accuracy = 1), breaks = seq(0, 0.50, 0.02)) # Prozent-Format und Einheiten Der Buchstabe n kommt in den Romanen am häufigsten vor, gefolgt von den Buchstaben: r, s, t, h, d. Selten sind die Buchstaben: x, q, p, ß, v. 12.7 Vokal-Konsonant-Verhältnis Welches Zahlenverhältnis besteht zwischen den Vokalen und Konsonanten? 21 konsonantische Buchstaben und 9 vokalische Buchstaben. Pro Silbe sind in den deutschen Texten 1 Vokal und ungefähr 2 Konsonanten zu erwarten, also Silbenstrukturen wie z.B. KVK, KKV, VKK. bs_ratio = char_freq %&gt;% mutate(buchstabe = ifelse(str_detect(character, vokale), &quot;Vokal&quot;, &quot;Konsonant&quot;)) %&gt;% count(buchstabe) %&gt;% mutate(Prozent = n / sum(n)) bs_ratio ## buchstabe n Prozent ## 1 Konsonant 21 0.7 ## 2 Vokal 9 0.3 Der höhere Anteil der Konsonanten entspricht der größeren Konsonantenmenge. ggplot(bs_ratio, aes(x = &quot;&quot;, y = Prozent, fill = buchstabe)) + geom_col(color = &quot;black&quot;, size = 2) + coord_polar(theta = &quot;y&quot;, start = -0 * pi / 180) + # scale_fill_discrete(labels = c(&quot;Konsonanten&quot;, &quot;Vokale&quot;)) + scale_fill_manual(labels = c(&quot;Konsonanten&quot;, &quot;Vokale&quot;), values = c(&quot;#9E9AC8&quot;, &quot;#6A51A3&quot;)) + theme(legend.position = &quot;top&quot;, axis.text.y = element_blank(), axis.ticks = element_blank()) + labs(y = &quot;&quot;, x = &quot;Anteil % in Romanen&quot;) + scale_x_discrete(NULL, expand = c(0, 0)) + scale_y_continuous( labels = percent_format(accuracy = 1), breaks = seq(0, 1, 0.1)) # Prozent-Format und Einheiten Diese Zahlenwerte und -verhältnisse bilden einen möglichen Ausgangspunkt für intra- oder interlinguale Vergleiche. 12.8 Anzahl der Silben Wie viele Silben kommen in den Romanen schätzungsweise vor und wie viele Buchstaben pro Silbe? Die genaue Bestimmung der Silbenanzahl für eine bestimmte Sprache kann aufgrund zahlreicher Besonderheiten ziemlich kompliziert sein. Die Anzahl der Silben schätzen wir daher mit einer Funktion des Programms nsyllable (Alternatives Programm: qdap). Da wir die Silbenzählfunktion nur ein einziges Mal bemühen, rufen wir sie in der unten sichtbaren Form auf: nsyllable::nsyllable(buchstabenfolge). novels_words = novels %&gt;% unnest_tokens(word, text, token = &quot;words&quot;, to_lower = TRUE, drop = T) %&gt;% mutate(syllables = nsyllable::nsyllable(as.character(word), language = &quot;en&quot;)) %&gt;% mutate(letters = nchar(word)) novels_words %&gt;% head(100) %&gt;% DT::datatable(fillContainer = FALSE, filter = &quot;top&quot;, options = list(pageLength = 6)) Insgesamt (d.h. kumulativ gesehen) fast 139 Tausend Silben in den Romanen. Diese Zahl bietet einen möglichen Ausgangspunkt für Textvergleiche. novels_words %&gt;% count(syllables) %&gt;% summarise(Silben = sum(n)) ## Silben ## 1 138811 Die meisten Wortformen in den Romanen bestehen aus einer Silbe (fast 60%) oder zwei Silben (fast 30%). Das ist typisch für deutsche Texte. Kurze Funktionswörter (meist eine Silbe) kommen wesentlich häufiger vor als andere Wortklassen (Substantive, Verben, Adjektive, Adverbien). novels_words %&gt;% count(syllables) %&gt;% mutate(Prozent = n / sum(n)) %&gt;% ggplot(aes(syllables, Prozent, fill = factor(syllables))) + geom_col() + theme(legend.position = &quot;none&quot;) + labs(x = &quot;Silben&quot;) + scale_y_continuous( labels = percent, breaks = seq(0, 0.75, 0.1)) # Prozent-Format und Einheiten ## Warning: Removed 1 rows containing missing values (position_stack). Berücksichtig man lediglich distinktive Wortformen (also keine Wortwiederholungen), dann ergibt sich die folgende Verteilung, in der die Zweisilber (mehr als 30%) und Dreisilber (fast 30%) den größten Anteil haben. novels_words %&gt;% distinct(word, .keep_all = T) %&gt;% count(syllables) %&gt;% mutate(Prozent = n / sum(n)) %&gt;% ggplot(aes(syllables, Prozent, fill = factor(syllables))) + geom_col() + theme(legend.position = &quot;none&quot;) + labs(x = &quot;Silben&quot;) + scale_y_continuous( labels = percent, breaks = seq(0, 0.75, 0.1)) # Prozent-Format und Einheiten ## Warning: Removed 1 rows containing missing values (position_stack). 12.9 Mittlere Wortlänge Wir können die Wortlänge in geschriebenen Texten auf zumindest zwei grundlegende Arten messen: - die Anzahl der Silben pro Wort(form), - die Anzahl der Buchstaben pro Wort(form). Die durchschnittliche Anzahl der Silben und Buchstaben pro Wort (distinkte Wortformen !) in den Romanen ist in der folgenden Tabelle zu sehen: - neben den Mittelwerten (Avg_Silben, Avg_Buchstaben) - auch die Standardabweichungen vom entsprechenden Mittelwert (Stdev_Silben, Stdev_Buchstaben). Die Mittelwerte oder arithmetischen Mittel können mit der Programmfunktion mean() berechnet werden, die Standardabweichungen mit der Funktion sd(). Die Standardabweichungen sind notwendig zur Feststellung nicht-zufälliger Unterschiede zwischen den Stichproben (d.h. den Romanen). Bei der Berechnung der Mittelwerte und Standardabweichungen geben wir dem Programm auch die Instruktion, etwaige leere Datenzeilen (NA) herauszufiltern, und zwar mit Hilfe von na.rm = TRUE. Wird dies unterlassen, kann dies dazu führen, dass ein Mittelwert bzw. Standardabweichung nicht berechnet werden kann. In der folgenden Tabelle werden nur distinktive Wortformen (Types) berücksichtigt, d.h. als ob jede Wortform nur einmal pro Roman vorkommt. novels_words %&gt;% group_by(title) %&gt;% distinct(word, .keep_all = T) %&gt;% add_count(word) %&gt;% summarise(Avg_Silben = mean(syllables, na.rm = TRUE), Stdev_Silben = sd(syllables, na.rm = TRUE), Avg_Buchstaben = mean(letters, na.rm = TRUE), Stdev_Buchstaben = sd(letters, na.rm = TRUE)) %&gt;% DT::datatable(fillContainer = TRUE, filter = &quot;top&quot;, options = list(pageLength = 4)) Die durchschnittliche Anzahl der Silben und Buchstaben pro Wortform (Token), bei Berücksichtigung von Wortwiederholungen in den Romanen, ist in der folgenden Tabelle zu sehen. novels_words %&gt;% group_by(title) %&gt;% summarise(Avg_Silben = mean(syllables, na.rm = TRUE), Stdev_Silben = sd(syllables, na.rm = TRUE), Avg_Buchstaben = mean(letters, na.rm = TRUE), Stdev_Buchstaben = sd(letters, na.rm = TRUE)) %&gt;% DT::datatable(fillContainer = TRUE, filter = &quot;top&quot;, options = list(pageLength = 4)) 12.10 Testen von Mittelwertunterschieden 12.10.1 t-Test Sind die berechneten Unterschiede zwischen den Mittelwerten relevant bzw. nicht-zufällig? Um diese Frage zu klären, kann man einen statistischen Test bemühen. Da wir lediglich zwei Samples (zwei Romanen) vergleichen wollen, kann uns ein parametrischer Test wie z.B. der t-Test Klarheit verschaffen. Wir verwenden die Programmfunktion t.test(). Der t-Test bestätigt, dass Der Prozess im Durchschnitt etwas längere Wörter aufweist (2,59 Silben pro Wort gegenüber 2,44 Silben pro Wort in Tom Sawyer) - wenn Anzahl distinktiver Wortformen verwendet. syls = novels_words %&gt;% group_by(title) %&gt;% distinct(word, .keep_all = T) %&gt;% add_count(word) %&gt;% drop_na() %&gt;% dplyr::select(title, word, syllables) %&gt;% pivot_wider(names_from = title, values_from = syllables) t.test(syls$prozess, syls$tom, var.equal = TRUE) # significant ## ## Two Sample t-test ## ## data: syls$prozess and syls$tom ## t = 9.5813, df = 17554, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.1263172 0.1912927 ## sample estimates: ## mean of x mean of y ## 2.598953 2.440148 Wenn die Wiederholung von Wortformen berücksichtigt wird, bestätigt der t-Test ebenfalls einen signifikanten Unterschied zwischen den beiden Texten. Die durchschnittliche Anzahl der Wortsilben ist niedriger, da kürzere Wortformen (solche von Konjunktionen, Präpositionen, Artikeln und anderen Funktionswörtern) häufig vorkommen. Schnelle Form des t-Tests: t.test(syllables ~ title, data = novels_words, var.equal = TRUE) ## ## Two Sample t-test ## ## data: syllables by title ## t = 11.37, df = 137403, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means between group prozess and group tom is not equal to 0 ## 95 percent confidence interval: ## 0.04307118 0.06101337 ## sample estimates: ## mean in group prozess mean in group tom ## 1.613350 1.561307 Dasselbe Ergebnis, aber aufwendiger zu programmieren, um den Datensatz in die entsprechende Form zu bringen: prozess_syl &lt;- novels_words %&gt;% filter(title == &quot;prozess&quot;) %&gt;% dplyr::select(syllables) %&gt;% rename(prozess = syllables) tom_syl &lt;- novels_words %&gt;% filter(title == &quot;tom&quot;) %&gt;% dplyr::select(syllables) %&gt;% rename(prozess = syllables) t.test(prozess_syl, tom_syl, var.equal = T) # significant ## ## Two Sample t-test ## ## data: prozess_syl and tom_syl ## t = 11.37, df = 137403, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.04307118 0.06101337 ## sample estimates: ## mean of x mean of y ## 1.613350 1.561307 Der nächste t-Test bestätigt ebenfalls, dass Der Prozess im Durchschnitt längere Wörter aufweist (8,79 Buchstaben pro Wort gegenüber 8.36 Buchstaben pro Wort in Tom Sawyer.) Berücksichtigt wurden distinkte Wortformen (keine wiederholten Wortformen). lets = novels_words %&gt;% group_by(title) %&gt;% distinct(word, .keep_all = T) %&gt;% add_count(word) %&gt;% drop_na() %&gt;% dplyr::select(title, word, letters) %&gt;% pivot_wider(names_from = title, values_from = letters) t.test(lets$prozess, lets$tom, var.equal = T) # significant ## ## Two Sample t-test ## ## data: lets$prozess and lets$tom ## t = 9.0317, df = 17554, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.3310081 0.5145045 ## sample estimates: ## mean of x mean of y ## 8.785878 8.363122 Wenn die Wiederholung von Wortformen berücksichtigt wird, bestätigt der t-Test wiederum einen signifikanten Unterschied zwischen den beiden Texten. Die durchschnittliche Anzahl der Buchstaben pro Wort ist niedriger, da kürzere Wortformen (solche von Konjunktionen, Präpositionen, Artikeln und anderen Funktionswörtern) häufig vorkommen. Schnelle Form des t-Tests: t.test(letters ~ title, data = novels_words, var.equal = TRUE) ## ## Two Sample t-test ## ## data: letters by title ## t = 5.0147, df = 138809, p-value = 5.319e-07 ## alternative hypothesis: true difference in means between group prozess and group tom is not equal to 0 ## 95 percent confidence interval: ## 0.04480651 0.10230526 ## sample estimates: ## mean in group prozess mean in group tom ## 5.048860 4.975304 Dasselbe Ergebnis, aber aufwendiger zu programmieren, um den Datensatz in die entsprechende Form zu bringen: prozess_let &lt;- novels_words %&gt;% filter(title == &quot;prozess&quot;) %&gt;% dplyr::select(letters) %&gt;% rename(prozess = letters) tom_let &lt;- novels_words %&gt;% filter(title == &quot;tom&quot;) %&gt;% dplyr::select(letters) %&gt;% rename(prozess = letters) t.test(prozess_let, tom_let, var.equal = TRUE) # significant ## ## Two Sample t-test ## ## data: prozess_let and tom_let ## t = 5.0147, df = 138809, p-value = 5.319e-07 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.04480651 0.10230526 ## sample estimates: ## mean of x mean of y ## 5.048860 4.975304 12.10.2 Lineare Regression Hat man mehr als zwei Stichproben zu vergleichen, kann man eine lineare Regression durchführen, die auch das Testen von mehreren Einflussgrößen (Prädiktoren) erlaubt. Hier folgt eine Demonstration anhand des bereits gehabten Datensatzes mit zwei Stichproben (Romanen). Die Ordinate im Koordinatensystem (Intercept, also der y-Abschnitt mit x = 0) ist bei zwei Stichproben gleich dem Mittelwert der ersten Stichprobe (title = prozess), d.h. 1,613350. Der geschätzte Mittelwert (Estimate) der zweiten Stichprobe (title = tom) ist um den Wert 0,052042 niedriger, d.h. 1,613350 - 0,052042 = 1,561308 (Dezimalkommas statt Dezimalpunkte!). Der R-Quadrat-Wert (R-squared) ist allerdings sehr klein, d.h. dass der Prädiktor title (Roman) nur einen Bruchteil der festgestellten Mittelwertvarianz (Veränderungen der Mittelwerte) zu erklären vermag. Möglicherweise gibt es andere Prädiktoren, die die Mittelwertvarianz besser erklären. m &lt;- lm(syllables ~ title, data = novels_words) summary(m) ## ## Call: ## lm(formula = syllables ~ title, data = novels_words) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.6133 -0.6133 -0.5613 0.4387 6.3867 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.613350 0.003183 506.85 &lt;2e-16 *** ## titletom -0.052042 0.004577 -11.37 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8479 on 137403 degrees of freedom ## (1406 observations deleted due to missingness) ## Multiple R-squared: 0.00094, Adjusted R-squared: 0.0009327 ## F-statistic: 129.3 on 1 and 137403 DF, p-value: &lt; 2.2e-16 anova(m) ## Analysis of Variance Table ## ## Response: syllables ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## title 1 93 92.937 129.28 &lt; 2.2e-16 *** ## Residuals 137403 98778 0.719 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Graphische Darstellung: der Mittelwertunterschied ist gering (nur 0,05 Silben), aber aufgrund der großen Stichproben statistisch signifikant. Der Faktor title erklärt nur einen verschwinded kleinen Bruchteil der Mittelwertunterschiede. library(effects) ## Loading required package: carData ## lattice theme set by effectsTheme() ## See ?effectsTheme for details. allEffects(m) ## model: syllables ~ title ## ## title effect ## title ## prozess tom ## 1.613350 1.561307 plot(allEffects(m)) Ergebnisse in Tabellenform: summary(lm(syllables ~ title, data = novels_words)) %&gt;% broom::tidy() %&gt;% DT::datatable(fillContainer = TRUE, filter = &quot;top&quot;, options = list(pageLength = 4)) Boxplot mit Jitterplot anhand des vollen Datensatzes: der Mittelwert ist hier der Median median() (d.h. ein Wert, der genau in der Mitte jeder Stichprobe liegt), das arithmetische Mittel / der Durchschnitt wird hier mit einem roten Quadrat symbolisiert. Der Median liegt in beiden Stichproben beim Wert 1, also weit unter dem jeweiligen Durchschnittswert. Dies zeigt, dass die Wortlängen nicht normalverteilt sind. Der Jitterplot veranschaulicht, dass der Prozess über mehr Wortformen mit 6, 7 oder 8 Silben verfügt. novels_words %&gt;% group_by(title) %&gt;% ggplot(aes(title, syllables, fill = title, group = title)) + geom_jitter(width = 0.4, alpha = 0.5, color = &quot;gray70&quot;) + geom_boxplot(notch = FALSE, width = 0.8) + stat_summary(fun.y=&quot;mean&quot;, color = &quot;red&quot;, shape = 15)+ expand_limits(y = -1) + scale_y_continuous(breaks = seq(-1, 8, 1)) + theme(legend.position = &quot;none&quot;) + labs(y = &quot;Mittlere Wortlänge (in Silben)&quot;, x = &quot;Roman&quot;) ## Warning: `fun.y` is deprecated. Use `fun` instead. ## Warning: Removed 1406 rows containing non-finite values (stat_boxplot). ## Warning: Removed 1406 rows containing non-finite values (stat_summary). ## Warning: Removed 1406 rows containing missing values (geom_point). ## Warning: Removed 2 rows containing missing values (geom_segment). Boxplot anhand der zusammengefassten Daten (Durchschnitt, Standardabweichung): df = novels_words %&gt;% group_by(title) %&gt;% summarise(Avg_Silben = mean(syllables, na.rm = TRUE), Stdev_Silben = sd(syllables, na.rm = TRUE), Avg_Buchstaben = mean(letters, na.rm = TRUE), Stdev_Buchstaben = sd(letters, na.rm = TRUE)) df %&gt;% ggplot(aes(title, Avg_Silben, fill = title, group = title)) + geom_pointrange(aes(ymin = Avg_Silben - Stdev_Silben, ymax = Avg_Silben + Stdev_Silben), stat=&quot;identity&quot;) + theme(legend.position = &quot;none&quot;) + labs(y = &quot;Mittlere Wortlänge (in Silben)&quot;) df %&gt;% ggplot(aes(title, fill = title, group = title)) + geom_boxplot(aes(lower = Avg_Silben - Stdev_Silben, upper = Avg_Silben + Stdev_Silben, middle = Avg_Silben, ymin = Avg_Silben - 3*Stdev_Silben, ymax = Avg_Silben + 3*Stdev_Silben), stat=&quot;identity&quot;) + theme(legend.position = &quot;none&quot;) + labs(y = &quot;Mittlere Wortlänge (in Silben)&quot;) 12.11 Quanteda-Funktionen Eine alternative Berechnung der Anzahl der Buchstaben pro Wort mit quanteda (ohne t-Test). Die Durchschnittswerte, die uns quanteda liefert, sind etwas höher als die tidyverse-Werte. Aber auch hier ist der Mittelwert für den Prozess höher als für Tom Sawyer. library(quanteda) library(quanteda.textstats) corp = corpus(novels_txt) stats = textstat_summary(corp) stats %&gt;% paged_table() stats %&gt;% group_by(document) %&gt;% transmute(buchstaben = (chars-puncts)/tokens) %&gt;% paged_table() Die Durchschnittswerte unterscheiden sich in den Berechnungen (tidyverse vs. quanteda), was mit der verschiedenen Art der Tokenisierung und der Aussonderung von nicht relevanten Tokens und leeren Datenzeilen (NA) zu tun hat. 12.12 Konsonantenverbindungen Welche Konsonantenverbindungen (Buchstabenverbindungen) kommen häufiger vor in den Texten? Wir zerlegen die Texte im Korpus in kleinere Einheiten (mittels tokens()), aber dieses Mal in alphanumerische Zeichen (Buchstaben). Anschließend wenden wir char_ngrams()-Funktion an, mit der man Verknüpfungen von Zeichen feststellen kann. tok_ch = tokens(corp, what = &quot;character&quot;, remove_punct = TRUE, remove_symbols = T, remove_numbers = T, remove_url = T, remove_separators = T) ngrams_ch = char_ngrams(as.character(tok_ch), n = c(2,3,4), concatenator = &quot;&quot;) Wir wandeln die ngram-Liste in einen Datensatz um (mittels tibble()), was das Zählen mit einer tidyverse-Funktion ermöglicht. ngrams_char = ngrams_ch %&gt;% as_tibble() %&gt;% rename(cluster = value) ngrams_char %&gt;% count(cluster, sort = TRUE) %&gt;% head(10) %&gt;% DT::datatable(fillContainer = FALSE, filter = &quot;top&quot;, options = list(pageLength = 10)) to be continued  12.13 Datensatz-Variante novels_words_char &lt;- novels %&gt;% unnest_tokens(word, text, token = &quot;words&quot;, to_lower = TRUE, drop = T) %&gt;% mutate(Silben = nsyllable::nsyllable(as.character(word), language = &quot;en&quot;)) %&gt;% unnest_tokens(character, word, token = &quot;characters&quot;, to_lower = TRUE, drop = F) %&gt;% mutate(buchstabe = ifelse(str_detect(character, vokale), &quot;Vokal&quot;, &quot;Konsonant&quot;)) head(novels_words_char) %&gt;% head(10) %&gt;% rmarkdown::paged_table() "],["lesbarkeitsindices.html", "Kapitel 13 Lesbarkeitsindices 13.1 Zur theoretischen Begründung 13.2 Programme 13.3 Texte lesen 13.4 Flesch Reading Ease - deutsche Version 13.5 Romane (deutsche oder ins Deutsche übersetzte) 13.6 Zeitungen 13.7 Statistische Tests: News 13.8 Datensätze vereinen 13.9 Zeitungen im Vergleich 13.10 Alle Texte im Vergleich 13.11 Flesch-Kincade Grade Level (USA, GB) 13.12 Wiener Sachtextformel", " Kapitel 13 Lesbarkeitsindices (Reading ease indices) 13.1 Zur theoretischen Begründung Lesbarkeitsformeln sind in der Forschung weitgehend etabliert. Viele, die sich mit Lesbarkeitsformeln befassen, stellen sich dennoch die Frage, wieso man bei Berücksichtigung nur sehr weniger Kriterien Aufschluss über die Lesbarkeit von Texten erhalten kann. Man hat ja doch leicht den Eindruck, dass Wort- und Satzlänge keine besonders triftigen Kriterien sein sollten. Schaut man sich aber an, mit welchen anderen Kriterien diese beiden genannten  und andere  verknüpft sind, kann man erkennen, dass zwar nur zwei Texteigenschaften direkt gemessen werden, damit aber indirekt eine ganze Reihe andere ebenfalls berücksichtigt werden.  https://de.wikipedia.org/wiki/Lesbarkeitsindex 13.2 Programme library(tidyverse) library(tidytext) library(quanteda) library(readtext) library(nsyllable) 13.3 Texte lesen stringsAsFactor = F prozess = read_lines(&quot;novels/book05.txt&quot;) enfants = read_lines(&quot;novels/book12.txt&quot;) romane = readtext(&quot;novels/*.txt&quot;) coronavirus2020 &lt;- readtext(&quot;news/*.json&quot;, text_field = &quot;text&quot;, encoding = &quot;LATIN1&quot;) 13.4 Flesch Reading Ease - deutsche Version knitr::include_graphics( &quot;pictures/Lesbarkeitsindex Flesch Ease deutsch.png&quot;) flres_de = readxl::read_xlsx(&quot;data/Flesch-Reading-Ease-Score-deutsch.xlsx&quot;) flres_de %&gt;% mutate(`Verständlich für` = replace_na(`Verständlich für`,&quot; &quot;)) %&gt;% rmarkdown::paged_table() 13.5 Romane (deutsche oder ins Deutsche übersetzte) lesbarkeitsindex_romane = as_tibble(romane) %&gt;% mutate(sentences = nsentence(text), words = ntoken(text, remove_punct = TRUE), syllables = nsyllable(text), flesch_ease_de = 180 - (words/sentences) - 58.5*(syllables/words)) lesbarkeitsindex_romane %&gt;% select(doc_id, syllables, sentences, words, flesch_ease_de) %&gt;% arrange(flesch_ease_de) %&gt;% rmarkdown::paged_table() # write_excel_csv2(lesbarkeitsindex_romane, &quot;data/lesbarkeitsindex_romane.csv&quot;) lesbarkeitsindex_romane = lesbarkeitsindex_romane %&gt;% mutate(book_name = str_sub(text, 1, 12)) # extract string in position 1-12 # reading score chart ggplot(lesbarkeitsindex_romane, aes(x = syllables/words, y = flesch_ease_de, color = book_name, name = book_name, size = sentences)) + geom_count(alpha = 0.5) + # scale_x_log10() + geom_smooth(se = FALSE, color = &quot;red&quot;, method = &quot;lm&quot;, size = 0.5, linetype = &quot;dashed&quot;) + scale_y_continuous(breaks = c(60,65,70,75,80,85)) + scale_x_continuous(breaks = c(1.3,1.4,1.5,1.6)) + scale_size_area(max_size = 12, guide = FALSE) + theme_minimal(base_size = 14) + labs(color = &quot;Roman&quot;, x = &quot;# Silben pro Wort&quot;, y = &quot;Lesbarkeitsindex&quot;) book &lt;- book[str_sub(book, 1, 12) == review/text:] # extract string in position 1-12 book &lt;- str_sub(book, start = 14) # extract substring starting in position 14 library(plotly) p &lt;- lesbarkeitsindex_romane %&gt;% ggplot(aes(syllables/words, flesch_ease_de, color = book_name, name = book_name, size = sentences)) + geom_count() + geom_smooth(se = FALSE, color = &quot;red&quot;, method = &quot;lm&quot;, size = 0.5, linetype = &quot;dashed&quot;) + scale_y_continuous(breaks = c(60,65,70,75,80,85)) + scale_x_continuous(breaks = c(1.3,1.4,1.5,1.6)) + scale_size_area(max_size = 12, guide = FALSE) + theme_minimal(base_size = 14) + labs(color = &quot;Roman&quot;, x = &quot;# Silben pro Wort&quot;, y = &quot;Lesbarkeitsindex&quot;) ggplotly(p, height = 500) 13.6 Zeitungen lesbarkeitsindex_news = as_tibble(coronavirus2020) %&gt;% mutate(syllables = nsyllable(text), sentences = nsentence(text), words = ntoken(text, remove_punct = TRUE), flesch_ease_de = 180 - (words/sentences) - 58.5*(syllables/words)) lesbarkeitsindex_news %&gt;% select(doc_id, syllables, sentences, words, flesch_ease_de) %&gt;% arrange(flesch_ease_de) %&gt;% rmarkdown::paged_table() # write_excel_csv2(lesbarkeitsindex_news, # &quot;data/lesbarkeitsindex_news_coronavirus_2020.csv&quot;) # reading score chart ggplot(lesbarkeitsindex_news, aes(x = syllables/words, y = flesch_ease_de, color = lubridate::month(date), size = sentences)) + geom_count(alpha = 0.5) + geom_jitter() + # scale_x_log10() + geom_smooth(se = FALSE, color = &quot;red&quot;, method = &quot;lm&quot;, size = 0.5, linetype = &quot;dashed&quot;) + scale_y_continuous(breaks = c(20,30,40,50,60,70,80,90,100)) + scale_x_continuous(limits = c(1,2.5), breaks = c(1.0,1.25,1.5,1.75,2.0,2.25,2.5)) + scale_size_area(max_size = 12, guide = &quot;none&quot;) + theme_minimal(base_size = 14) + labs(color = &quot;month 2020&quot;, x = &quot;# Syllables per word&quot;, y = &quot;Reading level&quot;) lesbarkeitsindex_news = lesbarkeitsindex_news %&gt;% mutate(news_name = str_sub(doc_id, 1, 7)) # reading score chart ggplot(lesbarkeitsindex_news, aes(x = syllables/words, y = flesch_ease_de, color = news_name, name = news_name, size = sentences)) + geom_count(alpha = 0.5) + geom_jitter() + scale_y_continuous(breaks = c(20,30,40,50,60,70,80,90,100)) + scale_x_continuous(limits = c(1,2.5), breaks = c(1,1.5,2.0,2.5)) + facet_wrap(~ news_name) + geom_smooth(se = FALSE, color = 1, method = &quot;lm&quot;, size = 0.5, linetype = &quot;dashed&quot;) + scale_size_area(max_size = 12, guide = &quot;none&quot;) + theme_light(base_size = 14) + labs(color = &quot;Zeitung&quot;, x = &quot;# Silben pro Wort&quot;, y = &quot;Lesbarkeitsindex&quot;) library(plotly) q = lesbarkeitsindex_news %&gt;% ggplot(aes(syllables/words, flesch_ease_de, color = lubridate::month(date), name = news_name, size = sentences)) + geom_point() + facet_wrap(~ news_name) + geom_jitter() + geom_smooth(se = FALSE, color = &quot;black&quot;, method = &quot;lm&quot;, size = 0.5, linetype = &quot;dashed&quot;) + scale_y_continuous(breaks = c(20,30,40,50,60,70,80,90,100)) + scale_x_continuous(limits = c(1,2.5), breaks = c(1,1.5,2.0,2.5)) + scale_size_area(max_size = 12, guide = &quot;none&quot;) + theme_light(base_size = 14) + labs(color = &quot;month 2020&quot;, x = &quot;# Silben pro Wort&quot;, y = &quot;Lesbarkeitsindex&quot;) ggplotly(q, height = 500) 13.7 Statistische Tests: News Können wir statistisch signifikante Unterschiede zwischen den Lesbarkeitsindices nachweisen? Mit dem t-Test können wir immer nur zwei Stichproben miteinander vergleichn. lesbarkeitsindex_news %&gt;% filter(news_name != &quot;welt_co&quot; &amp; news_name != &quot;stern_g&quot; &amp; news_name != &quot;focus_c&quot;) %&gt;% t.test(flesch_ease_de ~ news_name, data = ., paired = F, var.equal = T) ## ## Two Sample t-test ## ## data: flesch_ease_de by news_name ## t = -10.532, df = 4924, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means between group faz_cor and group spiegel is not equal to 0 ## 95 percent confidence interval: ## -3.313378 -2.273477 ## sample estimates: ## mean in group faz_cor mean in group spiegel ## 50.62753 53.42096 lesbarkeitsindex_news %&gt;% filter(news_name != &quot;spiegel&quot; &amp; news_name != &quot;stern_g&quot; &amp; news_name != &quot;focus_c&quot;) %&gt;% t.test(flesch_ease_de ~ news_name, data = ., paired = F, var.equal = T) ## ## Two Sample t-test ## ## data: flesch_ease_de by news_name ## t = -13.358, df = 4215, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means between group faz_cor and group welt_co is not equal to 0 ## 95 percent confidence interval: ## -4.819682 -3.586028 ## sample estimates: ## mean in group faz_cor mean in group welt_co ## 50.62753 54.83038 lesbarkeitsindex_news %&gt;% filter(news_name != &quot;faz_cor&quot; &amp; news_name != &quot;stern_g&quot; &amp; news_name != &quot;focus_c&quot;) %&gt;% t.test(flesch_ease_de ~ news_name, data = ., paired = F, var.equal = T) ## ## Two Sample t-test ## ## data: flesch_ease_de by news_name ## t = -6.1477, df = 5941, p-value = 8.374e-10 ## alternative hypothesis: true difference in means between group spiegel and group welt_co is not equal to 0 ## 95 percent confidence interval: ## -1.8588644 -0.9599905 ## sample estimates: ## mean in group spiegel mean in group welt_co ## 53.42096 54.83038 Eine lineare Regression ermöglicht den Vergleich von mehreren Stichproben und mehreren Variablen (Faktoren, Prädiktoren). lesbarkeitsindex_news %&gt;% lm(flesch_ease_de ~ news_name, data = .) %&gt;% summary() ## ## Call: ## lm(formula = flesch_ease_de ~ news_name, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -68.487 -5.290 -0.583 5.397 53.357 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.6275 0.2255 224.481 &lt;2e-16 *** ## news_namefocus_c -0.6147 2.3402 -0.263 0.793 ## news_namespiegel 2.7934 0.2745 10.178 &lt;2e-16 *** ## news_namestern_g 11.2601 0.4856 23.187 &lt;2e-16 *** ## news_namewelt_co 4.2029 0.2863 14.680 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.021 on 7993 degrees of freedom ## Multiple R-squared: 0.06919, Adjusted R-squared: 0.06872 ## F-statistic: 148.5 on 4 and 7993 DF, p-value: &lt; 2.2e-16 13.8 Datensätze vereinen lesbarkeitsindex = lesbarkeitsindex_romane %&gt;% mutate(book_name = &quot;romane&quot;) %&gt;% rename(news_name = book_name) %&gt;% bind_rows(lesbarkeitsindex_news[,c(1:2,7:11)]) (les_mean = mean(lesbarkeitsindex$flesch_ease_de)) ## [1] 53.81142 (les_sd = sd(lesbarkeitsindex$flesch_ease_de)) ## [1] 9.375399 (syl_mean = mean(lesbarkeitsindex$syllables/lesbarkeitsindex$words)) ## [1] 1.875349 library(plotly) p &lt;- lesbarkeitsindex %&gt;% ggplot(aes(syllables/words, flesch_ease_de, color = news_name, name = news_name, size = sentences)) + geom_count() + geom_smooth(se = FALSE, color = &quot;red&quot;, method = &quot;lm&quot;, size = 0.5, linetype = &quot;dashed&quot;) + geom_vline(xintercept = syl_mean, lty = 2) + geom_hline(yintercept = les_mean, lty = 2) + geom_hline(yintercept = les_mean + 2*les_sd, lty = 3) + geom_hline(yintercept = les_mean - 2*les_sd, lty = 3) + scale_y_continuous( breaks = c(0,10,20,30,40,50,60,70,80,90,100,110)) + scale_x_continuous( breaks = c(1.0,1.2,1.4,1.6,1.8,2.0,2.2,2.4,2.6)) + scale_size_area(max_size = 12, guide = &quot;none&quot;) + theme_minimal(base_size = 14) + labs(color = &quot;Medium&quot;, x = &quot;# Silben pro Wort&quot;, y = &quot;Lesbarkeitsindex&quot;) ggplotly(p, height = 400) 13.9 Zeitungen im Vergleich # global options # options(contrasts=c(&#39;contr.sum&#39;,&#39;contr.poly&#39;)) lesbarkeitsindex$news_name = as.factor(lesbarkeitsindex$news_name) # set contrast to &quot;contr.sum&quot; contrasts(lesbarkeitsindex$news_name) &lt;- &quot;contr.sum&quot; contrasts(lesbarkeitsindex$news_name) # take a look ## [,1] [,2] [,3] [,4] [,5] ## faz_cor 1 0 0 0 0 ## focus_c 0 1 0 0 0 ## romane 0 0 1 0 0 ## spiegel 0 0 0 1 0 ## stern_g 0 0 0 0 1 ## welt_co -1 -1 -1 -1 -1 lesbarkeitsindex %&gt;% lm(flesch_ease_de ~ news_name, data = .) %&gt;% summary() ## ## Call: ## lm(formula = flesch_ease_de ~ news_name, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -68.487 -5.290 -0.581 5.397 53.357 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 57.2894 0.5892 97.241 &lt; 2e-16 *** ## news_name1 -6.6618 0.6173 -10.793 &lt; 2e-16 *** ## news_name2 -7.2765 1.9908 -3.655 0.000259 *** ## news_name3 15.6675 2.2062 7.102 1.34e-12 *** ## news_name4 -3.8684 0.6028 -6.417 1.47e-10 *** ## news_name5 4.5982 0.6858 6.705 2.16e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.02 on 8004 degrees of freedom ## Multiple R-squared: 0.07494, Adjusted R-squared: 0.07436 ## F-statistic: 129.7 on 5 and 8004 DF, p-value: &lt; 2.2e-16 lesbarkeitsindex %&gt;% lm(flesch_ease_de ~ news_name, data = .) %&gt;% tidy() %&gt;% # filter(term != &quot;(Intercept)&quot;) %&gt;% mutate(term = str_replace(term, &quot;news_name&quot;, &quot;&quot;)) %&gt;% arrange(estimate) ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 -7.28 1.99 -3.66 2.59e- 4 ## 2 1 -6.66 0.617 -10.8 5.71e-27 ## 3 4 -3.87 0.603 -6.42 1.47e-10 ## 4 5 4.60 0.686 6.70 2.16e-11 ## 5 3 15.7 2.21 7.10 1.34e-12 ## 6 (Intercept) 57.3 0.589 97.2 0 # global default option # options(contrasts=c(&#39;contr.treatment&#39;,&#39;contr.poly&#39;)) # set contrast back to default (&quot;contr.treatment&quot;) contrasts(lesbarkeitsindex$news_name) &lt;- NULL contrasts(lesbarkeitsindex$news_name) # take a look ## focus_c romane spiegel stern_g welt_co ## faz_cor 0 0 0 0 0 ## focus_c 1 0 0 0 0 ## romane 0 1 0 0 0 ## spiegel 0 0 1 0 0 ## stern_g 0 0 0 1 0 ## welt_co 0 0 0 0 1 lesbarkeitsindex %&gt;% lm(flesch_ease_de ~ news_name, data = .) %&gt;% summary() ## ## Call: ## lm(formula = flesch_ease_de ~ news_name, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -68.487 -5.290 -0.581 5.397 53.357 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.6275 0.2255 224.511 &lt;2e-16 *** ## news_namefocus_c -0.6147 2.3399 -0.263 0.793 ## news_nameromane 22.3293 2.6136 8.543 &lt;2e-16 *** ## news_namespiegel 2.7934 0.2744 10.179 &lt;2e-16 *** ## news_namestern_g 11.2601 0.4856 23.190 &lt;2e-16 *** ## news_namewelt_co 4.2029 0.2863 14.682 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.02 on 8004 degrees of freedom ## Multiple R-squared: 0.07494, Adjusted R-squared: 0.07436 ## F-statistic: 129.7 on 5 and 8004 DF, p-value: &lt; 2.2e-16 lesbarkeitsindex %&gt;% lm(flesch_ease_de ~ news_name, data = .) %&gt;% tidy() %&gt;% # filter(term != &quot;(Intercept)&quot;) %&gt;% mutate(term = str_replace(term, &quot;news_name&quot;, &quot;&quot;), term = str_replace(term, &quot;(Intercept)&quot;, &quot;faz_cor&quot;)) %&gt;% arrange(estimate) ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 focus_c -0.615 2.34 -0.263 7.93e- 1 ## 2 spiegel 2.79 0.274 10.2 3.46e- 24 ## 3 welt_co 4.20 0.286 14.7 3.53e- 48 ## 4 stern_g 11.3 0.486 23.2 3.37e-115 ## 5 romane 22.3 2.61 8.54 1.54e- 17 ## 6 (faz_cor) 50.6 0.226 225. 0 library(effects) lesbarkeitsindex %&gt;% lm(flesch_ease_de ~ news_name, data = .) -&gt; mymodel plot(allEffects(mymodel)) 13.10 Alle Texte im Vergleich Die ausgewählten zwölf Romane haben einen deutlich höheren Lesbarkeitsindex als die Zeitungstexte. Woran liegt das: an der Wortlänge, der Silbenanzahl oder an der Satzlänge? lesbarkeitsindex %&gt;% mutate(wordnum_utterance = (words/sentences), sylnum_utterance = (syllables/sentences)) %&gt;% lm(flesch_ease_de ~ news_name*wordnum_utterance + news_name*sylnum_utterance, data = .) %&gt;% tidy() %&gt;% mutate(term = str_replace(term, &quot;news_name&quot;, &quot;&quot;)) %&gt;% arrange(-estimate) ## # A tibble: 18 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 68.7 0.210 327. 0 ## 2 romane 23.7 2.45 9.67 5.18e-22 ## 3 stern_g 10.2 0.539 18.9 4.70e-78 ## 4 wordnum_utterance 5.49 0.0420 131. 0 ## 5 welt_co 4.29 0.250 17.1 1.46e-64 ## 6 spiegel 1.99 0.284 7.02 2.48e-12 ## 7 romane:sylnum_utterance 0.558 0.409 1.37 1.72e- 1 ## 8 focus_c:wordnum_utterance 0.389 0.918 0.424 6.72e- 1 ## 9 spiegel:wordnum_utterance 0.211 0.0561 3.76 1.71e- 4 ## 10 welt_co:sylnum_utterance -0.0184 0.0253 -0.728 4.67e- 1 ## 11 focus_c:sylnum_utterance -0.131 0.395 -0.333 7.39e- 1 ## 12 spiegel:sylnum_utterance -0.182 0.0261 -6.98 3.20e-12 ## 13 stern_g:sylnum_utterance -0.219 0.0563 -3.90 9.88e- 5 ## 14 stern_g:wordnum_utterance -0.220 0.114 -1.92 5.44e- 2 ## 15 welt_co:wordnum_utterance -0.227 0.0531 -4.28 1.88e- 5 ## 16 romane:wordnum_utterance -2.22 0.668 -3.33 8.75e- 4 ## 17 focus_c -2.45 4.03 -0.607 5.44e- 1 ## 18 sylnum_utterance -3.39 0.0195 -174. 0 Nur statistisch signifikante Interaktionen: lesbarkeitsindex %&gt;% mutate(wordnum_utterance = (words/sentences), sylnum_utterance = (syllables/sentences)) %&gt;% lm(flesch_ease_de ~ news_name*wordnum_utterance + news_name*sylnum_utterance, data = .) %&gt;% tidy() %&gt;% mutate(term = str_replace(term, &quot;news_name&quot;, &quot;&quot;)) %&gt;% mutate(pval = case_when( p.value &lt; 0.05 ~ &quot;significant&quot;, TRUE ~ &quot;---&quot;)) %&gt;% mutate(pval = ifelse(term == &quot;(Intercept)&quot;, &quot;xxx&quot;, pval)) %&gt;% filter(pval == &quot;significant&quot; | pval == &quot;xxx&quot;) %&gt;% filter(str_detect(term, &quot;:&quot;)) %&gt;% arrange(-estimate) ## # A tibble: 5 x 6 ## term estimate std.error statistic p.value pval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 spiegel:wordnum_utterance 0.211 0.0561 3.76 1.71e- 4 significant ## 2 spiegel:sylnum_utterance -0.182 0.0261 -6.98 3.20e-12 significant ## 3 stern_g:sylnum_utterance -0.219 0.0563 -3.90 9.88e- 5 significant ## 4 welt_co:wordnum_utterance -0.227 0.0531 -4.28 1.88e- 5 significant ## 5 romane:wordnum_utterance -2.22 0.668 -3.33 8.75e- 4 significant In den Romanen sind die Wörter im Durchschnitt kürzer als in den Zeitungstexten (Spiegel ). Tidy-Version: lesbarkeitsindex %&gt;% group_by(news_name) %&gt;% summarise(syl_per_wrd = mean(syllables/words), sd_syl_per_wrd = sd(syllables/words)) ## # A tibble: 6 x 3 ## news_name syl_per_wrd sd_syl_per_wrd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 faz_cor 1.91 0.143 ## 2 focus_c 1.93 0.0888 ## 3 romane 1.47 0.0719 ## 4 spiegel 1.89 0.116 ## 5 stern_g 1.75 0.109 ## 6 welt_co 1.86 0.135 Die Anzahl der Silben pro Äußerung in den Romanen ist vergleichbar mit der in Zeitungstexten (z.B. Spiegel). lesbarkeitsindex %&gt;% group_by(news_name) %&gt;% summarise(syl_per_utt = mean(syllables/sentences), sd_syl_per_utt = sd(syllables/sentences)) ## # A tibble: 6 x 3 ## news_name syl_per_utt sd_syl_per_utt ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 faz_cor 33.8 8.61 ## 2 focus_c 33.3 5.88 ## 3 romane 31.0 8.88 ## 4 spiegel 30.6 5.91 ## 5 stern_g 27.9 5.89 ## 6 welt_co 30.4 8.74 Die Romane haben durchschnittlich mehr Wörter pro Äußerung. lesbarkeitsindex %&gt;% group_by(news_name) %&gt;% summarise(wrds_per_utt = mean(words/sentences), sd_wrds_per_utt = sd(words/sentences)) ## # A tibble: 6 x 3 ## news_name wrds_per_utt sd_wrds_per_utt ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 faz_cor 17.6 3.99 ## 2 focus_c 17.2 2.53 ## 3 romane 20.9 5.43 ## 4 spiegel 16.2 2.75 ## 5 stern_g 15.9 2.93 ## 6 welt_co 16.2 4.37 lesbarkeit = lesbarkeitsindex %&gt;% mutate(functiontype = case_when( news_name == &quot;romane&quot; ~ &quot;literatur&quot;, news_name != &quot;romane&quot; ~ &quot;zeitung&quot;, TRUE ~ &quot;other&quot; )) %&gt;% mutate(syl_per_wrd = (syllables/words), wordnum_utterance = (words/sentences), sylnum_utterance = (syllables/sentences)) lesbarkeit %&gt;% slice_sample(n = 30) %&gt;% select(-text) %&gt;% rmarkdown::paged_table() Die Romane haben kürzere Wörter als die Zeitungstexte (d.h. weniger Silben). # set contrast back to default (&quot;contr.treatment&quot;) contrasts(lesbarkeitsindex$news_name) &lt;- NULL levels(lesbarkeitsindex$news_name) ## [1] &quot;faz_cor&quot; &quot;focus_c&quot; &quot;romane&quot; &quot;spiegel&quot; &quot;stern_g&quot; &quot;welt_co&quot; contrasts(lesbarkeitsindex$news_name) &lt;- contr.treatment(6, base = 3) # base = romane ! contrasts(lesbarkeitsindex$news_name) # take a look ## 1 2 4 5 6 ## faz_cor 1 0 0 0 0 ## focus_c 0 1 0 0 0 ## romane 0 0 0 0 0 ## spiegel 0 0 1 0 0 ## stern_g 0 0 0 1 0 ## welt_co 0 0 0 0 1 lesbarkeit %&gt;% lm(syl_per_wrd ~ news_name, data = .) %&gt;% summary() ## ## Call: ## lm(formula = syl_per_wrd ~ news_name, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.73696 -0.07736 0.00306 0.07656 0.56661 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.910836 0.003187 599.592 &lt; 2e-16 *** ## news_namefocus_c 0.016906 0.033068 0.511 0.609 ## news_nameromane -0.438972 0.036937 -11.884 &lt; 2e-16 *** ## news_namespiegel -0.023832 0.003878 -6.145 8.39e-10 *** ## news_namestern_g -0.163793 0.006862 -23.869 &lt; 2e-16 *** ## news_namewelt_co -0.048874 0.004045 -12.081 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1275 on 8004 degrees of freedom ## Multiple R-squared: 0.08559, Adjusted R-squared: 0.08502 ## F-statistic: 149.8 on 5 and 8004 DF, p-value: &lt; 2.2e-16 Die Romane haben durchschnittlich mehr Wörter pro Äußerung. contrasts(lesbarkeitsindex$news_name) &lt;- NULL levels(lesbarkeitsindex$news_name) ## [1] &quot;faz_cor&quot; &quot;focus_c&quot; &quot;romane&quot; &quot;spiegel&quot; &quot;stern_g&quot; &quot;welt_co&quot; contrasts(lesbarkeitsindex$news_name) &lt;- contr.treatment(6, base = 3) # base = romane ! contrasts(lesbarkeitsindex$news_name) # take a look ## 1 2 4 5 6 ## faz_cor 1 0 0 0 0 ## focus_c 0 1 0 0 0 ## romane 0 0 0 0 0 ## spiegel 0 0 1 0 0 ## stern_g 0 0 0 1 0 ## welt_co 0 0 0 0 1 lesbarkeit %&gt;% lm(wordnum_utterance ~ news_name, data = .) %&gt;% summary() ## ## Call: ## lm(formula = wordnum_utterance ~ news_name, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.245 -2.098 -0.345 1.811 66.755 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17.58859 0.09039 194.575 &lt; 2e-16 *** ## news_namefocus_c -0.37430 0.93796 -0.399 0.68986 ## news_nameromane 3.35050 1.04770 3.198 0.00139 ** ## news_namespiegel -1.39927 0.11001 -12.720 &lt; 2e-16 *** ## news_namestern_g -1.67816 0.19464 -8.622 &lt; 2e-16 *** ## news_namewelt_co -1.34374 0.11475 -11.710 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.616 on 8004 degrees of freedom ## Multiple R-squared: 0.02589, Adjusted R-squared: 0.02528 ## F-statistic: 42.55 on 5 and 8004 DF, p-value: &lt; 2.2e-16 Noch deutlicher sichtbar in der Gegenüberstellung (Literatur vs. Zeitung): lesbarkeit %&gt;% lm(wordnum_utterance ~ functiontype, data = .) %&gt;% summary() ## ## Call: ## lm(formula = wordnum_utterance ~ functiontype, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.474 -2.239 -0.258 1.602 66.526 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.939 1.056 19.826 &lt; 2e-16 *** ## functiontypezeitung -4.465 1.057 -4.225 2.42e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.659 on 8008 degrees of freedom ## Multiple R-squared: 0.002224, Adjusted R-squared: 0.002099 ## F-statistic: 17.85 on 1 and 8008 DF, p-value: 2.42e-05 Längere Wörter (mehr Silben pro Wort) in Zeitungen als in Romanen. lesbarkeit %&gt;% lm(syl_per_wrd ~ functiontype, data = .) %&gt;% summary() ## ## Call: ## lm(formula = syl_per_wrd ~ functiontype, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.75095 -0.08350 0.00680 0.08442 0.55262 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.47186 0.03821 38.52 &lt;2e-16 *** ## functiontypezeitung 0.40409 0.03824 10.57 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1324 on 8008 degrees of freedom ## Multiple R-squared: 0.01376, Adjusted R-squared: 0.01363 ## F-statistic: 111.7 on 1 and 8008 DF, p-value: &lt; 2.2e-16 Aber die Anzahl der Silben pro Äußerung in den Romanen und Zeitungstexten unterscheidet sich nicht wesentlich (p = 0,979). lesbarkeit %&gt;% lm(sylnum_utterance ~ functiontype, data = .) %&gt;% summary() ## ## Call: ## lm(formula = sylnum_utterance ~ functiontype, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -24.274 -4.649 -0.522 3.946 125.976 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.96610 2.20905 14.018 &lt;2e-16 *** ## functiontypezeitung 0.05815 2.21071 0.026 0.979 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.652 on 8008 degrees of freedom ## Multiple R-squared: 8.64e-08, Adjusted R-squared: -0.0001248 ## F-statistic: 0.0006919 on 1 and 8008 DF, p-value: 0.979 13.11 Flesch-Kincade Grade Level (USA, GB) https://paldhous.github.io/NICAR/2019/r-text-analysis.html The code below uses the quanteda functions ntoken, nsentence and nsyllable to count the words, sentences, and syllables in each addresss. Then it uses those values to calculate the Flesch-Kincaid reading grade level, a widely used measure of linguistic complexity. sou = read_csv(&quot;data/sou.csv&quot;) # word, sentence, and syllable counts, plus reading scores sou = sou %&gt;% mutate(syllables = nsyllable(text), sentences = nsentence(text), words = ntoken(text, remove_punct = TRUE), fk_grade = 0.39*(words/sentences) + 11.8*(syllables/words) - 15.59) %&gt;% arrange(date) The following chart shows how the reading grade level of State of the Union addresses has declined over the years. The points on the chart are colored by party, and scaled by the length of the address in words. # color palette for parties party_pal &lt;- c(&quot;#1482EE&quot;,&quot;#228B22&quot;,&quot;#E9967A&quot;,&quot;#686868&quot;,&quot;#FF3300&quot;,&quot;#EEC900&quot;) # reading score chart ggplot(sou, aes(x = date, y = fk_grade, color = party, size = words)) + geom_point(alpha = 0.5) + geom_smooth(se = FALSE, color = &quot;black&quot;, method = &quot;lm&quot;, size = 0.5, linetype = &quot;dotted&quot;) + scale_size_area(max_size = 10, guide = &quot;none&quot;) + scale_color_manual(values = party_pal, name = &quot;&quot;, breaks = c(&quot;Democratic&quot;,&quot;Republican&quot;,&quot;Whig&quot;,&quot;Democratic-Republican&quot;,&quot;Federalist&quot;,&quot;None&quot;)) + scale_y_continuous(limits = c(4,27), breaks = c(5,10,15,20,25)) + theme_minimal(base_size = 18) + xlab(&quot;&quot;) + ylab(&quot;Reading level&quot;) + guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) + theme(legend.position = c(0.4,0.22), legend.text = element_text(color=&quot;#909090&quot;, size = 16), panel.grid.minor = element_blank()) library(plotly) # color palette for parties party_pal &lt;- c(&quot;#1482EE&quot;,&quot;#228B22&quot;,&quot;#E9967A&quot;,&quot;#686868&quot;,&quot;#FF3300&quot;,&quot;#EEC900&quot;) # reading score chart u &lt;- ggplot(sou, aes(x = date, y = fk_grade, color = party, size = words)) + geom_point(alpha = 0.5) + geom_smooth(se = FALSE, color = &quot;black&quot;, method = &quot;lm&quot;, size = 0.5, linetype = &quot;dotted&quot;) + scale_size_area(max_size = 10, guide = &quot;none&quot;) + scale_color_manual(values = party_pal, name = &quot;&quot;, breaks = c(&quot;Democratic&quot;,&quot;Republican&quot;,&quot;Whig&quot;,&quot;Democratic-Republican&quot;,&quot;Federalist&quot;,&quot;None&quot;)) + scale_y_continuous(limits = c(4,27), breaks = c(5,10,15,20,25)) + theme_minimal(base_size = 18) + xlab(&quot;&quot;) + ylab(&quot;Reading level&quot;) + guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) + theme(legend.position = c(0.4,0.22), legend.text = element_text(color=&quot;#909090&quot;, size = 16), panel.grid.minor = element_blank()) ggplotly(u, height = 500, width = 1000) 13.12 Wiener Sachtextformel https://de.wikipedia.org/wiki/Lesbarkeitsindex Die Wiener Sachtextformel dient zur Berechnung der Lesbarkeit deutschsprachiger Texte. Sie gibt an, für welche Schulstufe ein Sachtext geeignet ist. Die Skala beginnt bei Schulstufe 4 und endet bei 15, wobei ab der Stufe 12 eher von Schwierigkeitsstufen als von Schulstufen gesprochen werden sollte. Ein Wert von 4 steht demnach für sehr leichten Text, dagegen bezeichnet 15 einen sehr schwierigen Text. Die Formel wurde aufgestellt von Richard Bamberger und Erich Vanecek. MS ist der Prozentanteil der Wörter mit drei oder mehr Silben, SL ist die mittlere Satzlänge (Anzahl Wörter), IW ist der Prozentanteil der Wörter mit mehr als sechs Buchstaben, ES ist der Prozentanteil der einsilbigen Wörter. Die erste Wiener Sachtextformel WSTF1 = 0.1935 * MS + 0.1672 * SL + 0.1297 * IW - 0.0327 * ES - 0.875 Die zweite Wiener Sachtextformel WSTF2 = 0.2007 * MS + 0.1682 * SL + 0.1373 * IW - 2.7 Die dritte Wiener Sachtextformel WSTF3 = 0.2963 * MS + 0.1905 * SL - 1.114 Die vierte Wiener Sachtextformel (im Hinblick auf die Jahrgangsstufe) WSTF4 = 0.2744 * MS + 0.2656 * SL - 1.69 # Das Beispiel mit den Entchen liefert mit der ersten WSTF einen Index von 3.8: satz=&quot;Alle meine Entchen schwimmen auf dem See, Köpfchen unters Wasser, Schwänzchen in die Höh.&quot; WSTF1 = 0.1935 * 0 + 0.1672 * 14 + 0.1297 * 29 - 0.0327 * 43 - 0.875 In unserem Korpus: lesbarkeitsindex %&gt;% filter(news_name == &quot;faz_cor&quot;) %&gt;% add_count(news_name) %&gt;% unnest_tokens(word, text) %&gt;% mutate(ms = ifelse(nsyllable(word) &gt; 2, 1, 0), # Wort mit mehr als 2 Silben iw = ifelse(nchar(word) &gt; 5, 1, 0), # Wort mit mehr als 5 Buchstaben es = ifelse(nsyllable(word) &lt; 2, 1, 0)) %&gt;% # Wort mit einer Silbe drop_na() %&gt;% group_by(news_name) %&gt;% summarise(MS = sum(100*ms/(words*n)), # MS: Prozentanteil der Wörter mit 3 oder mehr Silben SL = mean(words/sentences), # SL: mittlere Satzlänge (Anzahl Wörter) IW = sum(100*iw/(words*n)), # IW: Prozentanteil der Wörter mit mehr als 6 Buchstaben ES = sum(100*es/(words*n))) %&gt;% # ES: Prozentanteil der einsilbigen Wörter mutate(WSTF1 = 0.1935 * MS + 0.1672 * SL + 0.1297 * IW - 0.0327 * ES - 0.875, WSTF2 = 0.2007 * MS + 0.1682 * SL + 0.1373 * IW - 2.7, WSTF3 = 0.2963 * MS + 0.1905 * SL - 1.114, WSTF4 = 0.2744 * MS + 0.2656 * SL - 1.69) ## # A tibble: 1 x 9 ## news_name MS SL IW ES WSTF1 WSTF2 WSTF3 WSTF4 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 faz_cor 25.4 17.8 43.8 47.1 11.1 11.4 9.80 10.0 lesbarkeitsindex %&gt;% filter(news_name == &quot;spiegel&quot;) %&gt;% add_count(news_name) %&gt;% unnest_tokens(word, text) %&gt;% mutate(ms = ifelse(nsyllable(word) &gt; 2, 1, 0), # Wort mit mehr als 2 Silben iw = ifelse(nchar(word) &gt; 5, 1, 0), # Wort mit mehr als 5 Buchstaben es = ifelse(nsyllable(word) &lt; 2, 1, 0)) %&gt;% # Wort mit einer Silbe drop_na() %&gt;% group_by(news_name) %&gt;% summarise(MS = sum(100*ms/(words*n)), # MS: Prozentanteil der Wörter mit 3 oder mehr Silben SL = mean(words/sentences), # SL: mittlere Satzlänge (Anzahl Wörter) IW = sum(100*iw/(words*n)), # IW: Prozentanteil der Wörter mit mehr als 6 Buchstaben ES = sum(100*es/(words*n))) %&gt;% # ES: Prozentanteil der einsilbigen Wörter mutate(WSTF1 = 0.1935 * MS + 0.1672 * SL + 0.1297 * IW - 0.0327 * ES - 0.875, WSTF2 = 0.2007 * MS + 0.1682 * SL + 0.1373 * IW - 2.7, WSTF3 = 0.2963 * MS + 0.1905 * SL - 1.114, WSTF4 = 0.2744 * MS + 0.2656 * SL - 1.69) ## # A tibble: 1 x 9 ## news_name MS SL IW ES WSTF1 WSTF2 WSTF3 WSTF4 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 spiegel 24.8 16.3 44.2 47.3 10.8 11.1 9.33 9.43 lesbarkeitsindex %&gt;% filter(news_name == &quot;welt_co&quot;) %&gt;% add_count(news_name) %&gt;% unnest_tokens(word, text) %&gt;% mutate(ms = ifelse(nsyllable(word) &gt; 2, 1, 0), # Wort mit mehr als 2 Silben iw = ifelse(nchar(word) &gt; 5, 1, 0), # Wort mit mehr als 5 Buchstaben es = ifelse(nsyllable(word) &lt; 2, 1, 0)) %&gt;% # Wort mit einer Silbe drop_na() %&gt;% group_by(news_name) %&gt;% summarise(MS = sum(100*ms/(words*n)), # MS: Prozentanteil der Wörter mit 3 oder mehr Silben SL = mean(words/sentences), # SL: mittlere Satzlänge (Anzahl Wörter) IW = sum(100*iw/(words*n)), # IW: Prozentanteil der Wörter mit mehr als 6 Buchstaben ES = sum(100*es/(words*n))) %&gt;% # ES: Prozentanteil der einsilbigen Wörter mutate(WSTF1 = 0.1935 * MS + 0.1672 * SL + 0.1297 * IW - 0.0327 * ES - 0.875, WSTF2 = 0.2007 * MS + 0.1682 * SL + 0.1373 * IW - 2.7, WSTF3 = 0.2963 * MS + 0.1905 * SL - 1.114, WSTF4 = 0.2744 * MS + 0.2656 * SL - 1.69) ## # A tibble: 1 x 9 ## news_name MS SL IW ES WSTF1 WSTF2 WSTF3 WSTF4 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 welt_co 24.0 16.4 43.1 48.6 10.5 10.8 9.11 9.24 lesbarkeitsindex %&gt;% filter(news_name == &quot;stern_g&quot;) %&gt;% add_count(news_name) %&gt;% unnest_tokens(word, text) %&gt;% mutate(ms = ifelse(nsyllable(word) &gt; 2, 1, 0), # Wort mit mehr als 2 Silben iw = ifelse(nchar(word) &gt; 5, 1, 0), # Wort mit mehr als 5 Buchstaben es = ifelse(nsyllable(word) &lt; 2, 1, 0)) %&gt;% # Wort mit einer Silbe drop_na() %&gt;% group_by(news_name) %&gt;% summarise(MS = sum(100*ms/(words*n)), # MS: Prozentanteil der Wörter mit 3 oder mehr Silben SL = mean(words/sentences), # SL: mittlere Satzlänge (Anzahl Wörter) IW = sum(100*iw/(words*n)), # IW: Prozentanteil der Wörter mit mehr als 6 Buchstaben ES = sum(100*es/(words*n))) %&gt;% # ES: Prozentanteil der einsilbigen Wörter mutate(WSTF1 = 0.1935 * MS + 0.1672 * SL + 0.1297 * IW - 0.0327 * ES - 0.875, WSTF2 = 0.2007 * MS + 0.1682 * SL + 0.1373 * IW - 2.7, WSTF3 = 0.2963 * MS + 0.1905 * SL - 1.114, WSTF4 = 0.2744 * MS + 0.2656 * SL - 1.69) ## # A tibble: 1 x 9 ## news_name MS SL IW ES WSTF1 WSTF2 WSTF3 WSTF4 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 stern_g 20.3 16.0 39.8 49.1 9.28 9.53 7.95 8.13 lesbarkeitsindex %&gt;% filter(news_name == &quot;focus_c&quot;) %&gt;% add_count(news_name) %&gt;% unnest_tokens(word, text) %&gt;% mutate(ms = ifelse(nsyllable(word) &gt; 2, 1, 0), # Wort mit mehr als 2 Silben iw = ifelse(nchar(word) &gt; 5, 1, 0), # Wort mit mehr als 5 Buchstaben es = ifelse(nsyllable(word) &lt; 2, 1, 0)) %&gt;% # Wort mit einer Silbe drop_na() %&gt;% group_by(news_name) %&gt;% summarise(MS = sum(100*ms/(words*n)), # MS: Prozentanteil der Wörter mit 3 oder mehr Silben SL = mean(words/sentences), # SL: mittlere Satzlänge (Anzahl Wörter) IW = sum(100*iw/(words*n)), # IW: Prozentanteil der Wörter mit mehr als 6 Buchstaben ES = sum(100*es/(words*n))) %&gt;% # ES: Prozentanteil der einsilbigen Wörter mutate(WSTF1 = 0.1935 * MS + 0.1672 * SL + 0.1297 * IW - 0.0327 * ES - 0.875, WSTF2 = 0.2007 * MS + 0.1682 * SL + 0.1373 * IW - 2.7, WSTF3 = 0.2963 * MS + 0.1905 * SL - 1.114, WSTF4 = 0.2744 * MS + 0.2656 * SL - 1.69) ## # A tibble: 1 x 9 ## news_name MS SL IW ES WSTF1 WSTF2 WSTF3 WSTF4 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 focus_c 26.2 16.9 45.2 46.0 11.4 11.6 9.86 9.98 lesbarkeitsindex %&gt;% filter(news_name == &quot;romane&quot;) %&gt;% add_count(news_name) %&gt;% unnest_tokens(word, text) %&gt;% mutate(ms = ifelse(nsyllable(word) &gt; 2, 1, 0), # Wort mit mehr als 2 Silben iw = ifelse(nchar(word) &gt; 5, 1, 0), # Wort mit mehr als 5 Buchstaben es = ifelse(nsyllable(word) &lt; 2, 1, 0)) %&gt;% # Wort mit einer Silbe drop_na() %&gt;% group_by(news_name) %&gt;% summarise(MS = sum(100*ms/(words*n)), # MS: Prozentanteil der Wörter mit 3 oder mehr Silben SL = mean(words/sentences), # SL: mittlere Satzlänge (Anzahl Wörter) IW = sum(100*iw/(words*n)), # IW: Prozentanteil der Wörter mit mehr als 6 Buchstaben ES = sum(100*es/(words*n))) %&gt;% # ES: Prozentanteil der einsilbigen Wörter mutate(WSTF1 = 0.1935 * MS + 0.1672 * SL + 0.1297 * IW - 0.0327 * ES - 0.875, WSTF2 = 0.2007 * MS + 0.1682 * SL + 0.1373 * IW - 2.7, WSTF3 = 0.2963 * MS + 0.1905 * SL - 1.114, WSTF4 = 0.2744 * MS + 0.2656 * SL - 1.69) ## # A tibble: 1 x 9 ## news_name MS SL IW ES WSTF1 WSTF2 WSTF3 WSTF4 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 romane 13.3 19.0 31.1 51.9 7.21 7.44 6.45 7.01 wstf1_idx = lesbarkeitsindex_romane %&gt;% unnest_tokens(word, text) %&gt;% mutate(ms = ifelse(nsyllable(word) &gt; 2, 1, 0), iw = ifelse(nchar(word) &gt; 5, 1, 0), es = ifelse(nsyllable(word) &lt; 2, 1, 0)) %&gt;% drop_na() %&gt;% group_by(book_name) %&gt;% summarise(MS = sum(100*ms/words), SL = mean(words/sentences), IW = sum(100*iw/words), ES = sum(100*es/words)) wstf1_idx %&gt;% rmarkdown::paged_table() lesbarkeitsindex_romane %&gt;% # filter(str_detect(book_name, &quot;Der Prozess&quot;)) %&gt;% unnest_tokens(word, text) %&gt;% mutate(ms = ifelse(nsyllable(word) &gt; 2, 1, 0), iw = ifelse(nchar(word) &gt; 5, 1, 0), es = ifelse(nsyllable(word) &lt; 2, 1, 0)) %&gt;% drop_na() %&gt;% group_by(book_name) %&gt;% summarise(MS = sum(100*ms/words), SL = mean(words/sentences), IW = sum(100*iw/words), ES = sum(100*es/words)) %&gt;% mutate(WSTF1 = 0.1935 * MS + 0.1672 * SL + 0.1297 * IW - 0.0327 * ES - 0.875, WSTF2 = 0.2007 * MS + 0.1682 * SL + 0.1373 * IW - 2.7, WSTF3 = 0.2963 * MS + 0.1905 * SL - 1.114, WSTF4 = 0.2744 * MS + 0.2656 * SL - 1.69) %&gt;% rmarkdown::paged_table() "],["untertitel.html", "Kapitel 14 Untertitel 14.1 Programme starten 14.2 Daten laden 14.3 Datensätze vorbereiten 14.4 Morphologie der Untertitel 14.5 Syntax: Dependenz", " Kapitel 14 Untertitel 14.1 Programme starten library(tidyverse) library(tidytext) library(scales) library(udpipe) stringsAsFactors = FALSE 14.2 Daten laden Die englischen und deutschen Untertitel zum Film Avatar stammen aus der Datensammlung von Natalia Levshina (Levshina 2015), die slowenischen Untertitel stammen von der Webseite nachschauen. Zuerst laden wir sechs Dateien mit Untertiteln zum Film Avatar. Hauptsächlich werden wir mit den Untertiteln in englischer, deutscher und slowenischer Sprache arbeiten. library(tidyverse) avatar_eng = read_lines(&quot;data/sub/Avatar_eng.txt&quot;) avatar_deu = read_lines(&quot;data/sub/Avatar_deu.txt&quot;) avatar_slv = read_lines(&quot;data/sub/Avatar_slv.txt&quot;) avatar_fra = read_lines(&quot;data/sub/Avatar_fra.txt&quot;) avatar_ita = read_lines(&quot;data/sub/Avatar_ita.txt&quot;) avatar_tur = read_lines(&quot;data/sub/Avatar_tur.txt&quot;) head(avatar_eng); head(avatar_deu); head(avatar_slv) ## [1] &quot;1&quot; ## [2] &quot;00:00:39,799 --&gt; 00:00:42,039&quot; ## [3] &quot;When I was lying there in the VA hospital,&quot; ## [4] &quot;&quot; ## [5] &quot;2&quot; ## [6] &quot;00:00:42,176 --&gt; 00:00:45,136&quot; ## [1] &quot;1&quot; ## [2] &quot;00:00:39,798 --&gt; 00:00:42,091&quot; ## [3] &quot;Als ich da im Veteranen-Krankenhaus lag,&quot; ## [4] &quot;&quot; ## [5] &quot;2&quot; ## [6] &quot;00:00:42,176 --&gt; 00:00:45,136&quot; ## [1] &quot;&quot; ## [2] &quot;1&quot; ## [3] &quot;00:00:38,160 --&gt; 00:00:40,720&quot; ## [4] &quot;&lt;i&gt;Ko sem leal v veteranski bolninici&lt;/i&gt;&quot; ## [5] &quot;&quot; ## [6] &quot;2&quot; 14.3 Datensätze vorbereiten 14.3.1 Textspalte vorbereiten Untertitel haben ein besonderes Format. Recht einfach sind Datenmodifizierungen mit den tidyverse-Funktionen. Die Voraussetzung für ihre Verwendung ist die Umwandlung der Texte ins Tabellenformat. Dann können wir z.B. auch neue Tabellenspalten mit den Zeitangaben bilden. a1 = avatar_eng %&gt;% as_tibble() %&gt;% mutate(row_tc = row_number()) %&gt;% filter(str_detect(value, &quot;--&gt;&quot;)) %&gt;% rename(timecode = value) a2 = avatar_eng %&gt;% as_tibble() %&gt;% mutate(row_id = row_number()) %&gt;% filter(str_detect(value, &quot;[a-zA-Z]&quot;)) %&gt;% rename(text = value) %&gt;% mutate(text = str_replace(text, &quot;\\\\&lt;i\\\\&gt;&quot;, &quot;&quot;)) %&gt;% mutate(text = str_replace(text, &quot;\\\\&lt;/i\\\\&gt;&quot;, &quot;&quot;)) %&gt;% mutate(language = &quot;eng&quot;) avatar_eng = bind_cols(a1,a2) %&gt;% select(timecode, text) %&gt;% separate(timecode, into = c(&quot;start&quot;, &quot;end&quot;), sep = &quot;\\\\-\\\\-\\\\&gt;&quot;) %&gt;% rmarkdown::paged_table() a2a = a2 %&gt;% mutate(sentence_id = row_number()) Da die Anfangs- und Endzeit der Untertitel in den drei Sprachen nicht übereinstimmt, wollen wir lediglich die Untertiteltexte beibehalten. b1 = avatar_deu %&gt;% as_tibble() %&gt;% mutate(row_tc = row_number()) %&gt;% filter(str_detect(value, &quot;--&gt;&quot;)) %&gt;% rename(timecode = value) b2 = avatar_deu %&gt;% as_tibble() %&gt;% mutate(row_id = row_number()) %&gt;% filter(str_detect(value, &quot;[a-zA-Z]&quot;)) %&gt;% rename(text = value) %&gt;% mutate(text = str_replace(text, &quot;\\\\&lt;i\\\\&gt;&quot;, &quot;&quot;)) %&gt;% mutate(text = str_replace(text, &quot;\\\\&lt;/i\\\\&gt;&quot;, &quot;&quot;)) %&gt;% mutate(language = &quot;deu&quot;) # avatar_deu = bind_cols(a1,a2) # select(timecode, text) %&gt;% # separate(timecode, into = c(&quot;start&quot;, &quot;end&quot;), sep = &quot;\\\\-\\\\-\\\\&gt;&quot;) # tail(avatar_deu) b2a = b2 %&gt;% mutate(sentence_id = row_number()) c1 = avatar_slv %&gt;% as_tibble() %&gt;% mutate(row_tc = row_number()) %&gt;% filter(str_detect(value, &quot;--&gt;&quot;)) %&gt;% rename(timecode = value) c2 = avatar_slv %&gt;% as_tibble() %&gt;% mutate(row_id = row_number()) %&gt;% filter(str_detect(value, &quot;[a-zA-Z]&quot;)) %&gt;% rename(text = value) %&gt;% mutate(text = str_replace(text, &quot;\\\\&lt;i\\\\&gt;&quot;, &quot;&quot;)) %&gt;% mutate(text = str_replace(text, &quot;\\\\&lt;/i\\\\&gt;&quot;, &quot;&quot;)) %&gt;% mutate(language = &quot;slv&quot;) # avatar_slv = bind_cols(a1,a2) # select(timecode, text) %&gt;% # separate(timecode, into = c(&quot;start&quot;, &quot;end&quot;), sep = &quot;\\\\-\\\\-\\\\&gt;&quot;) # tail(avatar_slv) c2a = c2 %&gt;% mutate(sentence_id = row_number()) avatar_fra = avatar_fra %&gt;% as_tibble() %&gt;% mutate(row_id = row_number()) %&gt;% filter(str_detect(value, &quot;[a-zA-Z]&quot;)) %&gt;% rename(text = value) %&gt;% mutate(text = str_replace(text, &quot;\\\\&lt;i\\\\&gt;&quot;, &quot;&quot;)) %&gt;% mutate(text = str_replace(text, &quot;\\\\&lt;/i\\\\&gt;&quot;, &quot;&quot;)) %&gt;% mutate(language = &quot;fra&quot;) %&gt;% mutate(sentence_id = row_number()) avatar_ita = avatar_ita %&gt;% as_tibble() %&gt;% mutate(row_id = row_number()) %&gt;% filter(str_detect(value, &quot;[a-zA-Z]&quot;)) %&gt;% rename(text = value) %&gt;% mutate(text = str_replace(text, &quot;\\\\&lt;i\\\\&gt;&quot;, &quot;&quot;)) %&gt;% mutate(text = str_replace(text, &quot;\\\\&lt;/i\\\\&gt;&quot;, &quot;&quot;)) %&gt;% mutate(language = &quot;ita&quot;) %&gt;% mutate(sentence_id = row_number()) avatar_tur = avatar_tur %&gt;% as_tibble() %&gt;% mutate(row_id = row_number()) %&gt;% filter(str_detect(value, &quot;[a-zA-Z]&quot;)) %&gt;% rename(text = value) %&gt;% mutate(text = str_replace(text, &quot;\\\\&lt;i\\\\&gt;&quot;, &quot;&quot;)) %&gt;% mutate(text = str_replace(text, &quot;\\\\&lt;/i\\\\&gt;&quot;, &quot;&quot;)) %&gt;% mutate(language = &quot;tur&quot;) %&gt;% mutate(sentence_id = row_number()) 14.3.2 Datensätze verknüpfen Nun verknüpfen wir die drei Datensätze zu einem einzigen. avatar = bind_rows(a2a,b2a,c2a, avatar_fra, avatar_ita, avatar_tur) 14.3.3 Merkmale hinzufügen Mit Hilfe von quanteda-Funktionen fügen wir dem Datensatz noch weitere Kenngrößen hinzu, und zwar die Anzahl der Wortformerscheinungen oder Tokens pro Äußerung (sentlen), die Anzahl der Silben pro Äußerung (syllables), die Wortlänge (wordlen), die Anzahl der verschiedenen Wortformen (Types) und das Type-Token-Verhältnis als bekanntes Maß für lexikalische Diversität. avatar = avatar %&gt;% mutate(txt = str_replace_all(text, &quot;[:punct:]&quot;, &quot;&quot;)) %&gt;% mutate(sentlen = quanteda::ntoken(txt)) %&gt;% mutate(syllables = nsyllable::nsyllable(txt)) %&gt;% mutate(types = quanteda::ntype(txt)) %&gt;% mutate(wordlen = syllables/sentlen) %&gt;% mutate(ttr = types/sentlen) %&gt;% select(-txt) Speichern für spätere Verwendung. write_rds(avatar, &quot;data/avatar.rds&quot;) write_csv(avatar, &quot;data/avatar.csv&quot;) avatar = read_rds(&quot;data/avatar.rds&quot;) 14.3.4 Konkordanzrecherche Ein Beispiel einer Konkordanzrecherche mit Hilfe von kwic - dem Konkordanz-Tool in quanteda: x = quanteda::corpus(avatar, text_field = &quot;text&quot;) %&gt;% quanteda::tokens() quanteda::kwic(x, pattern = &quot;planet&quot;) %&gt;% as_tibble() %&gt;% rmarkdown::paged_table() 14.3.5 Textzerlegung Zerlegung der Untertitellinien in Wörter: library(tidytext) avatar_words = avatar %&gt;% unnest_tokens(word, text, drop = FALSE) %&gt;% select(-text) avatar_words %&gt;% rmarkdown::paged_table() 14.3.6 Zerlegung und Annotation Zuerst müssen wir für jede Sprache ein udpipe-Sprachmodell laden, um für jede der drei Untertitelversionen eine morphosyntaktische Annotation vorzunehmen. Englisch: library(udpipe) destfile = &quot;english-ewt-ud-2.5-191206.udpipe&quot; if(!file.exists(destfile)){ language_model &lt;- udpipe_download_model(language = &quot;english&quot;) engmod &lt;- udpipe_load_model(language_model$file_model) } else { file_model = destfile engmod &lt;- udpipe_load_model(file_model) } x = udpipe_annotate(engmod, x = avatar$text[avatar$language == &quot;eng&quot;], trace = FALSE) udeng = as.data.frame(x) Deutsch: library(udpipe) destfile = &quot;german-hdt-ud-2.5-191206.udpipe&quot; # destfile = &quot;german-gsd-ud-2.5-191206.udpipe&quot; if(!file.exists(destfile)){ language_model &lt;- udpipe_download_model(language = &quot;german&quot;) deumod &lt;- udpipe_load_model(language_model$file_model) } else { file_model = destfile deumod &lt;- udpipe_load_model(file_model) } x = udpipe_annotate(deumod, x = avatar$text[avatar$language == &quot;deu&quot;], trace = F) uddeu = as.data.frame(x) Slowenisch: library(udpipe) destfile = &quot;slovenian-ssj-ud-2.5-191206.udpipe&quot; # destfile = &quot;german-gsd-ud-2.5-191206.udpipe&quot; if(!file.exists(destfile)){ language_model &lt;- udpipe_download_model(language = &quot;slovenian&quot;) slvmod &lt;- udpipe_load_model(language_model$file_model) } else { file_model = destfile slvmod &lt;- udpipe_load_model(file_model) } x = udpipe_annotate(slvmod, x = avatar$text[avatar$language == &quot;slv&quot;], trace = F) udslv = as.data.frame(x) Französisch: library(udpipe) destfile = &quot;french-gsd-ud-2.5-191206.udpipe&quot; if(!file.exists(destfile)){ language_model &lt;- udpipe_download_model(language = &quot;french-gsd&quot;) framod &lt;- udpipe_load_model(language_model$file_model) } else { file_model = destfile framod &lt;- udpipe_load_model(file_model) } x = udpipe_annotate(framod, x = avatar$text[avatar$language == &quot;fra&quot;], trace = FALSE) udfra = as.data.frame(x) Italienisch: library(udpipe) destfile = &quot;italian-isdt-ud-2.5-191206.udpipe&quot; if(!file.exists(destfile)){ language_model &lt;- udpipe_download_model(language = &quot;italian-isdt&quot;) itamod &lt;- udpipe_load_model(language_model$file_model) } else { file_model = destfile itamod &lt;- udpipe_load_model(file_model) } x = udpipe_annotate(itamod, x = avatar$text[avatar$language == &quot;ita&quot;], trace = FALSE) udita = as.data.frame(x) Türkisch: library(udpipe) destfile = &quot;turkish-imst-ud-2.5-191206.udpipe&quot; if(!file.exists(destfile)){ language_model &lt;- udpipe_download_model(language = &quot;turkish-imst&quot;) turmod &lt;- udpipe_load_model(language_model$file_model) } else { file_model = destfile turmod &lt;- udpipe_load_model(file_model) } x = udpipe_annotate(turmod, x = avatar$text[avatar$language == &quot;tur&quot;], trace = FALSE) udtur = as.data.frame(x) Anpassung der Tabellenspalte token_id als numeric(). udfra = udfra %&gt;% separate(token_id, into = c(&quot;token_id&quot;, &quot;token_id2&quot;), sep = &quot;-&quot;) %&gt;% mutate(token_id = as.numeric(token_id)) %&gt;% select(-token_id2) udita = udita %&gt;% separate(token_id, into = c(&quot;token_id&quot;, &quot;token_id2&quot;), sep = &quot;-&quot;) %&gt;% mutate(token_id = as.numeric(token_id)) %&gt;% select(-token_id2) udtur = udtur %&gt;% separate(token_id, into = c(&quot;token_id&quot;, &quot;token_id2&quot;), sep = &quot;-&quot;) %&gt;% mutate(token_id = as.numeric(token_id)) %&gt;% select(-token_id2) Die Datensätze wollen wir für anderweitige Verwendungen speichern, und zwar sowohl im conllu-Format als auch im csv-Format. In beiden Fällen erhalten wir Textdateien. write.table(as_conllu(udeng), file = &quot;data/Avatar_ud_eng.conllu&quot;, sep = &quot;\\t&quot;, quote = F, row.names = F) write.table(as_conllu(uddeu), file = &quot;data/Avatar_ud_deu.conllu&quot;, sep = &quot;\\t&quot;, quote = F, row.names = F) write.table(as_conllu(udslv), file = &quot;data/Avatar_ud_slv.conllu&quot;, sep = &quot;\\t&quot;, quote = F, row.names = F) write.table(as_conllu(udfra), file = &quot;data/Avatar_ud_fra.conllu&quot;, sep = &quot;\\t&quot;, quote = F, row.names = F) write.table(as_conllu(udita), file = &quot;data/Avatar_ud_ita.conllu&quot;, sep = &quot;\\t&quot;, quote = F, row.names = F) write.table(as_conllu(udtur), file = &quot;data/Avatar_ud_tur.conllu&quot;, sep = &quot;\\t&quot;, quote = F, row.names = F) write_csv(udeng, &quot;data/Avatar_ud_eng.csv&quot;) write_csv(uddeu, &quot;data/Avatar_ud_deu.csv&quot;) write_csv(udslv, &quot;data/Avatar_ud_slv.csv&quot;) write_csv(udfra, &quot;data/Avatar_ud_fra.csv&quot;) write_csv(udita, &quot;data/Avatar_ud_ita.csv&quot;) write_csv(udtur, &quot;data/Avatar_ud_tur.csv&quot;) udeng = read_csv(&quot;data/Avatar_ud_eng.csv&quot;) uddeu = read_csv(&quot;data/Avatar_ud_deu.csv&quot;) udslv = read_csv(&quot;data/Avatar_ud_slv.csv&quot;) udfra = read_csv(&quot;data/Avatar_ud_fra.csv&quot;) udita = read_csv(&quot;data/Avatar_ud_ita.csv&quot;) udtur = read_csv(&quot;data/Avatar_ud_tur.csv&quot;) Den drei annotierten Datensätzen wollen wir noch einige weitere Merkmale hinzufügen (und zwar mit den mutate()-Befehlen, in denen auch einfache quanteda-Funktionen verwendet werden). Außerdem soll die komplexe Tabellenspalte feats (features) in einzelne Spalten aufgeteilt werden (und zwar mit der cbind_morphological()-Funktion von udpipe). Da wir dies mit allen drei Datensätzen anstellen wollen, bilden wir eine Funktion dazu, die als Input eine Tabelle (tbl) verlangt, in denen die Spalten word, token, feats, sentence zur Verfügung stehen: tokenize_annotate = function(tbl){ tbl %&gt;% unnest_tokens(word, token, drop = F) %&gt;% cbind_morphological(term = &quot;feats&quot;, which = c(&quot;PronType&quot;,&quot;NumType&quot;,&quot;Poss&quot;,&quot;Reflex&quot;, &quot;Foreign&quot;,&quot;Abbr&quot;,&quot;Typo&quot;, &quot;Gender&quot;,&quot;Animacy&quot;,&quot;NounClass&quot;, &quot;Case&quot;,&quot;Number&quot;,&quot;Definite&quot;,&quot;Degree&quot;, &quot;VerbForm&quot;,&quot;Person&quot;,&quot;Tense&quot;,&quot;Mood&quot;, &quot;Aspect&quot;,&quot;Voice&quot;,&quot;Evident&quot;, &quot;Polarity&quot;,&quot;Polite&quot;,&quot;Clusivity&quot;)) %&gt;% mutate(txt = str_replace_all(sentence, &quot;[:punct:]&quot;, &quot;&quot;)) %&gt;% mutate(sentlen = quanteda::ntoken(txt)) %&gt;% mutate(syllables = nsyllable::nsyllable(txt)) %&gt;% mutate(types = quanteda::ntype(txt)) %&gt;% mutate(wordlen = syllables/sentlen) %&gt;% mutate(ttr = types/sentlen) %&gt;% select(-txt, -feats) } Die für die Verwendung der Funktion entsprechenden Tabellen sind die zuvor gebildeten Tabellen udeng, uddeu und udslv. Nach der Anreicherung der Datensätze verknüpfen wir sie zu einem einzigen. avatar_eng_udpiped &lt;- udeng %&gt;% tokenize_annotate() %&gt;% mutate(language = &quot;eng&quot;) avatar_deu_udpiped &lt;- uddeu %&gt;% tokenize_annotate() %&gt;% mutate(language = &quot;deu&quot;) avatar_slv_udpiped &lt;- udslv %&gt;% tokenize_annotate() %&gt;% mutate(language = &quot;slv&quot;) avatar_fra_udpiped &lt;- udfra %&gt;% tokenize_annotate() %&gt;% mutate(language = &quot;fra&quot;) avatar_ita_udpiped &lt;- udita %&gt;% tokenize_annotate() %&gt;% mutate(language = &quot;ita&quot;) avatar_tur_udpiped &lt;- udtur %&gt;% tokenize_annotate() %&gt;% mutate(language = &quot;tur&quot;) avatar_words_udpiped = bind_rows(avatar_eng_udpiped, avatar_deu_udpiped, avatar_slv_udpiped, avatar_fra_udpiped, avatar_ita_udpiped, avatar_tur_udpiped) avatar_words_udpiped %&gt;% rmarkdown::paged_table() Für spätere Verwendungen speichern wir den Datensatz in zwei verschiedenen Formaten. write_rds(avatar_words_udpiped, &quot;data/avatar_words_udpiped.rds&quot;) write_csv(avatar_words_udpiped, &quot;data/avatar_words_udpiped.csv&quot;) avatar_words_udpiped = read_rds(&quot;data/avatar_words_udpiped.rds&quot;) 14.4 Morphologie der Untertitel Um einzelne Wörter und ihre Funktionen im Text aufzuspüren, brauchen wir nur die filter()- und die select()-Funktion einzugeben. Beispielsweise das Lemma brother in den englischen Untertiteln: avatar_words_udpiped %&gt;% filter(lemma == &quot;brother&quot;) %&gt;% select(sentence, token, lemma, upos, dep_rel) %&gt;% rmarkdown::paged_table() Dasselbe mit dem deutschen Bruder und dem slowenischen brat: avatar_words_udpiped %&gt;% filter(lemma == &quot;Bruder&quot;) %&gt;% select(sentence, token, lemma, upos, dep_rel) %&gt;% rmarkdown::paged_table() avatar_words_udpiped %&gt;% filter(lemma == &quot;brat&quot;) %&gt;% select(sentence, token, lemma, upos, dep_rel) %&gt;% rmarkdown::paged_table() Das Lemma brother bzw. scheint in den englischen Untertiteln ein wenig häufiger vorzukommen als die deutsche bzw. slowenische Entsprechung Bruder bzw. brat. 14.4.1 XRay Brother An welchen Stellen kommt das Wort in den Untertiteln vor? quanteda.textplots::textplot_xray( quanteda::kwic(avatar %&gt;% pull(text), pattern = c(&quot;brother&quot;,&quot;Bruder&quot;,&quot;brat&quot;)), scale = &quot;relative&quot;) Um die Stellen aus drei Texten besser vergleichen zu können, müssen wir drei xray-Diagramme erstellen und sie mit Hilfe von patchwork zusammenkleben. p1 = quanteda.textplots::textplot_xray( quanteda::kwic(avatar %&gt;% filter(language == &quot;eng&quot;) %&gt;% pull(text), pattern = &quot;brother&quot;), scale = &quot;relative&quot;) p2 = quanteda.textplots::textplot_xray( quanteda::kwic(avatar %&gt;% filter(language == &quot;deu&quot;) %&gt;% pull(text), pattern = &quot;Bruder&quot;), scale = &quot;relative&quot;) p3 = quanteda.textplots::textplot_xray( quanteda::kwic(avatar %&gt;% filter(language == &quot;slv&quot;) %&gt;% pull(text), pattern = &quot;brat&quot;), scale = &quot;relative&quot;) library(patchwork) p1|p2|p3 14.4.2 Substantive im Plural Als nächstes wollen wir alle als Substantive (Noun) identifizierte Einheiten herausfinden, die im Plural auftreten. #Find all plural nouns (tokens) avatar_words_udpiped %&gt;% filter(language == &quot;eng&quot; &amp; upos == &quot;NOUN&quot; &amp; morph_number == &quot;Plur&quot;) %&gt;% select(sentence, token, lemma, upos, morph_number) %&gt;% rmarkdown::paged_table() avatar_words_udpiped %&gt;% filter(language == &quot;deu&quot; &amp; upos == &quot;NOUN&quot; &amp; morph_number == &quot;Plur&quot;) %&gt;% select(sentence, token, lemma, upos, morph_number) %&gt;% rmarkdown::paged_table() avatar_words_udpiped %&gt;% filter(language == &quot;slv&quot; &amp; upos == &quot;NOUN&quot; &amp; morph_number == &quot;Plur&quot;) %&gt;% select(sentence, token, lemma, upos, morph_number) %&gt;% rmarkdown::paged_table() avatar_words_udpiped %&gt;% dplyr::select(language, token, lemma, upos, morph_number) %&gt;% group_by(language) %&gt;% filter(upos == &quot;NOUN&quot;) %&gt;% count(morph_number) %&gt;% pivot_wider(names_from = language, values_from = n) %&gt;% # mutate(across(everything(), ~ replace_na(.x, 0))) %&gt;% mutate_if(is.numeric, ~ replace_na(.x, 0)) %&gt;% mutate(morph_number = str_replace(morph_number, &quot;0&quot;, &quot;Unknown&quot;)) %&gt;% mutate(morph_number = fct_relevel( morph_number, levels = c(&quot;Sing&quot;,&quot;Plur&quot;,&quot;Dual&quot;,&quot;Unknown&quot;))) %&gt;% arrange(morph_number) %&gt;% rmarkdown::paged_table() 14.4.3 Adjektive im Komparativ In unserer nächsten Recherche wollen wir Komparativformen von Adjektiven ausfindig machen und ihre Stelle im Untertitel. Zuerst zählen wir die Wortarten (upos). Hier fällt auf, dass der Anteil einiger Wortarten in den slowenischen Untertiteln größer ist als in den anderen beiden Sprachen (z.B. Verben, Substantive), in anderen Fällen jedoch kleiner (z.B. Pronomen, die ja im Slowenischen nicht obligatorisch auftreten müssen). # Frequencies of parts of speech avatar_words_udpiped %&gt;% group_by(language) %&gt;% count(upos, sort = TRUE) %&gt;% mutate(pct = round(100*n/sum(n),2)) %&gt;% pivot_wider(names_from = language, values_from = c(n, pct)) %&gt;% arrange(upos) %&gt;% rmarkdown::paged_table() In den englischen Untertiteln wurden 17 Komparativformen identifiziert, in den deutschen 20 und in den slownischen 4. Der Anteil der Komparativformen ist also in den englischen und deutschen Untertiteln größer als in den slowenischen. Ähnlich verhält es sich mit den Superlativformen: deutsch (35 = 6%), englisch (14 = 2,77%), slowenisch (6 = 1,65) avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(language == &quot;eng&quot; | language == &quot;deu&quot; | language == &quot;slv&quot;) %&gt;% filter(upos == &quot;ADJ&quot;) %&gt;% count(morph_degree, sort = TRUE) %&gt;% mutate(pct = round(100*n/sum(n),2)) %&gt;% pivot_wider(names_from = language, values_from = c(n, pct)) %&gt;% # mutate(across(everything(), ~ replace_na(.x, 0))) %&gt;% mutate_if(is.numeric, ~ replace_na(.x, 0)) %&gt;% mutate(morph_degree = str_replace(morph_degree, &quot;0&quot;, &quot;Unknown&quot;)) %&gt;% mutate(morph_degree = fct_relevel( morph_degree, levels = c(&quot;Pos&quot;,&quot;Cmp&quot;,&quot;Sup&quot;,&quot;Abs&quot;,&quot;Unkown&quot;))) %&gt;% arrange(morph_degree) %&gt;% rmarkdown::paged_table() Anmerkung: Die Klassifzierung für die deutsche Sprache (Variante: german-gsd) enthält diese Kategorie nicht. Wir haben daher die german-hdt-Variante gewählt. 14.5 Syntax: Dependenz Programme wie udpipe oder spacyr sind auch in der Lage, syntaktische Dependenzrelationen gemäß der Stanforder sprachübergreifenden Typologie zu identifizieren und als Annotation auszugeben. Typologische Grundlage für die Annotation: Universal Stanford Dependencies: A cross-linguistic typology (de Marneffe et al. 2014). knitr::include_graphics(&quot;pictures/Screenshot 2021-08-27 at 12-14-22 Universal Dependency Relations.png&quot;) Mehr über das Datenformat: CoNLL-U Format Frequenzwerte der syntaktischen Abhängigkeitsrelationen in den Avatar-Untertiteln (englisch, deutsch, slowenisch): avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(language == &quot;eng&quot; | language == &quot;deu&quot; | language == &quot;slv&quot;) %&gt;% count(dep_rel, sort = TRUE) %&gt;% mutate(pct = round(100*n/sum(n),2)) %&gt;% pivot_wider(names_from = language, values_from = c(n, pct)) %&gt;% # mutate(across(everything(), ~ replace_na(.x, 0))) %&gt;% mutate_if(is.numeric, ~ replace_na(.x, 0)) %&gt;% mutate(dep_rel = str_replace(dep_rel, &quot;0&quot;, &quot;Unknown&quot;)) %&gt;% rmarkdown::paged_table() Gemäß udpipe erscheinen in den englischen und deutschen Untertiteln die Dependenzrelationen root, nsubj, advmod, det, obj am häufigsten. In den slowenischen Untertiteln haben die Relationen root, advmod, obj, case, nsubj die größten Frequenzwerte. Die Dependenzrelation root gibt uns Auskunft darüber, ob eine Wortfolge als Satz identifiziert wurde. Sie wird gewöhnlich mit Hilfe des (finiten) Verbs im Satz bestimmt. In elliptischen Sätzen wird eine der vorkommenden Wortformen mit root assoziiert. In der Tabelle ist (unter root) zu sehen, dass in den englischen Untertiteln 2026 satzwertige Einheiten identifiziert wurden, in den deutschen 2366 und in den slowenischen 1807. In der Tabelle zeigen die Prozentzahlen beispielsweise einen bemerkenswerten Unterschied in der Häufigkeit der Dependenzrelation nsubj, d.h. die Anzahl der identifizierten Subjekte. In den slowenischen Untertiteln liegt der Anteil deutlich unter dem in den englischen und deutschen. Das hängt damit zusammen, dass Slowenisch eine Pro-drop-Sprache ist, dass also unbetonte Personalpronomen (in Subjekt-Funktion) nicht sprachlich realisiert zu sein brauchen. Besonder deutlich wird dies, wenn wir einen Beispielsatz aus allen drei Texten visualisieren. Mit Hilfe der folgenden Funktion können wir die Dependenzrelationen im Satz visualisieren. Wir geben der Funktion den Namen plot_annotation(). library(igraph) library(ggraph) library(ggplot2) plot_annotation &lt;- function(x, size = 3){ stopifnot(is.data.frame(x) &amp; all(c(&quot;sentence_id&quot;, &quot;token_id&quot;, &quot;head_token_id&quot;, &quot;dep_rel&quot;, &quot;token_id&quot;, &quot;token&quot;, &quot;lemma&quot;, &quot;upos&quot;, &quot;xpos&quot;, &quot;feats&quot;) %in% colnames(x))) x &lt;- x[!is.na(x$head_token_id), ] x &lt;- x[x$sentence_id %in% min(x$sentence_id), ] edges &lt;- x[x$head_token_id != 0, c(&quot;token_id&quot;, &quot;head_token_id&quot;, &quot;dep_rel&quot;)] edges$label &lt;- edges$dep_rel g &lt;- graph_from_data_frame(edges, vertices = x[, c(&quot;token_id&quot;, &quot;token&quot;, &quot;lemma&quot;, &quot;upos&quot;, &quot;xpos&quot;, &quot;feats&quot;)], directed = TRUE) windowsFonts(&quot;Arial Narrow&quot; = windowsFont(&quot;Arial&quot;)) ggraph(g, layout = &quot;linear&quot;) + geom_edge_arc(ggplot2::aes(label = dep_rel, vjust = -0.20), arrow = grid::arrow(length = unit(4, &#39;mm&#39;), ends = &quot;last&quot;, type = &quot;closed&quot;), end_cap = ggraph::label_rect(&quot;wordswordswords&quot;), label_colour = &quot;red&quot;, check_overlap = TRUE, label_size = size) + geom_node_label(ggplot2::aes(label = token), col = &quot;darkgreen&quot;, size = size, fontface = &quot;bold&quot;) + geom_node_text(ggplot2::aes(label = upos), nudge_y = -0.35, size = size) + theme_graph(base_family = &quot;Arial Narrow&quot;) + labs(title = &quot;udpipe output&quot;, subtitle = &quot;tokenisation, parts of speech tagging &amp; dependency relations&quot;) } Hier ist ein Beispiel eines Avatar-Untertitels in drei Sprachen. Wegen der deutschen bzw. slowenischen Sonderzeichen wandeln wir den Text mit Hilfe der Funktion enc2utf8() ins erforderliche UTF8-Format um. # English mytext = udpipe(&quot;I started having these dreams of flying&quot;, &quot;english&quot;) x1 = plot_annotation(mytext, size = 3) # German mytext = &quot;Ich träumte auf einmal vom Fliegen&quot; %&gt;% enc2utf8() x = udpipe(mytext, &quot;german&quot;) x2 = plot_annotation(x, size = 3) # Slovenian mytext = &quot;Zael sem sanjati o letenju&quot; %&gt;% enc2utf8() x = udpipe(mytext, &quot;slovenian&quot;) x3 = plot_annotation(x, size = 3) Englischer Satz: PRON: Personalpronomen mit Subjekt-Funktion (nsubj) NOUN, VERB: Substantiv, Verb AUX: das Hilfs- oder Auxiliarverb xcomp: hier eine Relation zwischen zwei Verben, die gemeinsam das Prädikat des Satzes bilden DET: Determiner (Determinans), Begleiter eines Substantivs (meist handelt es sich um einen Artikel) obj: Objektfunktion (hier ist these dreams das Objekt des Verbs have) SCONJ: subordinierende Konjunktion (aber hier wäre prep für Präposition angebracht) acl: gewöhnlich bezogen auf einen finiten oder infiniten Satz, der eine Nominalphrase modifiziert (im Kontrast zu advcl, die ein Prädikat modifizieren) mark: ein Marker, der eine untergerodnete Phrase / Satz kennzeichnet. x1 Deutscher Satz: PRON: Personalpronomen mit Subjekt-Funktion (nsubj) NOUN, VERB, ADP, ADV: Substantiv, Verb, Adposition (hier: Präposition), Adverb DET: Determiner (Determinans), Begleiter eines Substantivs (meist handelt es sich um einen Artikel) obl: eine Art von Adjunkt, in der Valenzgrammatik gewöhnlich als Präpositionalobjekt klassifizert (hier ist vom Fliegen das Objekt des Verbs have) case: Element, das den Kasus einer Phrase regiert (z.B. von regiert den Dativ der Nominalphrase dem Fliegen) advmod: Element, das das Prädikat modifizert (Adverbialphrase). x2 Slowenischer Satz: Das Personalpronomen mit Subjekt-Funktion fehlt, daher auch keine Subjekt-Relation (nsubj) angezeigt. In slowenischen Nominalphrasen sind Begleiter (DET) nicht obligatorisch bzw. default (slow. privzeto) wie etwa im Englischen oder Deutschen. NOUN, VERB, ADP: Substantiv, Verb, Adposition (hier: Präposition) AUX: das Hilfs- oder Auxiliarverb DET: Determiner (Determinans), Begleiter eines Substantivs (meist handelt es sich um einen Artikel) xcomp: hier eine Relation zwischen zwei Verben, die gemeinsam das Prädikat des Satzes bilden (zael sanjati) obl: eine Art von Adjunkt, in der Valenzgrammatik gewöhnlich als Präpositionalobjekt klassifizert (hier ist o letenju das Objekt des Verbs sanjati) case: Element, das den Kasus einer Phrase regiert (z.B. die Präposition o regiert den Dativ der Nominalphrase letenju). x3 Aus den drei Diagrammen ist ersichtlich, dass die Subjekt-Relation (nsubj) im englischen und deutschen Satz mittels eines Personalpronomens (PRON) realisiert wird, während das Subjekt im slowenischen Satz mittels der finiten Verbform, einem Hilfs- oder Auxiliarverbs (AUX), (mit)ausgedrückt wird, also im Hilfsverb versteckt auftritt. Im slowenischen Satz ist PRON syntaktisch nicht notwendig, im englischen und deutschen schon. Das wirkt sich natürlich auf die Frequenzwerte bzw. den Pronzenanteil aus (s. Tabelle). Die Diagramme zeigen strukturelle Ähnlichkeiten und Unterschiede zwischen den Sprachversionen: sowohl im englischen Untertitel als auch in der slowenischen Version wird eine xcomp-Relation angegeben, d.h. dass das Satzprädikat mit Hilfe von zwei Verben konstituiert wird (started having vs. zael sanjati). Die Verben started bzw. zaeti modifizeren das Hauptverb have bzw. sanjati temporal. Im deutschen Untertitel wird stattdessen ein einfaches Prädikat (träumte) verwendet, dass durch eine Adverbialphrase (auf einmal) temporal modifiziert wird. das englische Substantiv dream wird im deutschen und slowenischen Untertitel im Satzprädikat ausgedrückt (träumte, sanjati) der englische Subordinationsmarker of, der sich sowohl auf Nominalphrasen als auch auf Sätze beziehen kann, wird im deutschen und slowenischen Untertitel mit einer spezifischeren Wortklasse ausgedrückt, nämlich mit einer Präposition (ADP, Adposition). Universal POS tags Universal features UDPipe 14.5.1 Aktiv und Passiv Wie groß ist der Anteil aktivischer und passivischer Sätze in den drei Sprachversionen? Dies können wir mit Hilfe der nsubj-Relation erfahren. In den englischen und deutschen Untertiteln wurden je 34 passivische Subjekte identifizert, in den slowenischen keiner. avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(language == &quot;eng&quot; | language == &quot;deu&quot; | language == &quot;slv&quot;) %&gt;% filter(str_detect(dep_rel, &quot;nsubj&quot;)) %&gt;% count(dep_rel, sort = TRUE) %&gt;% mutate(pct = round(100*n/sum(n),2)) %&gt;% pivot_wider(names_from = language, values_from = c(n, pct)) %&gt;% # mutate(across(everything(), ~ replace_na(.x, 0))) %&gt;% mutate_if(is.numeric, ~ replace_na(.x, 0)) %&gt;% mutate(dep_rel = str_replace(dep_rel, &quot;0&quot;, &quot;Unknown&quot;)) %&gt;% rmarkdown::paged_table() Schauen wir uns ein paar dieser Untertitel in allen drei Sprachen an: avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(language == &quot;eng&quot;) %&gt;% filter(str_detect(dep_rel, &quot;nsubj:pass&quot;)) %&gt;% ungroup() %&gt;% select(sentence, sentence_id) %&gt;% distinct() %&gt;% head(5) %&gt;% rmarkdown::paged_table() Wir wählen einen englischen Untertitel als Beispiel, und zwar: And the concept is that ervery driver is matched to his own avatar*. Deutsche Version: Die Idee ist, dass jeder Operator auf seinen eigenen Avatar abgestimmt wird. Slowenische Version: Vsak upravljavec dobi svojega avatarja. avatar %&gt;% filter(language == &quot;deu&quot;) %&gt;% filter(str_detect(text, &quot;jeder Operator auf&quot;) | str_detect(text, &quot;Avatar abgestimmt&quot;)) %&gt;% select(text) %&gt;% rmarkdown::paged_table() avatar %&gt;% filter(language == &quot;slv&quot;) %&gt;% filter(str_detect(text, &quot;dobi svojega avatarja&quot;)) %&gt;% select(text) %&gt;% rmarkdown::paged_table() Wiederum visualisieren wir die drei Sprachversionen. # English mytext = udpipe(&quot;And the concept is that ervery driver is matched to his own avatar&quot;, &quot;english&quot;) x1 = plot_annotation(mytext, size = 3) # German mytext = &quot;Die Idee ist, dass jeder Operator auf seinen eigenen Avatar abgestimmt wird&quot; %&gt;% enc2utf8() x = udpipe(mytext, &quot;german&quot;) x2 = plot_annotation(x, size = 3) # Slovenian mytext = &quot;Vsak upravljavec dobi svojega avatarja&quot; %&gt;% enc2utf8() x = udpipe(mytext, &quot;slovenian&quot;) x3 = plot_annotation(x, size = 3) x1 x2 x3 Die slowenische Version ist syntaktisch am einfachsten, denn sie besteht lediglich aus einem Hauptsatz, im englischen und deutschen Untertitel dagegen aus Haupt- und Nebensatz, wobei letztere die hauptsächliche Information trägt (die auch im slowenischen Hauptsatz zu Tage tritt). Der Hauptsatz im englischen und deutschen Untertitel kann kommunikativ betrachtet als Vorreiter oder Vorschaltung eingeordnet werden, also als Ausdruck, der vor allem zur Orientierung oder Einordnung eines Gedankens (der im Nebensatz ausgedrückt wird) in ein Gedankenschema oder Frame dient. Die passivische Relation, die im englischen und deutschen Untertitel mittels passivischer Verbformen realisiert wird, wird im slowenischen Untertitel mit dem Verb dobiti zum Ausdruck gebracht (deutsch: bekommen, englisch: get). Das Subjekt des slowenischen Verb dobiti (hier: vsak upravljalec) ist semantisch gesehen ein Benefaktiv oder Nutznießer (benefaktive Relation), also ein Rezipient, für den eine Handlung vorteilhaft oder nutzbringend ist. Entsprechendes gilt auch für das deutsche bekommen-Passiv (z.B. jeder Operator bekommt einen Avatar. Die Ausdrucksweise im slowenischen Untertitel ist im Vergleich zu den anderen Sprachversionen semantisch ungenau, denn es bleibt dem Leser überlassen, ob er die im Film realisierte symbiotische Verbindung zwischen Reiter und Tier nachvollziehen kann. Die Ausdrucksweise im englischen und deutschen Untertitel ist dagegen spezifischer, d.h. es handelt sich um eher eine technische (fachbezogene) Ausdrucksweise (engl. matching, deutsch Abstimmung). Da es sich in diesem Fall um einen Vorgang oder Prozess handelt, gibt es keinen menschlichen Verursacher der Abstimmung, denn sowohl der Operator (driver, upravljalec) sind so wie das gerittene Tier lediglich Reagentien im Prozess. Das ist in allen drei Sprachversionen deckungsgleich. In allen drei Sprachversionen wird wird der (menschliche) Benefaktiv (d.h. das syntaktische Subjekt) als Ausgangspunkt einer neuen oder wichtigen Information verwendet. Die neue Information seinen eigenen Avatar wird ins Rampenlicht gerückt, also zum Rhema des Satzes gemacht. Die typische Verteilung Thema vor Rhema wird hiermit in allen drei Sprachversionen gewahrt. Außerdem wird damit auch die häufigere Reihenfolge Subjekt vor Objekt eingehalten. Im slowenischen Satz handelt es sich um ein direktes Objekt (Akkusativobjekt), im englischen und deutschen dagegen um ein Präpositionalobjekt (match to , abstimmen auf ). 14.5.2 Substantive und Pronomen als Satzglieder Nun lenken wir unsere Sichtweise auf die Wortklassen Substantiv (NOUN) und Pronomen (PRON) in Subjekt- oder Objekt-Funktion. avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(upos == &quot;NOUN&quot; | upos == &quot;PRON&quot;) %&gt;% filter(str_detect(dep_rel, &quot;nsubj|obj|obl&quot;)) %&gt;% count(upos, dep_rel) %&gt;% mutate(pct = round(100*n/sum(n),2)) %&gt;% pivot_wider(names_from = language, values_from = c(n, pct)) %&gt;% # mutate(across(everything(), ~ replace_na(.x, 0))) %&gt;% mutate_if(is.numeric, ~ replace_na(.x, 0)) %&gt;% arrange(upos) %&gt;% rmarkdown::paged_table() Ein erster Blick auf die Tabelle zeigt uns mehrere Unterschiede: - Pronomen erscheinen in den englischen und deutschen Untertiteln häufiger in Subjekt-Funktion als Substantive. In den slowenischen Untertiteln ist es umgekehrt, was wahrscheinlich damit zusammenhängt, dass Slowenisch eine Pro-Drop-Sprache ist (s.o.). - In Objekt-Funktion scheint das Verhältnis zwischen den beiden Wortklassen (NOUN, PRON) ausgewogener zu sein. - Das indirekte Objekt (iobj), dass sich, semantisch betrachtet, oft auf einen Adressaten oder Rezipienten bezieht, wird vorzugsweise mit einem Pronomen ausgedrückt, selten oder gar nicht mit einem Substantiv. - Für einige Satzglied-Funktionen liegen keine Zahlen vor. Das kann daran liegen, dass diese Funktionen in der verwendeten Grammatik einer der ausgewählten Sprachen nicht unterschieden wird. Für genauere Feststellungen empfiehlt es sich, nur jeweils zwei Satzgliedfunktionen miteinander zu vergleichen. 14.5.3 Subjekt nominal / pronominal Wie viele Subjekte im Aktiv oder Passiv werden mit Hilfe von Substantiven oder Personalpronomen ausgedrückt? avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(language == &quot;eng&quot; | language == &quot;deu&quot; | language == &quot;slv&quot;) %&gt;% filter(upos == &quot;NOUN&quot; | upos == &quot;PRON&quot;) %&gt;% filter(str_detect(dep_rel, &quot;nsubj&quot;)) %&gt;% count(upos, dep_rel) %&gt;% mutate(pct = round(100*n/sum(n),2)) %&gt;% pivot_wider(names_from = language, values_from = c(n, pct)) %&gt;% # mutate(across(everything(), ~ replace_na(.x, 0))) %&gt;% mutate_if(is.numeric, ~ replace_na(.x, 0)) %&gt;% arrange(upos) %&gt;% rmarkdown::paged_table() Treten Pronomen genau so häufig als Passiv-Subjekte auf wie Substantive? Die obige Tabelle scheint für die englischen und deutschen Untertitel das Gegenteil zu zeigen. Machen für doch für diese beiden Sprachversionen je einen Chi-Quadrat-Test! pivot_by_nsubj &lt;- function(tbl) { tbl %&gt;% filter(upos == &quot;NOUN&quot; | upos == &quot;PRON&quot;) %&gt;% filter(str_detect(dep_rel, &quot;nsubj&quot;)) %&gt;% count(upos, dep_rel) %&gt;% group_by(upos) %&gt;% mutate(pct = round(100*n/sum(n),2)) %&gt;% pivot_wider(names_from = upos, values_from = c(n, pct)) %&gt;% mutate_if(is.numeric, ~ replace_na(.x, 0)) # mutate(across(everything(), ~ replace_na(.x, 0))) } x = avatar_words_udpiped %&gt;% filter(language == &quot;eng&quot;) %&gt;% pivot_by_nsubj() x %&gt;% rmarkdown::paged_table() In der letzten Tabelle fällt auf, dass der Anteil der Pronomen (pct_PRON) in der nsubj:pass-Funktion größer ist als der der Substantive (pct_NOUN). Ist dieser Unterschied zufällig (aufgrund der Stichprobenauswahl entstanden) oder können wir ihn auf die Grundgesamtheit (auf umgangssprachliche Dialoge in der englischen Sprache) verallgemeinern? Eine Antwort darauf soll uns der Chi-Quadrat-Test geben. Für den Chi-Quadrat-Test benötigen wir lediglich die beiden Spalten mit den absoluten Zahlenwerten (also die zweite und dritte). In Base-R kann man dies sehr ökonomisch mit einer Bedingung in eckigen Klammern [] erreichen, mit der tidyverse-Funktion select() zwar transparenter und in Tabellenform, dafür muss man jedoch etwas mehr schreiben. # Base-R chisq.test(x[,c(2:3)]) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: x[, c(2:3)] ## X-squared = 2.2472, df = 1, p-value = 0.1339 # tidyverse x %&gt;% select(n_NOUN, n_PRON) %&gt;% chisq.test() %&gt;% tidy() ## # A tibble: 1 x 4 ## statistic p.value parameter method ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 2.25 0.134 1 Pearson&#39;s Chi-squared test with Yates&#39; continuity~ Der Chi-Quadrat-Test bestätigt mit dem p-Wert (0,1339) die Null-Hypothese, d.h. das es zwischen Pronomen und Substantiven als Subjekte in den englischen Untertiteln keinen statistisch signifikanten Unterschied gibt und dass der prozentuelle Unterschied aufgrund unserer Stichprobenauswahl entstanden ist (also zufällig). Dasselbe können wir mit den deutschen Pronomen und Substantiven in Subjekt-Funktion machen. In diesem Fall ist der prozentuelle Unterschied zwischen Substantiven und Pronomen als aktivische oder passivische Subjekte größer zu sein als in der englischen Stichprobe. x = avatar_words_udpiped %&gt;% filter(language == &quot;deu&quot;) %&gt;% pivot_by_nsubj() x %&gt;% rmarkdown::paged_table() In den deutschen Untertiteln können wir mit dem Chi-Quadrat-Test einen statistisch signifikanten Unterschied zwischen Pronomen und Substantiven nachweisen (p &lt; 0,05): Pronomen scheinen in Subjekt-Funktion seltener in Passivsätzen verwendet worden zu sein als Substantive. Möglicherweise gilt dies auch für die Grundgesamtheit in deutsche Sprache (hier vor allem für umgangssprachliche Dialoge). Anders ausgedrückt: wir haben eine (nicht zufällige) Tendenz nachgewiesen, dass das Subjekt in den deutschen Passivsätzen häufiger mit Substantiven, also autosemantischen Ausdrücken realisiert wurde. # Base-R chisq.test(x[,c(2:3)]) %&gt;% tidy() ## # A tibble: 1 x 4 ## statistic p.value parameter method ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 14.4 0.000149 1 Pearson&#39;s Chi-squared test with Yates&#39; continuit~ Wir können uns die entsprechenden Belege mit Passivsubjekten auch anschauen. Aus der Tabelle ist auch ersichtlich, dass die grammatische Analyse des Programms auch einige Fehler enthält (z.B. Avatar in (auf) seinen eigenen Avatar abgestimmt wird ist Bestandteil eines Präpositionalobjekts und kein Passivsubjekt) oder eine andere Klassifizierung sinnvoller wäre (z.B. Was ist hier passiert? - was wurde als Passivsubjekt des Verbs passieren eingeordnet). Im Untertitel wenn du nicht gekommen wärst scheint das Programm den sein-Passiv übergeneralisierend auf Sätze mit einem Verb im Perfekt anzuwenden. Einige Fehler kommen aufgrund unvollständiger Sätze als Input vor, andere wiederum aufgrund der Tatsache, dass bestimmte sprachliche Formen mehrere Funktionen in der Sprache erfüllen können. avatar_words_udpiped %&gt;% select(sentence, upos, dep_rel, language) %&gt;% filter(language == &quot;deu&quot;) %&gt;% filter(upos == &quot;NOUN&quot; | upos == &quot;PRON&quot;) %&gt;% filter(str_detect(dep_rel, &quot;nsubj:pass&quot;)) %&gt;% rmarkdown::paged_table() 14.5.4 Objekt nominal / pronominal Wird das direkte Objekt (obj), im Deutschen und Slowenischen auch Akkusativojekt genannt, in den Untertiteln häufiger nominal oder pronominal ausgedrückt? x = avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(upos == &quot;NOUN&quot; | upos == &quot;PRON&quot;) %&gt;% filter(dep_rel == &quot;obj&quot;) %&gt;% count(upos, dep_rel) %&gt;% mutate(pct = round(100*n/sum(n),2)) %&gt;% pivot_wider(names_from = language, values_from = c(n, pct)) %&gt;% # mutate(across(everything(), ~ replace_na(.x, 0))) %&gt;% mutate_if(is.numeric, ~ replace_na(.x, 0)) %&gt;% arrange(upos) x %&gt;% rmarkdown::paged_table() Die Tabelle deutet daraufhin, dass in den englischen Untertiteln kein wesentlicher Unterschied auftritt, in den deutschen und slowenischen Untertiteln dagegen schon. Das können wir wiederum testen. Der erste Chi-Quadrat-Test bestätigt für die englischen Untertitel die Null-Hypothese (bei p &gt; 0,05 also keinen Unterschied), für die deutschen und slowenischen Untertitel dagegen die alternative Hypothese (d.h. dass es bei p &lt; 0,05 einen Wahrscheinlichkeitsunterschied gibt, ob das Objekt nominal oder pronominal ausgedrückt ist). englisch = chisq.test(x[,4]) %&gt;% tidy() %&gt;% mutate(language = &quot;eng&quot;) deutsch = chisq.test(x[,3]) %&gt;% tidy() %&gt;% mutate(language = &quot;deu&quot;) slowenisch = chisq.test(x[,5]) %&gt;% tidy() %&gt;% mutate(language = &quot;slv&quot;) chisqtabelle = bind_rows(englisch, deutsch, slowenisch) %&gt;% select(language, statistic, p.value) chisqtabelle %&gt;% rmarkdown::paged_table() 14.5.5 Wortklasse von Subjekt / Objekt Welche sprachübergreifende (cross-linguistic) Tendenz werden mit den englisch- und deutschsprachigen Daten bestätigt? avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(upos == &quot;NOUN&quot; | upos == &quot;PRON&quot;) %&gt;% filter(dep_rel == &quot;nsubj&quot; | dep_rel == &quot;obj&quot;) %&gt;% count(upos, dep_rel) %&gt;% # group_by(upos) %&gt;% mutate(pct = round(100*n/sum(n),2)) %&gt;% pivot_wider(names_from = language, values_from = c(n, pct)) %&gt;% # mutate(across(everything(), ~ replace_na(.x, 0))) %&gt;% mutate_if(is.numeric, ~ replace_na(.x, 0)) %&gt;% arrange(upos) %&gt;% rmarkdown::paged_table() Englisch: pivot_by_obj &lt;- function(tbl) { tbl %&gt;% filter(upos == &quot;NOUN&quot; | upos == &quot;PRON&quot;) %&gt;% filter(dep_rel == &quot;nsubj&quot; | dep_rel == &quot;obj&quot;) %&gt;% count(upos, dep_rel) %&gt;% group_by(upos) %&gt;% mutate(pct = round(100*n/sum(n),2)) %&gt;% pivot_wider(names_from = upos, values_from = c(n, pct)) %&gt;% mutate_if(is.numeric, ~ replace_na(.x, 0)) # mutate(across(everything(), ~ replace_na(.x, 0))) } x = avatar_words_udpiped %&gt;% filter(language == &quot;eng&quot;) %&gt;% pivot_by_obj() x %&gt;% rmarkdown::paged_table() Deutsch: x = avatar_words_udpiped %&gt;% filter(language == &quot;deu&quot;) %&gt;% pivot_by_obj() x %&gt;% rmarkdown::paged_table() Sowohl in den englischen als auch deutschen Untertiteln wird das Subjekt häufiger pronominal und seltener nominal ausgedrückt. Beim direkten Objekt ist es umgekehrt, denn dieses wird häufiger nominal ausgedrückt. Das man darauf zurückgeführen, dass das Subjekt häufig definit ist (etwas vom Hörer / Leser Identifizierbares ausdrückt), das Objekt dagegen etwas (noch) nicht Identifziertes. Dieses Verhältnis können wir in den slowenischen Untertiteln nicht nachweisen, was wir wiederum auf die Tatsache zurückführen können, dass Slowenisch eine Pro-Drop-Sprachen ist und daher die Anzahl pronominalisierter Subjekte wesentlich geringer sein muss, da ein pronominalisiertes Subjekt nur bei Hervorhebung obligatorisch im Satz vorkommt. x = avatar_words_udpiped %&gt;% filter(language == &quot;slv&quot;) %&gt;% pivot_by_obj() x %&gt;% rmarkdown::paged_table() 14.5.6 Gattungs- und Eigennamen Treten Gattungsnamen (Appelativa) und Eigennamen häufiger in Subjekt- oder in Objekt-Funktion auf? Englisch: pivot_by_propn &lt;- function(tbl) { tbl %&gt;% filter(upos %in% c(&quot;NOUN&quot;, &quot;PROPN&quot;)) %&gt;% filter(dep_rel == &quot;nsubj&quot; | dep_rel == &quot;obj&quot;) %&gt;% count(upos, dep_rel) %&gt;% group_by(upos) %&gt;% mutate(pct = round(100*n/sum(n),2)) %&gt;% pivot_wider(names_from = upos, values_from = c(n, pct)) %&gt;% mutate_if(is.numeric, ~ replace_na(.x, 0)) # mutate(across(everything(), ~ replace_na(.x, 0))) } x = avatar_words_udpiped %&gt;% filter(language == &quot;eng&quot;) %&gt;% pivot_by_propn() x %&gt;% rmarkdown::paged_table() Deutsch: x = avatar_words_udpiped %&gt;% filter(language == &quot;deu&quot;) %&gt;% pivot_by_propn() x %&gt;% rmarkdown::paged_table() Slowenisch: x = avatar_words_udpiped %&gt;% filter(language == &quot;slv&quot;) %&gt;% pivot_by_propn() x %&gt;% rmarkdown::paged_table() In allen drei Sprachversionen treten die Eigennamen (PROPN) häufiger in Subjekt-Funktion als in Objekt-Funktion auf, was man wieder darauf zurückführen kann, dass Eigennamen eine definite (d.h. identifizierbare) Einheit bezeichnen. 14.5.7 Konstituentenfolge In SOV-Sprachen ist die Reihenfolge Modifizierer vor Kopf häufiger zu beobachten, in SVO-Sprachen dagegen Modifizierer nach Kopf. Das prüfen wir in unserem bisherigen Sprachmaterial. Als Modifizierer wählen wird attributive Adjektive, als Kopf von Nominalphrasen die davor oder dahinter stehenden Substantive oder andere Wortklassen (z.B. Pronomen). Englisch und Slowenisch werden gewöhnlich als SVO-Sprachen eingeordnet, Deutsch mit der Konstituentenfolge SVO in Hauptsätzen und SOV in Nebensätzen wird im Atlas Wals (https://wals.info) dagegen als Sprache ohne dominante Konstituentenfolge bezeichnet. Was zeigt nun unser Sprachmaterial? avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(dep_rel == &quot;amod&quot; &amp; upos == &quot;ADJ&quot;) %&gt;% mutate(word_order = ifelse(token_id &lt; head_token_id, &quot;adj before head&quot;, &quot;adj after head&quot;)) %&gt;% count(dep_rel, word_order) %&gt;% mutate(pct = round(100*n/sum(n),2)) %&gt;% pivot_wider(names_from = language, values_from = c(n, pct)) %&gt;% # mutate(across(everything(), ~ replace_na(.x, 0))) %&gt;% mutate_if(is.numeric, ~ replace_na(.x, 0)) %&gt;% mutate(dep_rel = str_replace(dep_rel, &quot;0&quot;, &quot;Unknown&quot;)) %&gt;% select(-dep_rel) %&gt;% rmarkdown::paged_table() Im Englischen, Deutschen und Slowenischen und Türkischen dominiert die Reihenfolge Adjektiv vor Substantiv, also Modifizierer vor Kopf. Im Französischen und Italienischen dagegen ist die umgekehrte Reihenfolge dominant bzw. sind sowohl vor- als auch nachgestellte adjektivische Attribute fast gleich gut vertreten. Schauen wir uns noch an, in welchen Untertiteln unser Programm nachgestellte Adjektive identifiziert hat! avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(dep_rel == &quot;amod&quot; &amp; upos == &quot;ADJ&quot;) %&gt;% mutate(word_order = ifelse(token_id &lt; head_token_id, &quot;adj before head&quot;, &quot;adj after head&quot;)) %&gt;% filter(word_order == &quot;adj after head&quot;) %&gt;% ungroup() %&gt;% select(sentence, token, word_order, -language) ## # A tibble: 276 x 3 ## sentence token word_order ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 You could do something important. impo~ adj after~ ## 2 There&#39;s nothing like an old-school safety brief to put your~ brief adj after~ ## 3 I&#39;m a man short. short adj after~ ## 4 However, it does present an opportunity both timely and uni~ time~ adj after~ ## 5 to the root of the tree next to it. next adj after~ ## 6 Don&#39;t do anything unusually stupid. stup~ adj after~ ## 7 There is something really interesting going on in there bio~ inte~ adj after~ ## 8 - Stay down, sir. sir adj after~ ## 9 I&#39;m talking about something real, real adj after~ ## 10 something measurable in the biology of the forest. meas~ adj after~ ## # ... with 266 more rows Im slowenischen Untertitel Gre za nekaj resninega wird das Adjektiv resninega als nachgestelltes Attribut zum Kopf nekaj, einem Pronomen, eingeordnet. Entsprechend im englischen Untertitel: Something measurable. Aber einige der Belege entpuppen sich als Fehlmeldung. 14.5.8 Kontexte von Modifizierern avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(dep_rel == &quot;amod&quot; &amp; upos == &quot;ADJ&quot;) %&gt;% mutate(word_order = ifelse(token_id &lt; head_token_id, &quot;adj before head&quot;, &quot;adj after head&quot;)) %&gt;% filter(lemma == &quot;big&quot; | lemma == &quot;groß&quot; | lemma == &quot;velik&quot;) %&gt;% ungroup() %&gt;% select(word_order, lemma, sentence) ## # A tibble: 30 x 3 ## word_order lemma sentence ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 adj before head big with a big hole blown through the middle of my life, ## 2 adj before head big I know it&#39;s a big inconvenience for everyone. ## 3 adj before head big Try and use big words. ## 4 adj before head big I might just give you a big wet kiss. ## 5 adj before head big That is one big damn tree. ## 6 adj before head big You big baby. ## 7 adj before head big Charlie 2-1, got big movement. ## 8 adj before head big Bravo 1-1, I got a big seating. ## 9 adj before head groß Möge die Große Mutter ## 10 adj before head groß dem größten Unobtanium-Vorkommen ## # ... with 20 more rows avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(dep_rel == &quot;amod&quot; &amp; upos == &quot;ADJ&quot;) %&gt;% mutate(word_order = ifelse(token_id &lt; head_token_id, &quot;adj before head&quot;, &quot;adj after head&quot;)) %&gt;% filter(lemma == &quot;big&quot; | lemma == &quot;groß&quot; | lemma == &quot;velik&quot;) %&gt;% count(lemma) ## # A tibble: 3 x 3 ## # Groups: language [3] ## language lemma n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 deu groß 11 ## 2 eng big 8 ## 3 slv velik 11 14.5.9 Greenbergs Universalie 25 If the pronominal object follows the verb, so does the nominal object. Zuerst wollen wir direkte Objekte (im Deutschen und Slowenischen auch als Akkusativobjekte bezeichnet) ausfinding machen, die sowohl Gattungsnamen als auch Eigennamen enthalten und die dem Kopf der Verbalphrase vorangehen (OV) oder folgen (VO). pivot_by_verb_obj = function(tbl){ tbl %&gt;% mutate(word_order = ifelse(token_id &gt; head_token_id, &quot;VO&quot;, &quot;OV&quot;)) %&gt;% count(dep_rel, word_order) %&gt;% mutate(pct = round(100*n/sum(n),2)) %&gt;% pivot_wider(names_from = language, values_from = c(n, pct)) %&gt;% # mutate(across(everything(), ~ replace_na(.x, 0))) %&gt;% mutate_if(is.numeric, ~ replace_na(.x, 0)) %&gt;% mutate(dep_rel = str_replace(dep_rel, &quot;0&quot;, &quot;Unknown&quot;)) %&gt;% select(-dep_rel) } vo_nominal = avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(dep_rel == &quot;obj&quot; &amp; upos %in% c(&quot;NOUN&quot;, &quot;PROPN&quot;)) %&gt;% pivot_by_verb_obj() %&gt;% mutate(word_class = &quot;NOUN&quot;) Nun machen wir dasselbe mit den pronominalen Objekten. vo_pronominal = avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(dep_rel == &quot;obj&quot; &amp; upos %in% c(&quot;PRON&quot;)) %&gt;% pivot_by_verb_obj() %&gt;% mutate(word_class = &quot;PRON&quot;) Dann fügen wir beide Tabellen zusammen. verb_object = bind_rows(vo_nominal, vo_pronominal) %&gt;% select(word_class, word_order:pct_tur) verb_object %&gt;% rmarkdown::paged_table() Unsere erste Abbildung der Verhältnisse in den sechs Sprachen. verb_object %&gt;% select(word_class, word_order, pct_deu:pct_tur) %&gt;% pivot_longer(pct_deu:pct_tur, names_to = &quot;language&quot;, values_to = &quot;pct&quot;) %&gt;% mutate(language = str_replace(language, &quot;pct_&quot;, &quot;&quot;)) %&gt;% ggplot(aes(pct, word_class, fill = word_order)) + geom_col() + facet_wrap(~ language) + theme(legend.position = &quot;top&quot;) ggsave(&quot;pictures/barplot_avatar_VO-OV_NN_PRON.png&quot;) Die nächste Abbildung zeigt die Entfernung bzw. Ähnlichkeit der Sprachen ähnlich wie auf einer Landkarte an. verb_object %&gt;% select(word_class, word_order, n_deu:n_tur) %&gt;% pivot_longer(n_deu:n_tur, names_to = &quot;language&quot;, values_to = &quot;n&quot;) %&gt;% mutate(language = str_replace(language, &quot;n_&quot;, &quot;&quot;)) %&gt;% pivot_wider(names_from = c(word_order, word_class), values_from = n) %&gt;% group_by(language) %&gt;% summarise(ov_nouns_prop = OV_NOUN/(VO_NOUN + OV_NOUN), ov_prons_prop = OV_PRON/(VO_PRON + OV_PRON)) %&gt;% ggplot(aes(ov_nouns_prop, ov_prons_prop, fill = language, group = language)) + # geom_text(aes(label = language, check.overlap = TRUE)) + geom_label(aes(label = language, check.overlap = TRUE)) + theme(legend.position = &quot;none&quot;) + labs(x = &quot;OV-Anteil bei Substantiven&quot;, y = &quot;OV-Anteil bei Pronomen&quot;) ggsave(&quot;pictures/koord_avatar_VO-OV_NN_PRON.png&quot;) 14.5.10 Subjekt-vor-Objekt-Präferenz 14.5.10.1 SO-/OS-Anteil Den Anteil der Subjekte und Objekte, nominal und pronominal ausgedrückt, kann man mit der folgenden Funktion berechnen. pivot_by_obj &lt;- function(tbl) { tbl %&gt;% filter(upos == &quot;NOUN&quot; | upos == &quot;PRON&quot;) %&gt;% filter(dep_rel == &quot;nsubj&quot; | dep_rel == &quot;obj&quot;) %&gt;% count(upos, dep_rel) %&gt;% group_by(upos) %&gt;% mutate(pct = round(100*n/sum(n),2)) %&gt;% pivot_wider(names_from = upos, values_from = c(n, pct)) %&gt;% mutate_if(is.numeric, ~ replace_na(.x, 0)) # mutate(across(everything(), ~ replace_na(.x, 0))) } x = avatar_words_udpiped %&gt;% filter(language == &quot;eng&quot;) %&gt;% pivot_by_obj() Die nächste Funktion prüft, in welcher Reihenfolge Subjekt, Objekt und Verb auftreten. Unterschieden werden sechs verschiedene Konstituentenfolgen: - SVO, SOV, VSO, OVS, OSV und VOS. Eine eingebaute Filterfunktion vereinfacht die Klassifizierung auf drei Klassen: SO (subjektinitialer Satz), OS (objektinitialer Satz) und other, da ähnliche graphische Darstellungen entstehen sollen wie oben im Zusammenhang mit der Verb-Objekt-Abfolge. pivot_by_subj_verb_obj &lt;- function(tbl) { tbl %&gt;% filter(dep_rel %in% c(&quot;nsubj&quot;, &quot;obj&quot;)) %&gt;% mutate(token_id_nsubj = ifelse(dep_rel %in% c(&quot;nsubj&quot;), token_id, NA), token_id_obj = ifelse(dep_rel %in% c(&quot;obj&quot;), token_id, NA)) %&gt;% pivot_wider(c(doc_id:sentence, language, head_token_id), names_from = dep_rel, values_from = c(token_id), values_fn = mean) %&gt;% mutate(word_order = case_when( nsubj &lt; obj &amp; nsubj &lt; head_token_id &amp; obj &gt; head_token_id ~ &quot;SVO&quot;, nsubj &lt; obj &amp; nsubj &lt; head_token_id &amp; obj &lt; head_token_id ~ &quot;SOV&quot;, nsubj &lt; obj &amp; nsubj &gt; head_token_id &amp; obj &gt; head_token_id ~ &quot;VSO&quot;, nsubj &gt; obj &amp; nsubj &gt; head_token_id &amp; obj &lt; head_token_id ~ &quot;OVS&quot;, nsubj &gt; obj &amp; nsubj &lt; head_token_id &amp; obj &lt; head_token_id ~ &quot;OSV&quot;, nsubj &lt; obj &amp; nsubj &gt; head_token_id &amp; obj &gt; head_token_id ~ &quot;VOS&quot;, TRUE ~ &quot;other&quot; )) %&gt;% # optional removal of &quot;V&quot; and other mutate(word_order = str_remove(word_order, &quot;V&quot;)) %&gt;% filter(word_order != &quot;other&quot;) %&gt;% group_by(language) %&gt;% # no dep_rel anymore # count(dep_rel, word_order) %&gt;% count(word_order) %&gt;% mutate(pct = round(100*n/sum(n),2)) %&gt;% pivot_wider(names_from = language, values_from = c(n, pct)) %&gt;% mutate_if(is.numeric, ~ replace_na(.x, 0)) # mutate(across(everything(), ~ replace_na(.x, 0))) #%&gt;% # mutate(dep_rel = # str_replace(dep_rel, &quot;0&quot;, &quot;Unknown&quot;)) %&gt;% # select(-dep_rel) } Berechnung der Anteile der beiden interessierenden Konstituentenfolgen (SO und OS) bei nominaler und pronominaler Ausdrucksweise: svo_nominal = avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(dep_rel == &quot;nsubj&quot; | dep_rel == &quot;obj&quot;) %&gt;% filter(upos %in% c(&quot;NOUN&quot;, &quot;PROPN&quot;)) %&gt;% pivot_by_subj_verb_obj() %&gt;% mutate(word_class = &quot;NOUN&quot;) svo_pronominal = avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(dep_rel == &quot;nsubj&quot; | dep_rel == &quot;obj&quot;) %&gt;% filter(upos %in% c(&quot;PRON&quot;)) %&gt;% pivot_by_subj_verb_obj() %&gt;% mutate(word_class = &quot;PRON&quot;) so_nom_pron = avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(dep_rel == &quot;nsubj&quot; | dep_rel == &quot;obj&quot;) %&gt;% filter(upos %in% c(&quot;NOUN&quot;, &quot;PROPN&quot;, &quot;PRON&quot;)) %&gt;% pivot_by_subj_verb_obj() %&gt;% mutate(word_class = &quot;NOUN&quot;) Beide Tabellen werden zu einer vereint. subject_object = bind_rows(svo_nominal, svo_pronominal) %&gt;% select(word_class, word_order:pct_tur) subject_object %&gt;% rmarkdown::paged_table() In der ersten Abbildung verwenden wir liegende Säulendiagramme. Die relativ geringen Unterschiede zwischen den Sprachen waren zu erwarten, da nur SO-Sprachen ins Sample aufgenommen wurden. subject_object %&gt;% filter(word_order == &quot;SO&quot; | word_order == &quot;OS&quot;) %&gt;% select(word_class, word_order, pct_deu:pct_tur) %&gt;% pivot_longer(pct_deu:pct_tur, names_to = &quot;language&quot;, values_to = &quot;pct&quot;) %&gt;% mutate(language = str_replace(language, &quot;pct_&quot;, &quot;&quot;)) %&gt;% ggplot(aes(pct, word_class, fill = word_order)) + geom_col(position = &quot;fill&quot;) + scale_x_continuous(labels = percent_format(accuracy = 1)) + labs(x = &quot;&quot;) + facet_wrap(~ language) + theme(legend.position = &quot;top&quot;) ggsave(&quot;pictures/barplot_avatar_SO-OS_NN_PRON.png&quot;) Im zweiten Vergleich wählen wir wiederum eine landkartenähnliche Darstellungsweise. Da es sich um sechs Sprachen mit Subjekt-zuerst-Präferenz handelt, sind die Unterschiede relativ klein auf der Landkarte (besser zu sehen, wenn der Nullpunkt des Koordinaatensystems in die graphische Darstellung einbezogen ist - expand_limits()). subject_object %&gt;% select(word_class, word_order, n_deu:n_tur) %&gt;% filter(word_order == &quot;SO&quot; | word_order == &quot;OS&quot;) %&gt;% pivot_longer(n_deu:n_tur, names_to = &quot;language&quot;, values_to = &quot;n&quot;) %&gt;% mutate(language = str_replace(language, &quot;n_&quot;, &quot;&quot;)) %&gt;% pivot_wider(names_from = c(word_order, word_class), values_from = n) %&gt;% group_by(language) %&gt;% summarise(so_nouns_prop = SO_NOUN/(SO_NOUN + OS_NOUN), so_prons_prop = SO_PRON/(SO_PRON + OS_PRON)) %&gt;% ggplot(aes(so_nouns_prop, so_prons_prop, fill = language, group = language)) + # geom_text(aes(label = language, check.overlap = TRUE)) + geom_label(aes(label = language, check.overlap = TRUE)) + theme(legend.position = &quot;none&quot;) + # expand_limits(x = 0, y = 0) + scale_x_continuous(labels = percent_format(accuracy = 1)) + scale_y_continuous(labels = percent_format(accuracy = 1)) + labs(x = &quot;SO-Anteil bei Substantiven&quot;, y = &quot;SO-Anteil bei Pronomen&quot;) ggsave(&quot;pictures/koord_avatar_SO-SO_NN_PRON.png&quot;) 14.5.10.2 Belege für OS-Abfolgen Wir wandeln die oben stehende Funktion pivot_by_subj_verb_obj() ein wenig ab, um zu erfahren, in welchen Sätzen OS-Abfolgen gemäß der automatischen Klassifizierung erscheinen. pivot_by_subj_verb_obj_which &lt;- function(tbl){ tbl %&gt;% filter(dep_rel %in% c(&quot;nsubj&quot;, &quot;obj&quot;)) %&gt;% mutate(token_id_nsubj = ifelse(dep_rel %in% c(&quot;nsubj&quot;), token_id, NA), token_id_obj = ifelse(dep_rel %in% c(&quot;obj&quot;), token_id, NA)) %&gt;% pivot_wider(c(doc_id:sentence, language, head_token_id), names_from = dep_rel, values_from = c(token_id), values_fn = mean) %&gt;% mutate(word_order = case_when( nsubj &lt; obj &amp; nsubj &lt; head_token_id &amp; obj &gt; head_token_id ~ &quot;SVO&quot;, nsubj &lt; obj &amp; nsubj &lt; head_token_id &amp; obj &lt; head_token_id ~ &quot;SOV&quot;, nsubj &lt; obj &amp; nsubj &gt; head_token_id &amp; obj &gt; head_token_id ~ &quot;VSO&quot;, nsubj &gt; obj &amp; nsubj &gt; head_token_id &amp; obj &lt; head_token_id ~ &quot;OVS&quot;, nsubj &gt; obj &amp; nsubj &lt; head_token_id &amp; obj &lt; head_token_id ~ &quot;OSV&quot;, nsubj &lt; obj &amp; nsubj &gt; head_token_id &amp; obj &gt; head_token_id ~ &quot;VOS&quot;, TRUE ~ &quot;other&quot; )) %&gt;% # optional removal of &quot;V&quot; and other mutate(word_order = str_remove(word_order, &quot;V&quot;)) %&gt;% filter(word_order != &quot;other&quot;) %&gt;% group_by(language) } Deutsch: OS-Abfolge mit Nomen oder Pronomen Eine Reihe von Fehlanzeigen! Sätze mit zwei NPs im Nominativ (z.B. Die Starken jagen die Schwachen) sind morphosyntaktisch nicht eindeutig bestimmbar. Menschen würden in den meisten Fällen die NP die Starken als Subjekt bestimmen, weil es zuerst genannt wird und weil unsere Erfahrungen das für wahrscheinlicher halten. Sätze wie wenn ihr die Maske verliert, d.h. mit angesprochenem Subjekt (ihr), scheinen auch für Probleme zu sorgen. avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(dep_rel == &quot;nsubj&quot; | dep_rel == &quot;obj&quot;) %&gt;% # filter(upos %in% c(&quot;NOUN&quot;, &quot;PROPN&quot;)) %&gt;% pivot_by_subj_verb_obj_which() %&gt;% filter(language == &quot;deu&quot;) %&gt;% ungroup() %&gt;% select(sentence, obj, nsubj, word_order, -language) %&gt;% filter(word_order == &quot;OS&quot;) ## # A tibble: 65 x 4 ## sentence obj nsubj word_order ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Die Starken jagen die Schwachen. 2 5 OS ## 2 Die Starken jagen die Schwachen. 2 5 OS ## 3 Tommy war der Wissenschaftler, nicht ich. 1 4 OS ## 4 Denkt dran, wenn ihr die Maske verliert, 5 7 OS ## 5 So was wie einen Ex-Marine gibt es nicht. 2 7 OS ## 6 aber die Einstellung verliert man nie. 3 5 OS ## 7 Oh, Mann, das gibt&#39;s doch gar nicht! 1 7 OS ## 8 Was wir sehen, was wir fühlen. 1 2 OS ## 9 Was wir sehen, was wir fühlen. 5 6 OS ## 10 Sie mag Pflanzen nämlich lieber 1 3 OS ## # ... with 55 more rows Slowenisch: OS-Abfolge mit Nomen oder Pronomen Geringerer Fehleranteil als in den deutschen Belegen. Das Gentivobjekt naih navad wurde als Subjekt klassifiziert, das Subjekt moja hi übersehen. Das negierte Subjekt nihe wurde nicht erkannt. avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(dep_rel == &quot;nsubj&quot; | dep_rel == &quot;obj&quot;) %&gt;% # filter(upos %in% c(&quot;NOUN&quot;, &quot;PROPN&quot;)) %&gt;% pivot_by_subj_verb_obj_which() %&gt;% filter(language == &quot;slv&quot;) %&gt;% ungroup() %&gt;% select(sentence, obj, nsubj, word_order, -language) %&gt;% filter(word_order == &quot;OS&quot;) ## # A tibble: 15 x 4 ## sentence obj nsubj word_order ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Me slii, Jake? 1 4 OS ## 2 Ste me iskali, polkovnik? 2 5 OS ## 3 Kaj dela, Tsu&#39;tey? 1 4 OS ## 4 Moja hi te bo uila naih navad. 2 7 OS ## 5 ko ga je lovil razdraen thanator. 2 6 OS ## 6 ugotovi, kaj hoejo te modre opice. 3 7 OS ## 7 Kaj to pomeni? 1 2 OS ## 8 ki ga mora opraviti vsak mlad lovec. 2 7 OS ## 9 Nai mu reejo veliki leonopteriks. 2 5 OS ## 10 Tem drevesom reemo Utraya Mokri. 2 5 OS ## 11 Nocoj prvo rundo plaam jaz. 3 5 OS ## 12 Ve, kaj to pomeni. 3 4 OS ## 13 Nihe mu ni ni mogel. 2 2.5 OS ## 14 Recite jim, da jih klie Toruk Macto. 5 7 OS ## 15 Ko jih je Toruk Macto poklical, so prili. 2 4 OS Deutsch: OS-Abfolge mit Nomen Hier erweist sich, dass Udpipe die Satzglieder nicht einwandfrei identifizieren konnte. Fehler! avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(dep_rel == &quot;nsubj&quot; | dep_rel == &quot;obj&quot;) %&gt;% filter(upos %in% c(&quot;NOUN&quot;, &quot;PROPN&quot;)) %&gt;% pivot_by_subj_verb_obj_which() %&gt;% filter(language == &quot;deu&quot;) %&gt;% ungroup() %&gt;% select(sentence, obj, nsubj, word_order, -language) %&gt;% filter(word_order == &quot;OS&quot;) ## # A tibble: 3 x 4 ## sentence obj nsubj word_order ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Die Starken jagen die Schwachen. 2 5 OS ## 2 Die Starken jagen die Schwachen. 2 5 OS ## 3 Tommy war der Wissenschaftler, nicht ich. 1 4 OS Deutsch: OS-Abfolge mit Pronomen Besser. Aber einige Fehler in der Analyse. avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(dep_rel == &quot;nsubj&quot; | dep_rel == &quot;obj&quot;) %&gt;% filter(upos %in% c(&quot;PRON&quot;)) %&gt;% pivot_by_subj_verb_obj_which() %&gt;% filter(language == &quot;deu&quot;) %&gt;% ungroup() %&gt;% select(sentence, obj, nsubj, word_order, -language) %&gt;% filter(word_order == &quot;OS&quot;) ## # A tibble: 45 x 4 ## sentence obj nsubj word_order ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 So was wie einen Ex-Marine gibt es nicht. 2 7 OS ## 2 Was wir sehen, was wir fühlen. 1 2 OS ## 3 Was wir sehen, was wir fühlen. 5 6 OS ## 4 Das haben Sie nicht. 1 3 OS ## 5 Das Letzte, was ich da brauche, 4 5 OS ## 6 Was haben Sie sich dabei gedacht? 2.5 3 OS ## 7 Die betäuben Sie sonst. 1 3 OS ## 8 - Wen haben Sie erwartet, Sie Dumpfbacke? 2 4 OS ## 9 Mir fehlt jemand. 1 3 OS ## 10 Besorgen Sie mir, was ich brauche, 5 6 OS ## # ... with 35 more rows Slowenisch: OS-Abfolge mit Nomen Fehler in beiden Belegen! Im ersten Beleg: Moja hi ist das Subjekt, das klitisierte Pronomen te das Objekt. Fälschlicherweise wird das Genitivobjekt naih navad als Subjekt analysiert. Im zweiten Beleg ist das Subjekt nur (implizit) in der finiten Verbform reemo enthalten. Das Dativobjekt tem drevesom steht vor der finiten Verbform. Fälschlicherweise wird die Phrase Utraya Mokri (x nennt man so. Wie nennt man x ?) als Subjekt klassifiziert statt als Prädikativ oder Adverbialbestimmung. avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(dep_rel == &quot;nsubj&quot; | dep_rel == &quot;obj&quot;) %&gt;% filter(upos %in% c(&quot;NOUN&quot;, &quot;PROPN&quot;)) %&gt;% pivot_by_subj_verb_obj_which() %&gt;% filter(language == &quot;slv&quot;) %&gt;% ungroup() %&gt;% select(sentence, obj, nsubj, word_order, -language) %&gt;% filter(word_order == &quot;OS&quot;) ## # A tibble: 2 x 4 ## sentence obj nsubj word_order ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Moja hi te bo uila naih navad. 2 7 OS ## 2 Tem drevesom reemo Utraya Mokri. 2 5 OS Slowenisch: OS-Abfolge mit Pronomen Besser. Aber einige Fehler in der Analyse. avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(dep_rel == &quot;nsubj&quot; | dep_rel == &quot;obj&quot;) %&gt;% filter(upos %in% c(&quot;PRON&quot;)) %&gt;% pivot_by_subj_verb_obj_which() %&gt;% filter(language == &quot;slv&quot;) %&gt;% ungroup() %&gt;% select(sentence, obj, nsubj, word_order, -language) %&gt;% filter(word_order == &quot;OS&quot;) ## # A tibble: 1 x 4 ## sentence obj nsubj word_order ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Nihe mu ni ni mogel. 2 2.5 OS 14.5.11 Mehrere Konstituenten # avatar_words_udpiped %&gt;% filter(language == &quot;deu&quot;) pivot_by_constituents &lt;- function(tbl) { tbl %&gt;% filter(dep_rel %in% c(&quot;nsubj&quot;, &quot;obj&quot;, &quot;advmod&quot;, &quot;aux&quot;, &quot;cop&quot;, &quot;root&quot;)) %&gt;% mutate(token_id_nsubj = ifelse(dep_rel %in% c(&quot;nsubj&quot;), token_id, NA), token_id_obj = ifelse(dep_rel %in% c(&quot;obj&quot;), token_id, NA), token_id_advmod = ifelse(dep_rel %in% c(&quot;advmod&quot;), token_id, NA), token_id_cop = ifelse(dep_rel %in% c(&quot;cop&quot;), token_id, NA), token_id_root = ifelse(dep_rel %in% c(&quot;root&quot;), token_id, NA), token_id_aux = ifelse(dep_rel %in% c(&quot;aux&quot;), token_id, NA)) %&gt;% pivot_wider(c(doc_id:sentence, language, head_token_id), names_from = dep_rel, values_from = c(token_id), values_fn = mean) %&gt;% mutate(word_order = case_when( advmod &lt; aux &amp; aux &lt; nsubj &amp; nsubj &lt; obj &amp; nsubj &lt; head_token_id &amp; obj &gt; head_token_id ~ &quot;AaSVO&quot;, advmod &lt; aux &amp; aux &lt; nsubj &amp; nsubj &lt; obj &amp; nsubj &lt; head_token_id &amp; obj &lt; head_token_id ~ &quot;AaSOV&quot;, advmod &lt; nsubj &amp; nsubj &lt; obj &amp; nsubj &lt; head_token_id &amp; obj &gt; head_token_id ~ &quot;ASVO&quot;, advmod &lt; nsubj &amp; nsubj &lt; obj &amp; nsubj &lt; head_token_id &amp; obj &lt; head_token_id ~ &quot;ASOV&quot;, advmod &lt; head_token_id &amp; nsubj &lt; obj &amp; nsubj &gt; head_token_id &amp; obj &gt; head_token_id ~ &quot;AVSO&quot;, advmod &lt; obj &amp; nsubj &gt; obj &amp; nsubj &gt; head_token_id &amp; obj &lt; head_token_id ~ &quot;AOVS&quot;, advmod &lt; obj &amp; nsubj &gt; obj &amp; nsubj &lt; head_token_id &amp; obj &lt; head_token_id ~ &quot;AOSV&quot;, advmod &lt; head_token_id &amp; nsubj &lt; obj &amp; nsubj &gt; head_token_id &amp; obj &gt; head_token_id ~ &quot;AVOS&quot;, nsubj &lt; obj &amp; nsubj &lt; head_token_id &amp; obj &gt; head_token_id ~ &quot;SVO&quot;, nsubj &lt; obj &amp; nsubj &lt; head_token_id &amp; obj &lt; head_token_id ~ &quot;SOV&quot;, nsubj &lt; obj &amp; nsubj &gt; head_token_id &amp; obj &gt; head_token_id ~ &quot;VSO&quot;, nsubj &gt; obj &amp; nsubj &gt; head_token_id &amp; obj &lt; head_token_id ~ &quot;OVS&quot;, nsubj &gt; obj &amp; nsubj &lt; head_token_id &amp; obj &lt; head_token_id ~ &quot;OSV&quot;, nsubj &lt; obj &amp; nsubj &gt; head_token_id &amp; obj &gt; head_token_id ~ &quot;VOS&quot;, TRUE ~ &quot;other&quot; )) %&gt;% # optional removal of &quot;V&quot; and other # mutate(word_order = str_remove(word_order, &quot;V&quot;)) %&gt;% filter(word_order != &quot;other&quot;) %&gt;% group_by(language) %&gt;% # no dep_rel anymore # count(dep_rel, word_order) %&gt;% count(word_order) %&gt;% mutate(pct = round(100*n/sum(n),2)) %&gt;% pivot_wider(names_from = language, values_from = c(n, pct)) %&gt;% mutate_if(is.numeric, ~ replace_na(.x, 0)) # mutate(across(everything(), ~ replace_na(.x, 0))) #%&gt;% # mutate(dep_rel = # str_replace(dep_rel, &quot;0&quot;, &quot;Unknown&quot;)) %&gt;% # select(-dep_rel) } asvo_nom_pron &lt;- avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(dep_rel %in% c(&quot;nsubj&quot;, &quot;obj&quot;, &quot;advmod&quot;, &quot;aux&quot;, &quot;cop&quot;, &quot;root&quot;)) %&gt;% filter(upos %in% c(&quot;NOUN&quot;, &quot;PROPN&quot;, &quot;PRON&quot;, &quot;ADV&quot;, &quot;AUX&quot;)) %&gt;% pivot_by_constituents() %&gt;% mutate(word_class = &quot;differs&quot;) asvo_nom_pron ## # A tibble: 12 x 14 ## word_order n_deu n_eng n_fra n_ita n_slv n_tur pct_deu pct_eng pct_fra ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AaSOV 7 1 0 0 0 0 1.56 0.29 0 ## 2 ASOV 2 0 3 1 0 0 0.45 0 1.08 ## 3 ASVO 2 16 3 1 0 1 0.45 4.6 1.08 ## 4 AVSO 12 0 1 1 0 0 2.67 0 0.36 ## 5 OSV 34 22 14 2 2 14 7.57 6.32 5.04 ## 6 OVS 31 0 8 8 11 7 6.9 0 2.88 ## 7 SOV 124 1 75 17 11 29 27.6 0.29 27.0 ## 8 SVO 186 306 172 49 20 2 41.4 87.9 61.9 ## 9 VSO 51 0 0 0 0 0 11.4 0 0 ## 10 AaSVO 0 1 0 0 0 0 0 0.29 0 ## 11 AOSV 0 1 0 0 0 0 0 0.29 0 ## 12 AOVS 0 0 2 0 0 0 0 0 0.72 ## # ... with 4 more variables: pct_ita &lt;dbl&gt;, pct_slv &lt;dbl&gt;, pct_tur &lt;dbl&gt;, ## # word_class &lt;chr&gt; pivot_by_constituents_which &lt;- function(tbl){ tbl %&gt;% filter(dep_rel %in% c(&quot;nsubj&quot;, &quot;obj&quot;, &quot;advmod&quot;, &quot;aux&quot;, &quot;cop&quot;, &quot;root&quot;)) %&gt;% mutate(token_id_nsubj = ifelse(dep_rel %in% c(&quot;nsubj&quot;), token_id, NA), token_id_obj = ifelse(dep_rel %in% c(&quot;obj&quot;), token_id, NA), token_id_advmod = ifelse(dep_rel %in% c(&quot;advmod&quot;), token_id, NA), token_id_cop = ifelse(dep_rel %in% c(&quot;cop&quot;), token_id, NA), token_id_root = ifelse(dep_rel %in% c(&quot;root&quot;), token_id, NA), token_id_aux = ifelse(dep_rel %in% c(&quot;aux&quot;), token_id, NA)) %&gt;% pivot_wider(c(doc_id:sentence, language, head_token_id), names_from = dep_rel, values_from = c(token_id), values_fn = mean) %&gt;% mutate(word_order = case_when( advmod &lt; aux &amp; aux &lt; nsubj &amp; nsubj &lt; obj &amp; nsubj &lt; head_token_id &amp; obj &gt; head_token_id ~ &quot;AaSVO&quot;, advmod &lt; aux &amp; aux &lt; nsubj &amp; nsubj &lt; obj &amp; nsubj &lt; head_token_id &amp; obj &lt; head_token_id ~ &quot;AaSOV&quot;, advmod &lt; nsubj &amp; nsubj &lt; obj &amp; nsubj &lt; head_token_id &amp; obj &gt; head_token_id ~ &quot;ASVO&quot;, advmod &lt; nsubj &amp; nsubj &lt; obj &amp; nsubj &lt; head_token_id &amp; obj &lt; head_token_id ~ &quot;ASOV&quot;, advmod &lt; head_token_id &amp; nsubj &lt; obj &amp; nsubj &gt; head_token_id &amp; obj &gt; head_token_id ~ &quot;AVSO&quot;, advmod &lt; obj &amp; nsubj &gt; obj &amp; nsubj &gt; head_token_id &amp; obj &lt; head_token_id ~ &quot;AOVS&quot;, advmod &lt; obj &amp; nsubj &gt; obj &amp; nsubj &lt; head_token_id &amp; obj &lt; head_token_id ~ &quot;AOSV&quot;, advmod &lt; head_token_id &amp; nsubj &lt; obj &amp; nsubj &gt; head_token_id &amp; obj &gt; head_token_id ~ &quot;AVOS&quot;, nsubj &lt; obj &amp; nsubj &lt; head_token_id &amp; obj &gt; head_token_id ~ &quot;SVO&quot;, nsubj &lt; obj &amp; nsubj &lt; head_token_id &amp; obj &lt; head_token_id ~ &quot;SOV&quot;, nsubj &lt; obj &amp; nsubj &gt; head_token_id &amp; obj &gt; head_token_id ~ &quot;VSO&quot;, nsubj &gt; obj &amp; nsubj &gt; head_token_id &amp; obj &lt; head_token_id ~ &quot;OVS&quot;, nsubj &gt; obj &amp; nsubj &lt; head_token_id &amp; obj &lt; head_token_id ~ &quot;OSV&quot;, nsubj &lt; obj &amp; nsubj &gt; head_token_id &amp; obj &gt; head_token_id ~ &quot;VOS&quot;, TRUE ~ &quot;other&quot; )) %&gt;% # optional removal of &quot;V&quot; and other # mutate(word_order = str_remove(word_order, &quot;V&quot;)) %&gt;% filter(word_order != &quot;other&quot;) %&gt;% group_by(language) } avatar_words_udpiped %&gt;% group_by(language) %&gt;% filter(dep_rel %in% c(&quot;nsubj&quot;, &quot;obj&quot;, &quot;advmod&quot;, &quot;aux&quot;, &quot;cop&quot;, &quot;root&quot;)) %&gt;% filter(upos %in% c(&quot;PRON&quot;, &quot;NOUN&quot;, &quot;ADV&quot;, &quot;VERB&quot;, &quot;AUX&quot;)) %&gt;% pivot_by_constituents_which() %&gt;% filter(language == &quot;deu&quot;) %&gt;% ungroup() %&gt;% select(sentence, obj, nsubj, word_order, -language) %&gt;% filter(word_order %in% c(&quot;AaSOV&quot;, &quot;AaSVO&quot;, &quot;ASVO&quot;, &quot;ASOV&quot;, &quot;AVSO&quot;)) ## # A tibble: 23 x 4 ## sentence obj nsubj word_order ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Hier werden wir viel Zeit verbringen. 5 3 AaSOV ## 2 Hier verlinken wir uns mit den Avataren. 4 3 AVSO ## 3 und irgendwann vertrauen sie uns. 5 4 AVSO ## 4 Also nutzen Sie Ihre Möglichkeiten 5 3 AVSO ## 5 Wie fühlen Sie sich, Jake? 4 3 AVSO ## 6 aber Sie berichten mir. 4 2 ASVO ## 7 Vielleicht kann ich was von dir lernen. 4 3 AaSOV ## 8 Wo hast du unsere Sprache gelernt? 5 3 AaSOV ## 9 warum bringst du sie zu uns? 4 3 AVSO ## 10 aber es gab ein Zeichen von Eywa. 5 2 ASVO ## # ... with 13 more rows 14.5.11.1 Zusätzliche graphische Darstellungen Übersichtsgraphik mit Korrelationsberechnung: library(GGally) ggpairs(verb_object[,c(1:8)]) Korrelationsgraphik: library(correlation) library(see) results = summary(correlation(verb_object[,c(1:8)])) p = plot(results) p + scale_fill_material_c(palette = &quot;rainbow&quot;) + theme_abyss() library(ggpage) avatar_deu[1:100] %&gt;% ggpage_build() %&gt;% mutate(long_word = stringr::str_length(word) &gt; 8) %&gt;% ggpage_plot(aes(fill = long_word)) + labs(title = &quot;Longer words throughout the Avatar subtitles&quot;) + scale_fill_manual(values = c(&quot;grey70&quot;, &quot;blue&quot;), labels = c(&quot;8 or less&quot;, &quot;9 or more&quot;), name = &quot;Word length&quot;) library(ggwordcloud) library(tidytext) stoplist_eng = as_tibble(c(&quot;not&quot;,&quot;hey&quot;,&quot;yeah&quot;,&quot;to&quot;,&quot;go&quot;,&quot;get&quot;)) %&gt;% rename(word = value) x = avatar_words_udpiped %&gt;% filter(language == &quot;eng&quot;) %&gt;% anti_join(stop_words) %&gt;% # anti_join(stoplist_eng) %&gt;% # filter(!lemma %in% c(stoplist_eng)) %&gt;% filter(!str_detect(lemma, c(&quot;not&quot;,&quot;hey&quot;,&quot;yeah&quot;,&quot;to&quot;,&quot;go&quot;,&quot;get&quot;))) %&gt;% count(lemma, sort = TRUE) x %&gt;% ggplot(aes(label = lemma, size = n)) + geom_text_wordcloud() + scale_size_area(max_size = 40) + theme_minimal() Schnell eine Graphik mit dem Datensatz erstellen, ohne zu programmieren: References "],["konnektoren-1.html", "Kapitel 15 Konnektoren 15.1 Pakete 15.2 Text einlesen 15.3 UDPipe laden 15.4 Text annotieren 15.5 Wortklassen und Konnektoren", " Kapitel 15 Konnektoren 15.1 Pakete library(tidyverse) library(scales) library(janitor) library(readtext) library(quanteda) library(quanteda.textmodels) library(quanteda.textstats) library(quanteda.textplots) library(tidytext) library(readxl) library(writexl) library(udpipe) 15.2 Text einlesen txt = readtext(&quot;data/books/*.txt&quot;, encoding = &quot;UTF-8&quot;) txt ## readtext object consisting of 2 documents and 0 docvars. ## # Description: df [2 x 2] ## doc_id text ## &lt;chr&gt; &lt;chr&gt; ## 1 prozess.txt &quot;\\&quot;Der Prozes\\&quot;...&quot; ## 2 tom.txt &quot;\\&quot;Tom Sawyer\\&quot;...&quot; 15.3 UDPipe laden library(udpipe) destfile = &quot;german-gsd-ud-2.5-191206.udpipe&quot; if(!file.exists(destfile)){ sprachmodell &lt;- udpipe_download_model(language = &quot;german&quot;) udmodel_de &lt;- udpipe_load_model(sprachmodell$file_model) } else { file_model = destfile udmodel_de &lt;- udpipe_load_model(file_model) } 15.4 Text annotieren https://universaldependencies.org/ # Na zaetku je readtext prebral besedila, shranili smo jih v spremenljivki &quot;txt&quot;. x &lt;- udpipe_annotate(udmodel_de, x = txt$text, trace = TRUE) ## 2022-03-20 21:29:31 Annotating text fragment 1/2 ## 2022-03-20 21:31:30 Annotating text fragment 2/2 # # samo prvo besedilo: # x &lt;- udpipe_annotate(udmodel_de, x = txt$text[1], trace = TRUE) x &lt;- as.data.frame(x) # write_rds(x, &quot;data/prozess_tom_udpiped.rds&quot;) # x = read_rds(&quot;data/prozess_tom_udpiped.rds&quot;) 15.5 Wortklassen und Konnektoren tabela = x %&gt;% group_by(doc_id) %&gt;% count(upos) %&gt;% filter(!is.na(upos), upos != &quot;PUNCT&quot;) head(tabela) %&gt;% rmarkdown::paged_table() vezniki = tabela %&gt;% filter(upos %in% c(&quot;CCONJ&quot;, &quot;SCONJ&quot;)) %&gt;% mutate(prozent = n/sum(n)) %&gt;% pivot_wider(id_cols = upos, names_from = doc_id, values_from = n:prozent) head(vezniki) %&gt;% rmarkdown::paged_table() konnektoren = x %&gt;% mutate(token = str_to_lower(token)) %&gt;% group_by(doc_id, token) %&gt;% filter(!is.na(upos), upos != &quot;PUNCT&quot;) %&gt;% filter(upos %in% c(&quot;CCONJ&quot;, &quot;SCONJ&quot;)) %&gt;% count(upos, sort = T) connectors = x %&gt;% mutate(token = str_to_lower(token)) %&gt;% group_by(token) %&gt;% filter(!is.na(upos), upos != &quot;PUNCT&quot;) %&gt;% filter(upos %in% c(&quot;CCONJ&quot;, &quot;SCONJ&quot;)) %&gt;% count(upos, sort = T) connectors %&gt;% filter(upos == &quot;CCONJ&quot;) %&gt;% pull(token) %&gt;% head(50) ## [1] &quot;und&quot; &quot;aber&quot; &quot;oder&quot; ## [4] &quot;denn&quot; &quot;sondern&quot; &quot;wie&quot; ## [7] &quot;als&quot; &quot;weder&quot; &quot;doch&quot; ## [10] &quot;noch&quot; &quot;schrie&quot; &quot;kroch&quot; ## [13] &quot;desto&quot; &quot;um&quot; &quot;woher&quot; ## [16] &quot;du&quot; &quot;entweder&quot; &quot;hatte&quot; ## [19] &quot;sowie&quot; &quot;statt&quot; &quot;,aber&quot; ## [22] &quot;,und&quot; &quot;sowohl&quot; &quot;irgendwie&quot; ## [25] &quot;unnötigerweise&quot; &quot;,denn&quot; &quot;,nun&quot; ## [28] &quot;aschfahl&quot; &quot;aß&quot; &quot;ausnahmsweise&quot; ## [31] &quot;besinn&quot; &quot;brauch&quot; &quot;daß&quot; ## [34] &quot;dazu&quot; &quot;dennoch&quot; &quot;genau&quot; ## [37] &quot;hoch&quot; &quot;insbesondere&quot; &quot;kund&quot; ## [40] &quot;laß&quot; &quot;manch&quot; &quot;ob&quot; ## [43] &quot;sinn&quot; &quot;stahl&quot; &quot;such&quot; ## [46] &quot;unvorsichtigerweise&quot; &quot;verzeih&quot; &quot;wieder&quot; ## [49] &quot;wozu&quot; connectors %&gt;% filter(upos == &quot;SCONJ&quot;) %&gt;% pull(token) %&gt;% head(50) ## [1] &quot;daß&quot; &quot;wenn&quot; &quot;als&quot; &quot;wie&quot; &quot;da&quot; ## [6] &quot;denn&quot; &quot;während&quot; &quot;ob&quot; &quot;bis&quot; &quot;weil&quot; ## [11] &quot;obwohl&quot; &quot;indem&quot; &quot;laß&quot; &quot;nachdem&quot; &quot;damit&quot; ## [16] &quot;sobald&quot; &quot;ehe&quot; &quot;ohne&quot; &quot;solange&quot; &quot;soweit&quot; ## [21] &quot;bevor&quot; &quot;das&quot; &quot;aber&quot; &quot;seit&quot; &quot;seitdem&quot; ## [26] &quot;strich&quot; &quot;gleich&quot; &quot;begann&quot; &quot;falls&quot; &quot;vergaß&quot; ## [31] &quot;worum&quot; &quot;halb&quot; &quot;hätt&quot; &quot;maß&quot; &quot;sehe&quot; ## [36] &quot;statt&quot; &quot;warum&quot; &quot;wohl&quot; &quot;,das&quot; &quot;,denn&quot; ## [41] &quot;,ich&quot; &quot;befahl&quot; &quot;bestrich&quot; &quot;dann&quot; &quot;dasaß&quot; ## [46] &quot;dass&quot; &quot;fern&quot; &quot;fortwährend&quot; &quot;fühl&quot; &quot;gebührend&quot; Wir können auch Listen mit neben- und unterordnenden Konjunktionen sowie Konjunktionaladverbien aus dem Internet abrufen. Dann können wir sie genauer zählen. case_when str_detect(seznam_prirednih_konektorjev, ||) str_detect(seznam_podrednih_konektorjev, ||) str_detect(seznam_prislovnih_konektorjev, ||) "],["rolling-stones-auf-twitter.html", "Kapitel 16 Rolling Stones auf Twitter 16.1 Der Tod von Charlie Watts 16.2 Programme 16.3 Datenstrom holen 16.4 Tweets holen 16.5 Datensatz erstellen 16.6 Datensatz speichern 16.7 Datensatz laden 16.8 Verfasser der Tweets 16.9 Timeplot 16.10 User-Informationen 16.11 Geographische Verteilung 16.12 Speichern des erweiterten Datensatzes 16.13 Laden des erweiterten Datensatzes 16.14 Zeitlicher geographischer Verlauf 16.15 Lange Wörter 16.16 Topwörter 16.17 Netzwerke 16.18 Sentiment &amp; Emotion 16.19 Sentiment (Animation)", " Kapitel 16 Rolling Stones auf Twitter 16.1 Der Tod von Charlie Watts 16.2 Programme library(tidyverse) library(tidytext) library(rtweet) library(lubridate) library(plotly) library(leaflet) library(glue) library(htmlwidgets) library(DT) library(gganimate) library(gifski) library(ggthemes) Das rtweet-Programm ermöglicht den Zugang zu Twitter-Texten und Twitter-Usern. Dazu benötigt man einen Twitter-Account, außerdem muss man eine Twitter-App einstellen. Das erfordert eine Telefonnummer zur Verifizierung. Genauere Angaben sind auf verschiedenen Internetseiten zu finden, z.B. Twitter-App setup. Im rtweet package stehen mehrere Funktionen zur Verfügung, um tweets vom Twitter-API zu erfassen: search_tweets()  erfasst Tweets der letzten 6-9 Tage, die einem vom User festgelegten Stichwort (query) entsprechen. stream_tweets()  erfasst Tweets aus dem live Datenstrom get_timelines()  erfasst Tweets von ausgewählten Twitter-Nutzern. Die Tweets (d.h. der Text und zahlreiche Metadaten) werden in der Form eines Datensatzes (data.frame bzw. tibble) organisiert und können auch auf der Festplatte gespeichert werden. 16.3 Datenstrom holen Eine Möglichkeit ist die Tweets live einzufangen, und zwar mit der Funktion stream_tweets(). library(tidyverse) library(rtweet) ## Stream keywords used to filter tweets q &lt;- &quot;RollingStones&quot; ## Stream for 30 minutes streamtime &lt;- 30 * 60 ## Filename to save json data (backup) filename &lt;- &quot;data/rollingstones.json&quot; ## Stream tweets rt_rollingstones &lt;- stream_tweets(q = q, timeout = streamtime, file_name = filename) Laden der gespeicherten Daten von Disk: library(jsonlite) fromJSON(&quot;data/rollingstones.json&quot;) 16.4 Tweets holen Typischerweise laden wir die Tweets der letzten sechs bis neun Tage herunter, indem wir in die Funktion search_tweets() ein Schlagwort (q) eingeben, das unserem Ziel entspricht. Die Anzahl der Tweets in unserer Recherche ist begrenzt (n = 18000 Tweets). Will man zwei oder mehrere miteinander auftretende Wörter (etwa Kollokationen) in einer Recherche verwenden, sollte man ein Pluszeichen zwischen die Wörter setzen, z.B. q = \"Charlie+Watts\". Die Argumente q und n sind notwendig zur Recherche. Die Suchfunktion hat mehrere optionale Argumente, z.B. die Sprache lang, Herausfiltern von Retweets mit Hilfe des Schalters include_rts = FALSE u.a. Tweets in deutscher Sprache zum Thema RollingStones: library(tidyverse) library(rtweet) q &lt;- &quot;RollingStones&quot; tweets_rollingstones_de &lt;- search_tweets(q = q, n = 18000, token = bearer_token(), include_rts = FALSE, `-filter` = &quot;replies&quot;, lang = &quot;de&quot;) tweets_rollingstones_de Ein Retweet bedeutet, dass ein Nutzer den Tweet einer anderen Person teilt, so dass auch die Follower den Beitrag sehen können. Tweets und Retweets kann man separat aus dem Datensatz abrufen. Tweets in slowenischer Sprache zum Thema RollingStones: q &lt;- &quot;RollingStones&quot; tweets_rollingstones_sl &lt;- search_tweets(q = q, n = 18000, token = bearer_token(), include_rts = FALSE, `-filter` = &quot;replies&quot;, lang = &quot;sl&quot;) tweets_rollingstones_sl Tweets in englischer Sprache zum Thema RollingStones: q &lt;- &quot;RollingStones&quot; tweets_rollingstones_en &lt;- search_tweets(q = q, n = 18000, token = bearer_token(), include_rts = FALSE, `-filter` = &quot;replies&quot;, lang = &quot;en&quot;) tweets_rollingstones_en 16.5 Datensatz erstellen Die drei sprachspezifischen Tabellen wollen wir in einer gemeinsamen Tabelle speichern. Die Tabellenspalte lang enthält eine Angabe darüber, welche Sprache in den einzelnen Tweets verwendet wurden. tweets_rollingstones &lt;- bind_rows(tweets_rollingstones_sl, tweets_rollingstones_de, tweets_rollingstones_en) 16.6 Datensatz speichern Die gemeinsame Tabelle können wir nun auf Festplatte speichern. Die Datei mit der Endung rds kann nur vom Programm R gelesen werden. Eine Excel-Datei kann man beispielsweise mit dem writexl-Packet schreiben. library(writexl) write_xlsx(tweets_rollingstones, &quot;data/tweets_rollingstones.xlsx&quot;) write_rds(tweets_rollingstones, &quot;data/tweets_rollingstones.rds&quot;) 16.7 Datensatz laden Am nächsten Tag fahren wir mit unsere Analyse fort und laden unseren Datensatz. tweets_rollingstones &lt;- read_rds(&quot;data/tweets_rollingstones.rds&quot;) 16.8 Verfasser der Tweets Unter den Twitternutzern, die Tweets anlässlich des Todes von Charlie Watts, dem Drummer der Rolling Stones, sind einige uns bekannte Namen zu finden, z.B. der slowenische Radiosender Val202. Die Schreiber der Tweets findet man in der Tabellenspalte screen_name. tweets_rollingstones %&gt;% filter(str_detect(screen_name, &quot;Val202&quot;)) %&gt;% rmarkdown::paged_table() tweets_rollingstones_sl &lt;- subset(tweets_rollingstones, lang == &quot;sl&quot;) ## plot multiple time series by first grouping the data by screen name tweets_rollingstones_sl %&gt;% dplyr::group_by(screen_name) %&gt;% ts_plot() + ggplot2::labs( title = &quot;Tweets after the death of Charlie Watts, drummer&quot;, subtitle = &quot;Tweets collected, parsed, and plotted using `rtweet`&quot; ) Tweets der deutschen Zeitung FAZ (Frankfurter Allgemeine Zeitung): tweets_rollingstones %&gt;% filter(str_detect(screen_name, &quot;faznet&quot;)) %&gt;% rmarkdown::paged_table() Der Tagesspiegel: tweets_rollingstones %&gt;% filter(str_detect(screen_name, &quot;Tagesspiegel&quot;)) %&gt;% rmarkdown::paged_table() tweets_rollingstones_de &lt;- subset(tweets_rollingstones, lang == &quot;de&quot;) ## plot multiple time series by first grouping the data by screen name tweets_rollingstones_de %&gt;% group_by(created_at, screen_name) %&gt;% count(screen_name, sort = TRUE) %&gt;% ungroup() %&gt;% mutate(screen_name = fct_lump(screen_name, 9)) %&gt;% dplyr::group_by(screen_name) %&gt;% ts_plot() + ggplot2::labs(y = &quot;log10(# Tweets)&quot;, title = &quot;Tweets after the death of Charlie Watts, drummer&quot;, subtitle = &quot;Tweets collected, parsed, and plotted using `rtweet`&quot; ) + scale_y_log10() tweets_rollingstones_en &lt;- subset(tweets_rollingstones, lang == &quot;en&quot;) ## plot multiple time series by first grouping the data by screen name tweets_rollingstones_en %&gt;% group_by(created_at, screen_name) %&gt;% count(screen_name, sort = TRUE) %&gt;% ungroup() %&gt;% mutate(screen_name = fct_lump(screen_name, 9)) %&gt;% dplyr::group_by(screen_name) %&gt;% ts_plot() + ggplot2::labs(y = &quot;log10(# Tweets)&quot;, title = &quot;Tweets after the death of Charlie Watts, drummer&quot;, subtitle = &quot;Tweets collected, parsed, and plotted using `rtweet`&quot; ) + scale_y_log10() 16.9 Timeplot Die Anzahl der veröffentlichten Tweets zum Thema können wir in einem Diagramm chronologisch darstellen. Im folgenden Diagramm bildet die Anzahl der Tweets innerhalb von drei Stunden jeweils einen Datenpunkt. Zunächst bilden wir die englischsprachigen Tweets ab. ## subset dataframe tweets_rollingstones_en = subset(tweets_rollingstones, lang == &quot;en&quot;) ## plot time series of tweets p1 &lt;- ts_plot(tweets_rollingstones_en, &quot;3 hours&quot;, color = &quot;red&quot;) + ggplot2::theme_minimal() + ggplot2::theme(plot.title = ggplot2::element_text(face = &quot;bold&quot;)) + ggplot2::labs( x = NULL, y = NULL, title = &quot;Frequency of #rollingstones Twitter statuses from past 9 days&quot;, subtitle = &quot;Twitter status (tweet) counts aggregated using three-hour intervals&quot;, caption = &quot;\\nSource: Data collected from Twitter&#39;s REST API via rtweet&quot;) p1 Ein interaktives Diagramm können wir mit plotly erstellen. library(plotly) ggplotly(p1) %&gt;% layout() Die deutschsprachigen Tweets: ## subset dataframe tweets_rollingstones_de = subset(tweets_rollingstones, lang == &quot;de&quot;) ## plot time series of tweets p2 &lt;- ts_plot(tweets_rollingstones_de, &quot;3 hours&quot;, color = &quot;darkgreen&quot;) + ggplot2::theme_minimal() + ggplot2::theme(plot.title = ggplot2::element_text(face = &quot;bold&quot;)) + ggplot2::labs( x = NULL, y = NULL, title = &quot;Frequency of #rollingstones Twitter statuses from past 9 days&quot;, subtitle = &quot;Twitter status (tweet) counts aggregated using three-hour intervals&quot;, caption = &quot;\\nSource: Data collected from Twitter&#39;s REST API via rtweet&quot;) p2 Ein interaktives Diagramm können wir wiederum mit plotly erstellen. library(plotly) ggplotly(p2) %&gt;% layout() In beiden Fällen flacht die Verlaufskurve schnell ab. 16.10 User-Informationen Aus unserem Datensatz können wir mit users_data() auch Informationen über die Twitter-Nutzer herausholen, die einen Tweet zum Thema #RollingStones (in den vergangenen 6-9 Tagen) verfasst hat, z.B. wie viele Follower sie haben, wie viele Freunde, wie viele Tweets sie verfasst haben, Beschreibungen über sich selbst u.a. tweets_rollingstones %&gt;% users_data() %&gt;% rmarkdown::paged_table() Ähnliche (obwohl weniger nützliche) Funktionen sind users_with_tweets() und tweets_with_users(). users_with_tweets(tweets_rollingstones) tweets_with_users(tweets_rollingstones) 16.11 Geographische Verteilung Aus welchen Ländern stammen die englischsprachigen Tweets? In der Tabellenspalte country finden wir leider in den meisten Fällen keine Angabe (NA), aber die am häufigsten angezeigten Staaten sind die USA, Großbritannien, Ausralien, Kanada und Irland, in denen Englisch Amtssprache ist. tweets_rollingstones_en %&gt;% count(country, sort = TRUE) ## # A tibble: 48 x 2 ## country n ## &lt;chr&gt; &lt;int&gt; ## 1 &lt;NA&gt; 17024 ## 2 United States 393 ## 3 United Kingdom 153 ## 4 Australia 69 ## 5 Canada 53 ## 6 Ireland 20 ## 7 France 19 ## 8 Argentina 15 ## 9 Brazil 15 ## 10 Italy 15 ## # ... with 38 more rows Wählen wir die Tabellenspalte location als Ausgangspunkt, dann erhalten wir recht chaotische Sammlung von Stadt-, Staats- und anderen geographischen Bezeichnungen, die man nach vorheriger Bereinigung nutzen könnte, z.B. kartographisch. tweets_rollingstones_en %&gt;% count(location, sort = TRUE) ## # A tibble: 6,264 x 2 ## location n ## &lt;chr&gt; &lt;int&gt; ## 1 &quot;&quot; 4682 ## 2 &quot;United States&quot; 155 ## 3 &quot;London, England&quot; 130 ## 4 &quot;London&quot; 126 ## 5 &quot;Los Angeles, CA&quot; 109 ## 6 &quot;United Kingdom&quot; 106 ## 7 &quot;Canada&quot; 101 ## 8 &quot;Chicago, IL&quot; 95 ## 9 &quot;New York, NY&quot; 91 ## 10 &quot;UK&quot; 75 ## # ... with 6,254 more rows Für eine graphische Darstellung wählen wir an dieser Stelle die (unkomplizierte) country-Spalte. library(lubridate) p3 &lt;- tweets_rollingstones_en %&gt;% select(created_at, country) %&gt;% mutate(date = lubridate::as_date(created_at)) %&gt;% group_by(date, country) %&gt;% count(country, sort = TRUE) %&gt;% drop_na() %&gt;% ungroup() %&gt;% mutate(country = fct_lump(country, 9)) %&gt;% # nur 9+1 Staaten ggplot(aes(date, n, fill = country)) + geom_col() p3 library(plotly) ggplotly(p3) %&gt;% layout() Die graphische Darstellung für die deutschen Tweets nach Staaten: p4 &lt;- tweets_rollingstones_de %&gt;% select(created_at, country) %&gt;% mutate(date = lubridate::as_date(created_at)) %&gt;% group_by(date, country) %&gt;% count(country, sort = TRUE) %&gt;% drop_na() %&gt;% ungroup() %&gt;% mutate(country = fct_lump(country, 9)) %&gt;% ggplot(aes(date, n, fill = country)) + geom_col() p4 library(plotly) ggplotly(p4) %&gt;% layout() library(ggthemes) m1 &lt;- tweets_rollingstones %&gt;% unnest(geo_coords) %&gt;% unnest(coords_coords) %&gt;% separate(geo_coords, into = c(&quot;latitude&quot;,&quot;longit&quot;), sep = &quot;;&quot;, remove = FALSE, extra = &quot;merge&quot;) %&gt;% separate(coords_coords, into = c(&quot;longitude&quot;,&quot;latit&quot;), sep = &quot;;&quot;, remove = FALSE, extra = &quot;merge&quot;) %&gt;% mutate(latitude = parse_number(latitude), longitude = parse_number(longitude)) %&gt;% select(country, latitude, longitude, screen_name, status_id, location, text) %&gt;% filter(latitude != &quot;&quot; | longitude != &quot;&quot;) %&gt;% distinct(status_id, .keep_all = TRUE) %&gt;% ggplot(aes(longitude, latitude, color = country)) + borders() + geom_point() + theme_map() + theme(legend.position = &quot;bottom&quot;) + labs(title = &quot;Tweets around the World after Death of Charlie Watts&quot;, color = &quot;Country&quot;) m1 Ein interaktives Diagramm können wir wiederum mit plotly erstellen. library(plotly) ggplotly(m1) %&gt;% layout() Es gibt mehrere Datensätze, in denen die geographische Lage von Städten (also Längen- und Breitengrad) gespeichert ist. Das rtweet-Paket verfügt über einen kleineren Datensatz, erreichbar durch: rtweet:::citycoords %&gt;% rmarkdown::paged_table() Einen wesentlich umfangreicheren Datensatz findet man auf der Webseite von simplempas, neben den Angaben zur geographischen Lage und Staatszugehörigkeit auch die Einwohnerzahl. world_cities &lt;- read_csv(&quot;data/worldcities.csv&quot;) world_cities &lt;- world_cities %&gt;% select(city_ascii, country, lat, lng) %&gt;% rename(city = city_ascii, state_country = country) world_cities %&gt;% rmarkdown::paged_table() Die letztere Datensammlung fügen wir mit der Funktion left_join() unserem Twitter-Datensatz hinzu. Im Twitter-Datensatz nehmen wir außerdem mehrere Korrekturen vor, und zwar mit den tidyverse-Funktionen separate() zur Trennung von Tabellenspalten, str_replace() zum Austausch von Angaben in den Tabellenspalten, str_to_sentence() zur Vereinheitlichung der Schreibweise sowie ifelse() und str_detect(), um Bezeichnungen unter passenden Bedingungen in einer Tabellenspalte ausfindig zu machen und zu ersetzen. tweets_rollingstones_cities &lt;- tweets_rollingstones %&gt;% separate(location, into = c(&quot;city&quot;, &quot;state_country&quot;), extra = &quot;merge&quot;, fill = &quot;right&quot;, sep = &quot;, &quot;, remove = FALSE) %&gt;% separate(location, into = c(&quot;city&quot;, &quot;state_country&quot;), extra = &quot;merge&quot;, fill = &quot;right&quot;, sep = &quot; &quot;, remove = FALSE) %&gt;% mutate(state_country = str_replace(state_country, &quot;- &quot;, &quot;&quot;)) %&gt;% mutate(state_country = str_replace(state_country, &quot;\\\\| &quot;, &quot;&quot;)) %&gt;% mutate(city = str_replace(city, &quot;,&quot;, &quot;&quot;)) %&gt;% mutate(city = str_to_sentence(city)) %&gt;% mutate(state_country = str_to_sentence(state_country)) %&gt;% mutate(state_country = ifelse( country == &quot;United States&quot;, &quot;United States&quot;, state_country)) %&gt;% mutate(state_country = ifelse( country == &quot;United Kingdom&quot;, &quot;United Kingdom&quot;, state_country)) %&gt;% mutate(state_country = ifelse( str_detect(country, &quot;Ireland&quot;), &quot;Ireland&quot;, state_country)) %&gt;% mutate(state_country = ifelse( str_detect(country, &quot;Australia&quot;), &quot;Australia&quot;, state_country)) %&gt;% mutate(state_country = str_replace( state_country, &quot;Western australia&quot;, &quot;Australia&quot;)) %&gt;% mutate(state_country = str_replace(state_country, &quot;Slovenija&quot;, &quot;Slovenia&quot;)) %&gt;% mutate(state_country = str_replace(state_country, &quot;Deutschland&quot;, &quot;Germany&quot;)) %&gt;% mutate(state_country = str_replace(state_country, &quot;Österreich&quot;, &quot;Austria&quot;)) %&gt;% mutate(state_country = str_replace(state_country, &quot;Schweiz&quot;, &quot;Switzerland&quot;)) %&gt;% mutate(state_country = str_replace(state_country, &quot;England&quot;, &quot;United Kingdom&quot;)) %&gt;% mutate(state_country = str_replace(state_country, &quot;am Main&quot;, &quot;Germany&quot;)) %&gt;% mutate(city = str_replace(city, &quot;Zürich&quot;, &quot;Zurich&quot;)) %&gt;% mutate(city = str_replace(city, &quot;Wien&quot;, &quot;Vienna&quot;)) %&gt;% mutate(city = str_replace(city, &quot;München&quot;, &quot;Munich&quot;)) %&gt;% mutate(city = str_replace(city, &quot;Köln&quot;, &quot;Cologne&quot;)) %&gt;% mutate(city = str_replace(city, &quot;Düsseldorf&quot;, &quot;Dusseldorf&quot;)) %&gt;% mutate(city = str_replace(city, &quot;Gießen&quot;, &quot;Giessen&quot;)) %&gt;% mutate(state_country = ifelse( city %in% c(&quot;Ljubljana&quot;,&quot;Maribor&quot;,&quot;Celje&quot;,&quot;Kranj&quot;, &quot;Lucija&quot;), &quot;Slovenia&quot;, state_country)) %&gt;% mutate(state_country = ifelse(city %in% c(&quot;Zurich&quot;,&quot;Basel&quot;), &quot;Switzerland&quot;, state_country)) %&gt;% mutate(state_country = ifelse(city %in% c(&quot;Vienna&quot;,&quot;Graz&quot;), &quot;Austria&quot;, state_country)) %&gt;% mutate(state_country = ifelse(city %in% c(&quot;Teheran&quot;), &quot;Iran&quot;, state_country)) %&gt;% mutate(state_country = ifelse(city %in% c(&quot;Strasbourg&quot;), &quot;France&quot;, state_country)) %&gt;% mutate(state_country = ifelse( city %in% c(&quot;Berlin&quot;,&quot;Munich&quot;,&quot;Hamburg&quot;,&quot;Essen&quot;,&quot;Heilbronn&quot;, &quot;Dortmund&quot;,&quot;Cologne&quot;,&quot;Frankfurt&quot;,&quot;Hannover&quot;, &quot;Giessen&quot;,&quot;Konstanz&quot;,&quot;Rostock&quot;,&quot;Dusseldorf&quot;, &quot;Augsburg&quot;,&quot;Lohmar&quot;), &quot;Germany&quot;, state_country)) %&gt;% mutate(state_country = ifelse( str_detect(location, &quot;United Kingdom&quot;), &quot;United Kingdom&quot;, state_country)) %&gt;% mutate(city = ifelse(location == &quot;United Kingdom&quot;, &quot;United Kingdom&quot;, city)) %&gt;% mutate(state_country = ifelse( str_detect(location, &quot;United States&quot;), &quot;United States&quot;, state_country)) %&gt;% mutate(city = ifelse(location == &quot;United States&quot;, &quot;&quot; , city)) %&gt;% mutate(state_country = ifelse( str_detect(location, &quot;Germany&quot;), &quot;Germany&quot;, state_country)) %&gt;% mutate(city = ifelse(location == &quot;Germany&quot;, &quot;&quot; , city)) %&gt;% mutate(city = ifelse(location == &quot;Las Vegas, NV&quot;, &quot;Las Vegas&quot; , city)) %&gt;% mutate(state_country = ifelse(location == &quot;Las Vegas, NV&quot;, &quot;United States&quot; , state_country)) %&gt;% mutate(city = ifelse(location == &quot;Toronto/Las Vegas&quot;, &quot;Toronto&quot; , city)) %&gt;% mutate(state_country = ifelse(location == &quot;Toronto/Las Vegas&quot;, &quot;Canada&quot; , state_country)) %&gt;% mutate(city = ifelse(location == &quot;san francisco&quot;, &quot;San Francisco&quot; , city)) %&gt;% mutate(state_country = ifelse(location == &quot;san francisco&quot;, &quot;United States&quot; , state_country)) Der korrigierte und mit left_join() erweiterte Datensatz: tweets_rollingstones_cities_joined &lt;- tweets_rollingstones_cities %&gt;% left_join(world_cities, by = c(&quot;city&quot;, &quot;state_country&quot;)) tweets_rollingstones_cities_joined %&gt;% select(location, country, city, state_country, lat, lng) %&gt;% rmarkdown::paged_table() 16.12 Speichern des erweiterten Datensatzes write_rds(tweets_rollingstones_cities_joined, &quot;data/tweets_rollingstones_cities_joined.rds&quot;) library(writexl) write_xlsx(tweets_rollingstones_cities_joined, &quot;data/tweets_rollingstones_cities_joined.xlsx&quot;) 16.13 Laden des erweiterten Datensatzes Am nächsten Tag brauchen wir nicht all die oben durchgeführten Schritte noch einmal durchzuführen, sondern hier an dieser Stelle den relevanten Datensatz laden und weitermachen. tweets_rollingstones_cities_joined &lt;- read_rds(&quot;data/tweets_rollingstones_cities_joined.rds&quot;) Eine einfache Weltkarte aus dem ggthemes-Paket, auf der die einzelnen Twitterorte als farbige Punkte eingetragen sind. library(ggthemes) m2 &lt;- tweets_rollingstones_cities_joined %&gt;% # unnest(geo_coords) %&gt;% # unnest(coords_coords) %&gt;% # separate(geo_coords, into = c(&quot;latitude&quot;,&quot;longit&quot;), # sep = &quot;;&quot;, remove = FALSE, extra = &quot;merge&quot;) %&gt;% # separate(coords_coords, into = c(&quot;longitude&quot;,&quot;latit&quot;), # sep = &quot;;&quot;, remove = FALSE, extra = &quot;merge&quot;) %&gt;% # mutate(latitude = parse_number(latitude), # longitude = parse_number(longitude)) %&gt;% select(country, lat, lng, state_country, screen_name, status_id, location, text) %&gt;% filter(lat != &quot;&quot; | lng != &quot;&quot;) %&gt;% # distinct(status_id, .keep_all = TRUE) %&gt;% ggplot(aes(lng, lat, color = state_country, alpha = 0.3)) + borders() + geom_point() + theme_map() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Tweets around the World after Death of Charlie Watts&quot;, color = &quot;Country&quot;) ggsave(&quot;pictures/worldmap_tweets_rollingstones.png&quot;, width = 12, height = 9) m2 Ein interaktives Diagramm können wir wiederum mit plotly erstellen. library(plotly) ggplotly(m2) %&gt;% layout() Eine detailliertere und vergrößerbare Weltkarte (Paket leaflet) mit den Twitterorten aus unserem Twitter-Datensatz über den Drummer Charlie Watts und die Rolling Stones. library(leaflet) library(glue) library(htmlwidgets) library(DT) template &lt;- &quot;&lt;&gt;{ city }&lt;/p&gt;&lt;p&gt;{ state_country }&lt;/p&gt;&quot; tweet_map &lt;- tweets_rollingstones_cities_joined %&gt;% gather(key, value, city) %&gt;% mutate(key = str_to_title(str_replace_all(key, &quot;_&quot;, &quot; &quot;)), key = paste0(&quot;&lt;b&gt;&quot;, key, &quot;&lt;/b&gt;&quot;)) %&gt;% replace_na(list(value = &quot;Unknown&quot;)) %&gt;% nest(data = c(key, value)) %&gt;% mutate(html = map(data, knitr::kable, format = &quot;html&quot;, escape = FALSE, col.names = c(&quot;&quot;, &quot;&quot;))) %&gt;% leaflet() %&gt;% addTiles() %&gt;% addCircleMarkers(lat = ~ lat, lng = ~ lng, color = ~ state_country, popup = ~ html, radius = 3) %&gt;% addMeasure() tweet_map 16.14 Zeitlicher geographischer Verlauf Mit dem Paket gganimate sind auch Animationen möglich, die die Entwicklung eines Prozesses graphisch darstellen. In diesem Fall soll gezeigt werden, wann und wo auf der Welt Tweets nach dem Tod von Charlie Watts verfasst wurden. Nach den oben durchgeführten Korrekturen und Erweiterungen verfügen wir zumindest über fast 1000 Tweets mit ausgewiesenen Twitterorten (d.h. mit den Koordinaaten ihrer geographischen Lage), was für eine Demonstration der Animation ausreicht. Im ersten Schritt leiten wir mit Hilfe von mutate() aus der Tabellenspalte created at eine Datumsspalte (date_time) ab und fügen sie unserem Datensatz hinzu. Im folgenden Chunk haben wir noch zwei weitere Datumsspalten als Varianten geschaffen, die wir bei Bedarf verwenden könnten. tweets_rollingstones_time &lt;- tweets_rollingstones_cities_joined %&gt;% mutate(date_time = lubridate::as_datetime(created_at)) %&gt;% mutate(dates = lubridate::as_date(created_at)) %&gt;% mutate(seconds = as.numeric(lubridate::as_datetime(created_at))) tweets_rollingstones_time %&gt;% select(date_time, dates, seconds) %&gt;% arrange(date_time) %&gt;% head(3) ## # A tibble: 3 x 3 ## date_time dates seconds ## &lt;dttm&gt; &lt;date&gt; &lt;dbl&gt; ## 1 2021-08-24 16:38:55 2021-08-24 1629823135 ## 2 2021-08-24 16:40:43 2021-08-24 1629823243 ## 3 2021-08-24 16:42:03 2021-08-24 1629823323 Nun wird die Animation vorbereitet, angezeigt und als gif-Datei gespeichert. library(gganimate) library(gifski) library(ggthemes) start = lubridate::as_datetime(&quot;2021-08-24 16:38:55&quot;) # start = as_date(&quot;2021-08-24 16:38:55&quot;) anim_graph &lt;- tweets_rollingstones_time %&gt;% arrange(date_time) %&gt;% filter(!is.na(lat) &amp; !is.na(lng)) %&gt;% filter(date_time &gt;= start) %&gt;% add_count(city, name = &quot;city_count&quot;) %&gt;% mutate(volume = city_count) %&gt;% ggplot(aes(lng, lat)) + borders() + geom_point(aes(size = volume, color = volume)) + theme_map() + scale_color_gradient2(low = &quot;blue&quot;, high = &quot;red&quot;, midpoint = log10(.01), trans = &quot;log10&quot;, guide = &quot;none&quot;) + scale_size_continuous(range = c(1, 6)) + # 38 # transition_reveal(start_date) + transition_time(date_time) + labs( title = &quot;Tweets about the Rolling Stones: { round(frame_time) }&quot;) + theme(legend.position = &quot;none&quot;) # animation.hook=&quot;gifski&quot; animate(anim_graph, nframes = 300, fps = 5, device = &quot;png&quot;) anim_save(&quot;pictures/rolling_stones_tweets_animated.png&quot;) anim_graph 16.15 Lange Wörter Ein spezielles Paket (ggpage), mit dem wir uns eine grobe graphische Übersicht über bestimmte quantifizierte Eigenschaften von Texten verschaffen können, und zwar über die Länge von Wörtern und ihre Textstelle. Diese graphische Darstellung kann insbesondere bei kürzeren Texten effektvoll eingesetzt werden. library(ggpage) tweets_rollingstones_de %&gt;% ggpage_build() %&gt;% mutate(long_word = stringr::str_length(word) &gt; 8) %&gt;% ggpage_plot(aes(fill = long_word)) + labs(title = &quot;Longer words throughout the German Tweets&quot;) + scale_fill_manual(values = c(&quot;grey70&quot;, &quot;blue&quot;), labels = c(&quot;8 or less&quot;, &quot;9 or more&quot;), name = &quot;Word length&quot;) 16.16 Topwörter Für die Textzerlegung und Analyse stehen uns mehrere Pakete zur Verfügung, z.B. quanteda, udpipe oder tidytext, mit denen wir schon gearbeitet haben. Wir wählen hier das tidytext-Programm für die Textzerlegung. Zuerst bereiten wir eine Stoppwordliste für die deutsche Sprache vor. Dann wählen wir für die Zerlegung des Textes in Wörter eine besondere Einstellung, nämlich token = \"tweets\", bei der hashtags (#) und URL erhalten bleiben, so dass man danach suchen kann oder sie auch abzählen oder analysieren kann. Will man nur den reinen Text analysieren, ist es notwendig, diese besonderen Zeichenfolgen aus den Tweets zu entfernen. Verwendet man die tidytext-Funktionen, kann man das Argument token = \"tweets\" einfach weglassen oder eine der anderen Optionen verwenden (z.B. token = \"words\"). Je nach Zielsetzung erhalten wir Großbuchstaben (to_lower = FALSE) oder nicht (to_lower = TRUE). Wörter in deutschen Tweets: library(tidytext) stoplist_de &lt;- as_tibble(quanteda::stopwords(&quot;german&quot;)) %&gt;% rename(word = value) tw_rs_de_words &lt;- tweets_rollingstones_de %&gt;% unnest_tokens(word, text, token = &quot;tweets&quot;, to_lower = TRUE) %&gt;% anti_join(stoplist_de) %&gt;% count(word, sort = TRUE) tw_rs_de_words %&gt;% rmarkdown::paged_table() Wortwolke aus deutschen Tweets: set.seed(1321) library(wordcloud2) w1 &lt;- wordcloud2(tw_rs_de_words) w1 # save it in html library(webshot) library(htmlwidgets) saveWidget(w1,&quot;tmp3.html&quot;,selfcontained = F) # save as png and pdf webshot(&quot;tmp3.html&quot;,&quot;pictures/wcloud_tweets_rs_de.png&quot;, delay =5, vwidth = 1000, vheight=800) webshot(&quot;tmp3.html&quot;,&quot;pictures/wcloud_tweets_rs_de.pdf&quot;, delay =5, vwidth = 800, vheight=600) Englische Tweets: stoplist_en &lt;- stop_words tw_rs_en_words &lt;- tweets_rollingstones_en %&gt;% unnest_tokens(word, text, token = &quot;tweets&quot;, to_lower = TRUE) %&gt;% anti_join(stoplist_en) %&gt;% count(word, sort = TRUE) tw_rs_en_words %&gt;% rmarkdown::paged_table() Die am häufigsten verwendeten Schlagwörter (topfeatures) sind in beiden Sprachen annähernd gleich. set.seed(1321) library(wordcloud2) w2 &lt;- wordcloud2(tw_rs_en_words) w2 # save it in html library(webshot) library(htmlwidgets) saveWidget(w2,&quot;tmp4.html&quot;,selfcontained = F) # save as png and pdf webshot(&quot;tmp4.html&quot;,&quot;pictures/wcloud_tweets_rs_en.png&quot;, delay =5, vwidth = 1000, vheight=800) webshot(&quot;tmp4.html&quot;,&quot;pictures/wcloud_tweets_rs_en.pdf&quot;, delay =5, vwidth = 800, vheight=600) 16.17 Netzwerke Welche Wörter erscheinen häufiger gemeinsam in Tweets? Welche Wortpaare (also ngrams mit n = 2 Mitgliedern) lassen sich aus den Tweets herausholen? Arbeitet man mit tidyverse-Funktionen, kann man das Programm widyr verwenden. Im quanteda.textstats-Programm gibt es die Funktion textstat_collocations(). Auch das Programm udpipe hat Funktionen zur Ermittlung von ngrams, Kollokationen und Netzwerk-Diagramme. library(widyr) # remove punctuation, convert to lowercase, add id for each tweet! tweets_rollingstones_paired_words_de &lt;- tweets_rollingstones_de %&gt;% unnest_tokens(paired_words, text, token = &quot;ngrams&quot;, n = 2) tweets_rollingstones_paired_words_de %&gt;% count(paired_words, sort = TRUE) ## # A tibble: 8,770 x 2 ## paired_words n ## &lt;chr&gt; &lt;int&gt; ## 1 https t.co 464 ## 2 charlie watts 203 ## 3 rollingstones https 106 ## 4 charliewatts rollingstones 86 ## 5 die rollingstones 74 ## 6 der rollingstones 73 ## 7 rolling stones 57 ## 8 80 jahren 46 ## 9 rollingstones charliewatts 40 ## 10 charliewatts https 38 ## # ... with 8,760 more rows Wir verwenden wiederum die Stoppwortliste, um Funktionswörter herauszufiltern, so dass möglichst nur Kollokationen mit Inhaltswörtern übrig bleiben. library(tidyr) # Wörter in getrennten Tabllenspalten tweets_rs_separated_words_de &lt;- tweets_rollingstones_paired_words_de %&gt;% separate(paired_words, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) # Stoppwortliste erweitern stoplist_de &lt;- c(pull(stoplist_de), &quot;https&quot;,&quot;t.co&quot;) %&gt;% as_tibble() %&gt;% rename(word = value) # Filtern rs_tweets_filtered_de &lt;- tweets_rs_separated_words_de %&gt;% filter(!word1 %in% stoplist_de$word) %&gt;% filter(!word2 %in% stoplist_de$word) # new bigram counts: rs_words_counts_de &lt;- rs_tweets_filtered_de %&gt;% count(word1, word2, sort = TRUE) head(rs_words_counts_de) %&gt;% rmarkdown::paged_table() Aus der vorher erstellten Stoppwortliste haben wir auch die URL-Bestandteile (https, t.co) entfernt, die zwar in vielen Beiträgen vorkommen, aber keine konventionellen Wörter der deutschen Sprache darstellen. Oben wurde schon erwähnt, dass man die URL mit tidytext- oder quanteda-Funktionen bequem entfernen kann. Eine zwar unübersichtliche, aber ökonomische Methode ist die Verwendung eines regulären Ausdrucks (Regex), um die gesamte URL (und nicht nur die beiden oben angeführten Bestandteile) aus den Tweets zu entfernen, z.B. diese mit der Funktion gsub(), die nach einem vorgegebenen Muster (pattern) den Inhalt eines Textes oder einer Tabellenspalte durch eine (leere) Zeichenfolge ersetzt (replacement). # Remove all urls gsub(pattern = &quot;\\\\s?(f|ht)(tp)(s?)(://)([^\\\\.]*)[\\\\.|/](\\\\S*)&quot;, replacement = &quot;&quot;, tweets_rollingstones$text) # unser Tweet-Text Das Netzwerk-Diagramm zeigt, welche Wörter (vor allem Inhaltswörter) in den deutschen Tweets häufiger miteinander auftreten. library(igraph) library(ggraph) # plot climate change word network # (plotting graph edges is currently broken) rs_words_counts_de %&gt;% filter(n &gt;= 5) %&gt;% # Wie häufig muss ein Wort sein? graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + # geom_edge_link(aes(edge_alpha = n, edge_width = n)) geom_edge_link(aes(edge_alpha = n, edge_width = n)) + geom_node_point(color = &quot;darkslategray4&quot;, size = 3) + geom_node_text(aes(label = name), vjust = 1.8, size = 3) + labs( title = &quot;Word Network: Tweets using the hashtag - RollingStones&quot;, subtitle = &quot;Text mining twitter data &quot;, x = &quot;&quot;, y = &quot;&quot;) 16.18 Sentiment &amp; Emotion Ein Paket mit zahlreichen Möglichkeiten (in verschiedenen Sprachen) für die Sentiment-Analyse ist library(syuzhet). Zusätzliche Möglichkeiten bietet auch das Paket udpipe, und zwar mit der Negationsumkehrung. Da wir bereits Twitter-Texte in unserem Datensatz geladen haben, ist unser erster Schritt die Umwandlung der Tweets in Äußerungen. Das wird mit der Funktion get_sentences() bewerkstelligt. library(syuzhet) # We need to parse the text into sentences. v_en &lt;- get_sentences(tweets_rollingstones_en$text) Dann wird mit der Funktion get_sentiment() der Sentiment-Wert für jede einzelne Äußerung (z.B. die Summe der einzelnen Werte für die Wörter in einer Äußerung) ermittelt, und zwar auf der Grundlage von Wortlisten, in denen Wörtern emotionale Zahlenwerte zugeordnet wurden (d.h. positive oder negative Werte). # Then we calculate a sentiment value for each sentence. rs_sentiment &lt;- get_sentiment(v_en) head(rs_sentiment) ## [1] 0.00 0.00 0.00 1.80 0.00 -0.25 Mit der Funktion get_nrc_sentiment() können außer den polarisierten Werten (positiv vs. negativ) auch verschiedene Emtionen wie etwa Freude (joy), Trauer (sadness) und einige weitere ermittelt werden, und zwar wieder auf der Grundlage von Wortlisten, die von Versuchspersonen emotional bewertete Wörter enthalten. rs_nrc_sentiment &lt;- get_nrc_sentiment(v_en) joy_items &lt;- which(rs_nrc_sentiment$joy &gt; 0) head(v_en[joy_items], 4) ## [1] &quot;We ended up no more than 20ft off the stage that night and right by the catwalk it was brilliant.&quot; ## [2] &quot;I know this is late to the table but in honour of Charlie Watts @RShrubb I thought I share a great @harleydavidson tribute to @RollingStones #CharlieWatts RIP https://t.co/qeyqaMB8Fc&quot; ## [3] &quot;She and I queued and then ran when gates open straight to the mosh and sat there for 10hrs waiting in the sun on the hottest day of that year!!!&quot; ## [4] &quot;&lt;U+0001F538&gt;#Robonzo &lt;U+0001F941&gt; \\n&lt;U+0001F539&gt;#Song: &#39;On Top Of The World&#39; \\n[#alternative, #psychadelic~#Rock\\n#drummer #guitarist #musician\\n#RollingStones #Spotify #YouTube \\n#Apple~#Music #Amazon #Pandora]\\n&lt;U+0001F538&gt;#Website:\\n&lt;U+23E9&gt; https://t.co/zBW0D0wk6U\\n&lt;U+0001F538&gt;#Video via: (@RobonzoDrummer)\\n&lt;U+23E9&gt; https://t.co/SKRzZWNOrz https://t.co/ECWAc4tysM&quot; Hier ist ein Ausschnitt aus solch einer Tabelle, die wir nach der Ermittlung der emotionalen Werte von Wörtern erhalten: rs_nrc_sentiment[1:10, 1:10] %&gt;% rmarkdown::paged_table() Die emotionale Valenz (d.h. wie positiv oder negativ die Bedeutung eines Wortes bewertet wird): valence_rs &lt;- (rs_nrc_sentiment[, 9]*-1) + rs_nrc_sentiment[, 10] head(valence_rs) ## [1] 0 0 0 1 0 -1 In den englischen Tweets wurden Wörter gefunden, die häufig Emotionen wie Freude, Zuversicht, Erwartung, Trauer ausdrücken. Das Ergebnis können wir wenigen Zeilen graphisch darstellen. barplot( sort(colSums(prop.table(rs_nrc_sentiment[, 1:8]))), horiz = TRUE, cex.names = 0.7, las = 1, col = 9:2, main = &quot;Emotions in Tweets&quot;, xlab=&quot;Percentage&quot; ) Hier ist eine ästhetisch etwas ansprechendere, aber auch erweiterbare graphische Darstellung mit ggplot(): library(scales) rs_nrc_sentiment[,1:8] %&gt;% summarise(across(everything(), ~ mean(.))) %&gt;% pivot_longer(anger:trust, names_to = &quot;emotion&quot;, values_to = &quot;pct&quot;) %&gt;% mutate(emotion = fct_reorder(emotion, pct)) %&gt;% ggplot(aes(pct, emotion, fill = emotion)) + geom_col() + theme(legend.position = &quot;none&quot;) + scale_x_continuous(labels = percent) + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;Emotion in English Tweets after Death of Charlie Watts&quot;) ggsave(&quot;pictures/rs_emotions_en.png&quot;) Zum Vergleich die deutschen Tweets: Zunächst wiederum die Umwandlung in Äußerungen, gefolgt von der Ermittlung der emtionalen Werte von Wörtern. v_de &lt;- get_sentences(tweets_rollingstones_de$text) rs_nrc_sentiment_de &lt;- get_nrc_sentiment(v_de, language = &quot;german&quot;) joy_items_de &lt;- which(rs_nrc_sentiment_de$joy &gt; 0) head(v_de[joy_items_de], 4) ## [1] &quot;Wir gedenken dem @RollingStones Drummer mit einem stündigen #PopRoutes Special abem 9ni uf @srf3 \\n\\nhttps://t.co/yXUtyb12y4&quot; ## [2] &quot;Synagoge von Vilnius gefunden.&quot; ## [3] &quot;Und ganz ehrlich ich freue mich auf jeden Ton, der noch von ihnen kommt.&quot; ## [4] &quot;Niemand ist unsterblich.&quot; Die graphische Darstellung der emotionalen Werte in deutschen Tweets: rs_nrc_sentiment_de[,1:8] %&gt;% summarise(across(everything(), ~ mean(.))) %&gt;% pivot_longer(anger:trust, names_to = &quot;emotion&quot;, values_to = &quot;pct&quot;) %&gt;% mutate(emotion = fct_reorder(emotion, pct)) %&gt;% ggplot(aes(pct, emotion, fill = emotion)) + geom_col() + theme(legend.position = &quot;none&quot;) + scale_x_continuous(labels = percent) + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;Emotion in German Tweets after Death of Charlie Watts&quot;) ggsave(&quot;pictures/rs_emotions_de.png&quot;) Was für ein Unterschied! Woran das wohl liegen mag? Das müsste man sich auf jeden Fall mal genauer anschauen. 16.19 Sentiment (Animation) Auch hier können wir es mit einer Animation des Sentiments in den englischen Tweets versuchen. Da die Berechnung länger dauern könnte, wählen wir nur etwa 500 Tweets zur graphischen Darstellung des Sentiments aus. library(ggpage) library(purrr) library(gganimate) library(tidytext) library(zoo) # Beschränkung auf die Tweets 1000 bis 1500 prebuild &lt;- tweets_rollingstones_en$text[1000:1500] %&gt;% ggpage_build() %&gt;% left_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) midbuild &lt;- map_df(.x = 0:50 * 10 + 1, ~ prebuild %&gt;% mutate(score = ifelse(is.na(value), 0, value), score_smooth = zoo::rollmean(score, .x, 0), score_smooth = score_smooth / max(score_smooth), rolls = .x)) anim_pages &lt;- midbuild %&gt;% ggpage_plot(aes(fill = score_smooth)) + scale_fill_gradient2(low = &quot;red&quot;, high = &quot;blue&quot;, mid = &quot;grey&quot;, midpoint = 0) + guides(fill = &quot;none&quot;) + labs( title = &quot;Smoothed sentiment of Rolling Stones Tweets, rolling average of {round(frame_time)}&quot;) + transition_time(rolls) anim_pages "],["deutsche-zungenbrecher.html", "Kapitel 17 Deutsche Zungenbrecher 17.1 Programme laden 17.2 Tabelle laden 17.3 EDA", " Kapitel 17 Deutsche Zungenbrecher Ein Experiment mit Studierenden der Germanistik in Maribor: - 11 deutsche Zungenbrecher (tongue twisters, lomilci jezika, besedna zavozlanka) - Webadresse: https://www.youtube.com/watch?v=wuK_znJRKhU 17.1 Programme laden library(tidyverse) library(scales) library(janitor) library(readxl) library(writexl) 17.2 Tabelle laden Wir laden die gemeinsame Google-Tabelle: zungenbrecher = read_xlsx(&quot;data/Zungenbrecher.xlsx&quot;) %&gt;% mutate(across(where(is.numeric), ~ if_else(is.na(.), 0, as.numeric(.)))) 17.3 EDA tabelle1 = zungenbrecher %&gt;% drop_na %&gt;% summarise(Fehler = mean(Fehlersumme), Fehlerzeit = mean(Fehlerquote), Fehler_sd = sd(Fehlersumme), Fehlerzeit_sd = sd(Fehlerquote)) tabelle1 ## # A tibble: 1 x 4 ## Fehler Fehlerzeit Fehler_sd Fehlerzeit_sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.11 4826. 1.10 2058. Im Durchschnitt etwa {Fehler} Versprecher pro Zungenbrecher. Im Durchschnitt alle {Fehlerzeit} Millisekunden ein Versprecher. Fehler_insgesamt = tabelle1$Fehler Fehler_sd_insgesamt = tabelle1$Fehler_sd p1 = zungenbrecher %&gt;% group_by(ZungenbrNummer) %&gt;% drop_na %&gt;% summarise(Fehler = mean(Fehlersumme), Fehlerzeit = mean(Fehlerquote), Fehler_sd = sd(Fehlersumme), Fehlerzeit_sd = sd(Fehlerquote), .groups = &quot;keep&quot;) %&gt;% ggplot(aes(Fehler, ZungenbrNummer)) + geom_pointrange(aes(xmin = Fehler - Fehler_sd, xmax = Fehler + Fehler_sd)) + geom_vline(xintercept = Fehler_insgesamt, lty = 2, color = &quot;red&quot;) + geom_vline(xintercept = Fehler_insgesamt + Fehler_sd_insgesamt, lty = 2, color = &quot;blue&quot;) + geom_vline(xintercept = Fehler_insgesamt - Fehler_sd_insgesamt, lty = 2, color = &quot;darkgreen&quot;) + scale_x_continuous(breaks = seq(0, 6, 1)) + scale_y_continuous(breaks = seq(0, 11, 1)) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Mittlere Fehlerzahl pro Zungenbrecher&quot;) ggsave(&quot;pictures/zungenbrecher_versprecherzahl.png&quot;) p1 Fehlerzeit_insgesamt = tabelle1$Fehlerzeit Fehlerzeit_sd_insgesamt = tabelle1$Fehlerzeit_sd p2 = zungenbrecher %&gt;% group_by(ZungenbrNummer) %&gt;% drop_na %&gt;% summarise(Fehler = mean(Fehlersumme), Fehlerzeit = mean(Fehlerquote), Fehler_sd = sd(Fehlersumme), Fehlerzeit_sd = sd(Fehlerquote), .groups = &quot;keep&quot;) %&gt;% ggplot(aes(Fehlerzeit, ZungenbrNummer)) + geom_pointrange(aes(xmin = Fehlerzeit - Fehlerzeit_sd, xmax = Fehlerzeit + Fehlerzeit_sd)) + geom_vline(xintercept = Fehlerzeit_insgesamt, lty = 2, color = &quot;red&quot;) + geom_vline(xintercept = Fehlerzeit_insgesamt + Fehlerzeit_sd_insgesamt, lty = 2, color = &quot;blue&quot;) + geom_vline(xintercept = Fehlerzeit_insgesamt - Fehlerzeit_sd_insgesamt, lty = 2, color = &quot;darkgreen&quot;) + scale_x_continuous(breaks = seq(0, 12000, 1000)) + scale_y_continuous(breaks = seq(0, 11, 1)) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Mittlere Fehlerzeit pro Zungenbrecher&quot;) ggsave(&quot;pictures/zungenbrecher_versprecherzeit.png&quot;) p2 library(patchwork) p1+p2 ggsave(&quot;pictures/zungenbrecher_patch1.png&quot;, width = 12, height = 7) zungenbrecher %&gt;% drop_na %&gt;% ggplot(aes(Fehlersumme, fill = Versuchsperson)) + geom_density(alpha = 0.3) + scale_x_continuous(breaks = seq(0, 6, 1)) + labs(title = &quot;Anzahl der Fehler pro Zungenbrecher&quot;) zungenbrecher %&gt;% drop_na %&gt;% ggplot(aes(Fehlersumme, fill = Versuchsperson)) + geom_histogram(alpha = 0.5) + scale_x_continuous(breaks = seq(0, 6, 1)) + labs(title = &quot;Anzahl der Fehler pro Zungenbrecher&quot;) ggsave(&quot;pictures/zungenbrecher_versprecherzahl_histogram.png&quot;) zungenbrecher %&gt;% drop_na %&gt;% ggplot(aes(Fehlerquote, Versuchsperson, color = factor(ZungenbrNummer))) + geom_pointrange(xmin = 0, xmax = 12000) + scale_x_continuous(breaks = seq(0, 12000, 1000)) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Mittlere Fehlerzeit pro Zungenbrecher bei Versuchspersonen&quot;) ggsave(&quot;pictures/zungenbrecher_versprecherzeit_personen.png&quot;) zungenbrecher %&gt;% group_by(ZungenbrNummer) %&gt;% drop_na %&gt;% ggplot(aes(Fehlerquote, ZungenbrNummer, color = Versuchsperson)) + geom_pointrange(xmin = 0, xmax = 12000) + scale_x_continuous(breaks = seq(0, 12000, 1000)) + scale_y_continuous(breaks = seq(0, 11, 1)) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Mittlere Fehlerzeit pro Zungenbrecher bei Versuchspersonen&quot;) ggsave(&quot;pictures/zungenbrecher_versprecherzeit_personen.png&quot;) tabelle2 = zungenbrecher %&gt;% drop_na %&gt;% pivot_longer(Addition:Kontamination, names_to = &quot;Versprechertyp&quot;, values_to = &quot;Fehlerzahl&quot;) %&gt;% group_by(Versprechertyp) %&gt;% summarise(Versprecherzahl = mean(Fehlerzahl), Versprecherzahl_sd = sd(Fehlerzahl)) tabelle2 ## # A tibble: 4 x 3 ## Versprechertyp Versprecherzahl Versprecherzahl_sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Addition 0.2 0.483 ## 2 Deletion 0.235 0.427 ## 3 Kontamination 0.106 0.310 ## 4 Substitution 0.376 0.756 Versprecherzahl_insgesamt = mean(tabelle2$Versprecherzahl) Versprecherzahl_sd_insgesamt = mean(tabelle2$Versprecherzahl_sd) tabelle2 %&gt;% ggplot(aes(Versprecherzahl, Versprechertyp, color = Versprechertyp)) + geom_pointrange(aes(xmin = Versprecherzahl - Versprecherzahl_sd, xmax = Versprecherzahl + Versprecherzahl_sd)) + geom_vline(xintercept = Versprecherzahl_insgesamt, lty = 2, color = &quot;red&quot;) + geom_vline(xintercept = Versprecherzahl_insgesamt + Versprecherzahl_sd_insgesamt, lty = 2, color = &quot;blue&quot;) + geom_vline(xintercept = Versprecherzahl_insgesamt - Versprecherzahl_sd_insgesamt, lty = 2, color = &quot;darkgreen&quot;) + scale_x_continuous(breaks = seq(0, 1.2, 0.2)) + # scale_y_continuous(breaks = seq(0, 11, 1)) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Mittlere Fehlerzahl pro Zungenbrecher&quot;) ggsave(&quot;pictures/zungenbrecher_versprecherzahl_errortype.png&quot;) ## Saving 7 x 5 in image "],["komplexe-äußerungen.html", "Kapitel 18 Komplexe Äußerungen 18.1 Packages 18.2 Texte laden 18.3 Text 1 zerlegen 18.4 Texttabelle erstellen 18.5 Speichern der Tabelle 18.6 Auswahl nach Länge 18.7 Durchschnittslänge 18.8 Auswahl nach Konnektoren 18.9 Welche Konnektoren? 18.10 Text 2 zerlegen 18.11 Texttabelle erstellen 18.12 Speichern der Tabelle 18.13 Auswahl nach Länge 18.14 Durchschnittslänge 18.15 Auswahl nach Konnektoren 18.16 Welche Konnektoren? 18.17 Text 3 zerlegen 18.18 Texttabelle erstellen 18.19 Speichern der Tabelle 18.20 Auswahl nach Länge 18.21 Durchschnittslänge 18.22 Auswahl nach Konnektoren 18.23 Welche Konnektoren? 18.24 Vergleich der Äußerungslängen", " Kapitel 18 Komplexe Äußerungen 18.1 Packages library(tidyverse) library(scales) library(tidytext) library(ggtext) library(readtext) library(quanteda) library(quanteda.textstats) library(quanteda.textplots) library(udpipe) library(ggstatsplot) library(patchwork) 18.2 Texte laden kohlhaas &lt;- read_lines( &quot;data/books/kleist/Kleist_Kohlhaas_Projekt_Gutenberg.txt&quot;) steppenwolf &lt;- read_lines( &quot;data/books/hesse/Hermann Hesse Der Steppenwolf.txt&quot;) ausland2 &lt;- readRDS(&quot;data/spiegel_politik_deutschland2.rds&quot;) 18.3 Text 1 zerlegen Zunächst erstellen wir mit der quanteda-Funktion corpus() ein Korpus, das nach Dokumenten organisiert ist. Danach wird das Korpus mit der Funktion corpus_reshape() umgewandelt, so dass jede Einheit aus nur einer Äußerung besteht. kohlcrp &lt;- corpus(kohlhaas) kohlcorp &lt;- corpus_reshape(kohlcrp, to = &quot;sentences&quot;) 18.4 Texttabelle erstellen Das Äußerungskorpus wird in eine Tabelle umgewandelt. kohltxt &lt;- kohlcorp %&gt;% as_tibble(rownames = &quot;doc_id&quot;) %&gt;% rename(text = value) %&gt;% mutate(text = as.character(text) %&gt;% str_squish()) Die Äußerungstatistik erhält ebenfalls Tabellenform. kohlstats &lt;- summary(kohlcorp, n = 803) %&gt;% as_tibble() %&gt;% rename(doc_id = Text) Nun können wir die beiden Tabellen vereinen, und zwar mit Hilfe der gemeinsamen Spalte doc_id, die wir vorher in beiden Einzeltabellen vorbereitet und entsprechend benannt haben. Außerdem filtern wir auch die leeren Zeilen (d.h. jene ohne Tokens) heraus. Von den 803 Zeilen bleiben 767 Zeilen übrig. kohltab &lt;- kohlstats %&gt;% full_join(kohltxt, by = &quot;doc_id&quot;) %&gt;% filter(Tokens &gt; 0) 18.5 Speichern der Tabelle write_csv(kohltab, &quot;data/kohlhaas_tabelle.csv&quot;) 18.6 Auswahl nach Länge Die Tabelle wird zunächst mit der Funktion arrange() sortiert. Dann können wir die längsten Äußerungen auswählen und speichern. kohltab %&gt;% arrange(-Tokens) %&gt;% select(-Sentences) %&gt;% rmarkdown::paged_table() Die längste Äußerung enthält 437 Tokens (Interpunktionszeichen sind inbegriffen). Schauen wir uns mal diese Äußerung genauer an! Derartige Äußerungen mit mehreren Satzverbindungen vielen ineinander verschachtelten Nebensätzen nennt man eine Periode. kohlhaas_periode1 &lt;- kohltab %&gt;% filter(doc_id == &quot;text64.9&quot;) %&gt;% pull(text) write_lines( kohlhaas_periode1, &quot;data/kohlhaas_periode1.txt&quot;) Die zweitlängste Äußerung in der Novelle Michael Kohlhaas ist  Noch eine Periode. kohlhaas_periode2 &lt;- kohltab %&gt;% filter(doc_id == &quot;text62.16&quot;) %&gt;% pull(text) write_lines( kohlhaas_periode2, &quot;data/kohlhaas_periode2.txt&quot;) Perioden über Perioden. Hier sind eigentlich zwei zu sehen. Unser Programm hat die Interpunktionsfolge Punkt + Bindestrich wahrscheinlich nicht als Ende der ersten Äußerung gewertet. kohlhaas_periode3 &lt;- kohltab %&gt;% filter(doc_id == &quot;text22.11&quot;) %&gt;% pull(text) write_lines( kohlhaas_periode3, &quot;data/kohlhaas_periode3.txt&quot;) Das ist eine der mittellangen Äußerungen im Kohlhaas. kohlhaas_periode4 &lt;- kohltab %&gt;% filter(doc_id == &quot;text22.18&quot;) %&gt;% pull(text) write_lines( kohlhaas_periode4, &quot;data/kohlhaas_periode4.txt&quot;) Suchen wir mal die mittellangen Äußerungen heraus! Zu diesem Zweck verändern wir unsere Filtermethode. Wir wählen alle Äußerungen, die 50 bis 60 Tokens lang sind. Unser Programm hat 70 Äußerungen von dieser Länge gefunden. kohlhaas_utterances_50_60 &lt;- kohltab %&gt;% filter(Tokens &gt; 49 &amp; Tokens &lt; 61) %&gt;% pull(text) write_lines( kohlhaas_utterances_50_60, &quot;data/kohlhaas_utterances_50_60.txt&quot;) Äußerungen mit 20 bis 30 Tokens. kohlhaas_utterances_20_30 &lt;- kohltab %&gt;% filter(Tokens &gt; 19 &amp; Tokens &lt; 31) %&gt;% pull(text) write_lines( kohlhaas_utterances_20_30, &quot;data/kohlhaas_utterances_20_30.txt&quot;) Äußerungen mit 30 bis 40 Tokens. kohlhaas_utterances_30_40 &lt;- kohltab %&gt;% filter(Tokens &gt; 29 &amp; Tokens &lt; 41) %&gt;% pull(text) write_lines( kohlhaas_utterances_30_40, &quot;data/kohlhaas_utterances_30_40.txt&quot;) 18.7 Durchschnittslänge Wie lang sind die Äußerungen im Durchschnitt? - Etwa 54,76 Tokens pro Äußerung. kohl_mean &lt;- kohltab %&gt;% summarise(median_laenge = median(Tokens) %&gt;% round(2), mittlere_laenge = mean(Tokens) %&gt;% round(2), sd_laenge = sd(Tokens) %&gt;% round(2)) kohl_mean ## # A tibble: 1 x 3 ## median_laenge mittlere_laenge sd_laenge ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 45 54.8 45.0 Ist das viel? Das kann uns eigentlich nur ein Vergleich mit anderen Texten sagen. Das Histogramm zeigt die Spannbreite und welche Äußerungslängen für die Novelle charakteristisch sind. library(ggtext) ggk &lt;- kohltab %&gt;% ggplot(aes(Tokens)) + geom_histogram(aes(y=..density..), binwidth = 20, fill = &quot;darkgreen&quot;, alpha = 0.8, color = &quot;black&quot;) + scale_x_continuous(breaks = seq(0,500,20)) + # geom_freqpoly(binwidth = 20) + geom_density(fill = &quot;cyan&quot;, alpha = 0.4, size = 1) + geom_vline(xintercept = kohl_mean$median_laenge, color = &quot;blue&quot;, lty = 4, size = 1.3) + geom_vline(xintercept = kohl_mean$mittlere_laenge, color = &quot;red&quot;, lty = 2, size = 1) + geom_vline(xintercept = kohl_mean$mittlere_laenge + kohl_mean$sd_laenge, color = &quot;red&quot;, lty = 3, size = 1) + geom_vline(xintercept = kohl_mean$mittlere_laenge - kohl_mean$sd_laenge, color = &quot;red&quot;, lty = 3, size = 1) + labs(title = &quot;Äußerungslänge in _Kleists_ Novelle _Michael Kohlhaas_&quot;, caption = &#39;&lt;span style=&quot;color:red;&quot;&gt;**-.-.-. MEAN**&lt;/span&gt; +/- &lt;span style=&quot;color:red;&quot;&gt;**..... st. deviation**&lt;/span&gt; ; &lt;span style=&quot;color:blue;&quot;&gt;**-.-.-. MEDIAN**&lt;/span&gt;&#39;) + theme_bw() + theme(plot.title=element_markdown(size=18, color = &quot;darkred&quot;), plot.caption = element_markdown(color = &quot;darkgreen&quot;), axis.title.y=element_text(size = 12, vjust=+0.2), axis.title.x=element_text(size = 12, vjust=-0.2), axis.text.y=element_text(size = 10), axis.text.x=element_text(size = 10), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) ggk 18.8 Auswahl nach Konnektoren kohlhaas_utterances_aber1 &lt;- kohltab %&gt;% filter(str_detect(text, &quot;aber&quot;)) %&gt;% # filter(Tokens &gt; 29 &amp; Tokens &lt; 41) %&gt;% pull(text) write_lines( kohlhaas_utterances_aber1, &quot;data/kohlhaas_utterances_aber1.txt&quot;) 18.9 Welche Konnektoren? Die grammatische Analyse führen wir mit udpipe durch. Zuerst laden wir ein entsprechendes Sprachmodell. library(udpipe) destfile = &quot;german-gsd-ud-2.5-191206.udpipe&quot; if(!file.exists(destfile)){ sprachmodell &lt;- udpipe_download_model(language = &quot;german&quot;) udmodel_de &lt;- udpipe_load_model(sprachmodell$file_model) } else { file_model = destfile udmodel_de &lt;- udpipe_load_model(file_model) } Das Programm udpipe annotiert den Text mit Hilfe des Sprachmodells. x &lt;- udpipe_annotate(udmodel_de, x = kohlhaas, trace = FALSE) k &lt;- as.data.frame(x) %&gt;% mutate(doc_id = &quot;kleist_kohlhaas&quot;) Nun sind wir in der Lage, Wortklassen zu zählen, die udpipe identifiziert hat. Dazu verwenden wir die Funktion count(). Gezählt werden die Kategorien in der Spalte upos. k %&gt;% group_by(doc_id) %&gt;% count(upos, sort = TRUE) %&gt;% rmarkdown::paged_table() In der Spalte upos interessieren uns nur die Kategorien CCONJ (Junktoren) und SCONJ (Subjunktoren). Deshalb filtern wir alle anderen Kategorien heraus. k %&gt;% filter(upos == &quot;CCONJ&quot; | upos == &quot;SCONJ&quot;) %&gt;% count(upos, sort = TRUE) %&gt;% mutate(pct = round(100*n/sum(n), 2)) ## upos n pct ## 1 CCONJ 1232 53.47 ## 2 SCONJ 1072 46.53 Die Anzahl der Junktoren und Subjunktoren ist ziemlich ausgeglichen. Der Anteil der Nebensätze in Kleists Novelle scheint demnach höher zu sein in alltagssprachlichen Texten oder in modernen Zeitungstexten. tabelle_cconj_sconj &lt;- k %&gt;% filter(upos == &quot;CCONJ&quot; | upos == &quot;SCONJ&quot;) %&gt;% mutate(lemma = str_remove(lemma, &quot;[:PUNCT:]&quot;), lemma = str_to_lower(lemma), token = str_remove(token, &quot;[:PUNCT:]&quot;)) %&gt;% count(lemma, sort = TRUE) tabelle_cconj_sconj %&gt;% rmarkdown::paged_table() Steppenwolf von Hermann Hesse 18.10 Text 2 zerlegen Die einzelnen Textparapgraphen werden in einen einzigen vereint. steppenwolf &lt;- steppenwolf %&gt;% paste(collapse = &quot; &quot;) %&gt;% str_squish() str_sub(steppenwolf, start = 1, end = 227) ## [1] &quot;Hermann Hesse Der Steppenwolf Erzählung Vorwort des Herausgebers Dieses Buch enthält die uns geblichenen Aufzeichnungen jenes Mannes, welchen wir mit einem Ausdruck, den er selbst mehrmals gebrauchte, den «Steppenwolf» nannten.&quot; Wir erstellen nun ein Korpus, das aus einzelnen Äußerungen besteht, und zwar mit dem Namen stepcorp. stepcrp &lt;- corpus(steppenwolf) stepcorp &lt;- corpus_reshape(stepcrp, to = &quot;sentences&quot;) 18.11 Texttabelle erstellen Das Äußerungskorpus wird in eine Tabelle umgewandelt. steptxt &lt;- stepcorp %&gt;% as_tibble(rownames = &quot;doc_id&quot;) %&gt;% rename(text = value) %&gt;% mutate(text = as.character(text) %&gt;% str_squish()) Die Äußerungstatistik erhält ebenfalls Tabellenform. stepstats &lt;- summary(stepcorp, n = 3142) %&gt;% as_tibble() %&gt;% rename(doc_id = Text) Nun können wir die beiden Tabellen vereinen, und zwar mit Hilfe der gemeinsamen Spalte doc_id, die wir vorher in beiden Einzeltabellen vorbereitet und entsprechend benannt haben. Außerdem filtern wir auch die leeren Zeilen (d.h. jene ohne Tokens) heraus. steptab &lt;- stepstats %&gt;% full_join(steptxt, by = &quot;doc_id&quot;) %&gt;% filter(Tokens &gt; 0) 18.12 Speichern der Tabelle write_csv(steptab, &quot;data/steppenwolf_tabelle.csv&quot;) 18.13 Auswahl nach Länge Die Tabelle wird zunächst mit der Funktion arrange() sortiert. Dann können wir die längsten Äußerungen auswählen und speichern. steptab %&gt;% arrange(-Tokens) %&gt;% select(-Sentences) %&gt;% rmarkdown::paged_table() Die längste Äußerung enthält 157 Tokens (Interpunktionszeichen sind inbegriffen). Schauen wir uns mal diese Äußerung genauer an! Derartige Äußerungen mit mehreren Satzverbindungen vielen ineinander verschachtelten Nebensätzen nennt man eine Periode. steppenwolf_periode1 &lt;- steptab %&gt;% filter(doc_id == &quot;text1.232&quot;) %&gt;% pull(text) write_lines( steppenwolf_periode1, &quot;data/steppenwolf_periode1.txt&quot;) Die zweitlängste Äußerung in der Novelle Michael Kohlhaas ist  Noch eine Periode. steppenwolf_periode2 &lt;- steptab %&gt;% filter(doc_id == &quot;text1.230&quot;) %&gt;% pull(text) write_lines( steppenwolf_periode2, &quot;data/steppenwolf_periode2.txt&quot;) Perioden über Perioden. Hier sind eigentlich zwei zu sehen. Unser Programm hat die Interpunktionsfolge Punkt + Bindestrich wahrscheinlich nicht als Ende der ersten Äußerung gewertet. steppenwolf_periode3 &lt;- steptab %&gt;% filter(doc_id == &quot;text1.231&quot;) %&gt;% pull(text) write_lines( steppenwolf_periode3, &quot;data/steppenwolf_periode3.txt&quot;) Das ist eine der mittellangen Äußerungen im Kohlhaas. steppenwolf_periode4 &lt;- steptab %&gt;% filter(doc_id == &quot;text1.3037&quot;) %&gt;% pull(text) write_lines( steppenwolf_periode4, &quot;data/steppenwolf_periode4.txt&quot;) Suchen wir mal die mittellangen Äußerungen heraus! Zu diesem Zweck verändern wir unsere Filtermethode. Wir wählen alle Äußerungen, die 50 bis 60 Tokens lang sind. Unser Programm hat 70 Äußerungen von dieser Länge gefunden. steppenwolf_utterances_50_60 &lt;- steptab %&gt;% filter(Tokens &gt; 49 &amp; Tokens &lt; 61) %&gt;% pull(text) write_lines( steppenwolf_utterances_50_60, &quot;data/steppenwolf_utterances_50_60.txt&quot;) Äußerungen mit 20 bis 30 Tokens. steppenwolf_utterances_20_30 &lt;- steptab %&gt;% filter(Tokens &gt; 19 &amp; Tokens &lt; 31) %&gt;% pull(text) write_lines( steppenwolf_utterances_20_30, &quot;data/steppenwolf_utterances_20_30.txt&quot;) Äußerungen mit 30 bis 40 Tokens. steppenwolf_utterances_30_40 &lt;- steptab %&gt;% filter(Tokens &gt; 29 &amp; Tokens &lt; 41) %&gt;% pull(text) write_lines( steppenwolf_utterances_30_40, &quot;data/steppenwolf_utterances_30_40.txt&quot;) 18.14 Durchschnittslänge Wie lang sind die Äußerungen im Durchschnitt? - Etwa 25,58 Tokens pro Äußerung. Vergleichen Sie mit Kleists Kohlhaas: 54,76 Tokens pro Äußerung. step_mean &lt;- steptab %&gt;% summarise(median_laenge = median(Tokens) %&gt;% round(2), mittlere_laenge = mean(Tokens) %&gt;% round(2), sd_laenge = sd(Tokens) %&gt;% round(2)) step_mean ## # A tibble: 1 x 3 ## median_laenge mittlere_laenge sd_laenge ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20 25.6 20.4 Das Histogramm zeigt die Spannbreite und welche Äußerungslängen für die Novelle charakteristisch sind. library(ggtext) ggs &lt;- steptab %&gt;% ggplot(aes(Tokens)) + geom_histogram(aes(y=..density..), binwidth = 20, fill = &quot;darkgreen&quot;, alpha = 0.8, color = &quot;black&quot;) + scale_x_continuous(breaks = seq(0,500,20)) + # geom_freqpoly(binwidth = 20) + geom_density(fill = &quot;cyan&quot;, alpha = 0.4, size = 1) + geom_vline(xintercept = step_mean$median_laenge, color = &quot;blue&quot;, lty = 4, size = 1.3) + geom_vline(xintercept = step_mean$mittlere_laenge, color = &quot;red&quot;, lty = 2, size = 1) + geom_vline(xintercept = step_mean$mittlere_laenge + step_mean$sd_laenge, color = &quot;red&quot;, lty = 3, size = 1) + geom_vline(xintercept = step_mean$mittlere_laenge - step_mean$sd_laenge, color = &quot;red&quot;, lty = 3, size = 1) + labs(title = &quot;Äußerungslänge in _Hesses_ Roman _Steppenwolf_&quot;, caption = &#39;&lt;span style=&quot;color:red;&quot;&gt;**-.-.-. MEAN**&lt;/span&gt; +/- &lt;span style=&quot;color:red;&quot;&gt;**..... st. deviation**&lt;/span&gt; ; &lt;span style=&quot;color:blue;&quot;&gt;**-.-.-. MEDIAN**&lt;/span&gt;&#39;) + theme_bw() + theme(plot.title=element_markdown(size=18, color = &quot;darkred&quot;), plot.caption = element_markdown(color = &quot;darkgreen&quot;), axis.title.y=element_text(size = 12, vjust=+0.2), axis.title.x=element_text(size = 12, vjust=-0.2), axis.text.y=element_text(size = 10), axis.text.x=element_text(size = 10), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) ggs 18.15 Auswahl nach Konnektoren steppenwolf_utterances_aber1 &lt;- steptab %&gt;% filter(str_detect(text, &quot;aber&quot;)) %&gt;% # filter(Tokens &gt; 29 &amp; Tokens &lt; 41) %&gt;% pull(text) write_lines( steppenwolf_utterances_aber1, &quot;data/steppenwolf_utterances_aber1.txt&quot;) 18.16 Welche Konnektoren? Die grammatische Analyse führen wir mit udpipe durch. Zuerst laden wir ein entsprechendes Sprachmodell. library(udpipe) destfile = &quot;german-gsd-ud-2.5-191206.udpipe&quot; if(!file.exists(destfile)){ sprachmodell &lt;- udpipe_download_model(language = &quot;german&quot;) udmodel_de &lt;- udpipe_load_model(sprachmodell$file_model) } else { file_model = destfile udmodel_de &lt;- udpipe_load_model(file_model) } Das Programm udpipe annotiert den Text mit Hilfe des Sprachmodells. x &lt;- udpipe_annotate(udmodel_de, x = steppenwolf, trace = FALSE) s &lt;- as.data.frame(x) %&gt;% mutate(doc_id = &quot;hesse_steppenwolf&quot;) Nun sind wir in der Lage, Wortklassen zu zählen, die udpipe identifiziert hat. Dazu verwenden wir die Funktion count(). Gezählt werden die Kategorien in der Spalte upos. s %&gt;% group_by(doc_id) %&gt;% count(upos, sort = TRUE) %&gt;% rmarkdown::paged_table() In der Spalte xpos interessieren uns nur die Kategorien CCONJ (Junktoren) und SCONJ (Subjunktoren). Deshalb filtern wir alle anderen Kategorien heraus. s %&gt;% filter(upos == &quot;CCONJ&quot; | upos == &quot;SCONJ&quot;) %&gt;% count(upos, sort = TRUE) %&gt;% mutate(pct = round(100*n/sum(n), 2)) ## upos n pct ## 1 CCONJ 3857 79.13 ## 2 SCONJ 1017 20.87 Der Anteil der Junktoren beträgt fast vier Fünftel, der der Subjunktoren dagegen nur ein Fünftel. Der Anteil der Nebensätze in Kleists Novelle scheint höher zu sein als in Hesses Roman, einem Text aus dem 20. Jahrhundert. tabelle_cconj_sconj &lt;- s %&gt;% filter(upos == &quot;CCONJ&quot; | upos == &quot;SCONJ&quot;) %&gt;% mutate(lemma = str_remove(lemma, &quot;[:PUNCT:]&quot;), lemma = str_to_lower(lemma), token = str_remove(token, &quot;[:PUNCT:]&quot;)) %&gt;% count(lemma, sort = TRUE) tabelle_cconj_sconj %&gt;% rmarkdown::paged_table() Signifikanter Unterschied zwischen den durchschnittlichen Äußerungslängen im Kohlhaas und im Steppenwolf. step_tokens &lt;- steptab[,3] %&gt;% rename(step_tokens = Tokens) kohl_tokens &lt;- kohltab[,3] %&gt;% rename(kohl_tokens = Tokens) t.test(kohl_tokens, step_tokens, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: kohl_tokens and step_tokens ## t = 17.538, df = 844.34, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 25.91454 32.44608 ## sample estimates: ## mean of x mean of y ## 54.76402 25.58370 Nur Äußerungen mit weniger als 200 Tokens. kohltab_200 &lt;- kohltab %&gt;% filter(Tokens &lt; 200) steptab_200 &lt;- steptab %&gt;% filter(Tokens &lt; 200) ggks &lt;- ggplot() + geom_density(aes(Tokens, fill = &quot;kohltab_200&quot;), alpha = .4, data = kohltab_200) + geom_density(aes(Tokens, fill = &quot;steptab_200&quot;), alpha = .4, data = steptab_200) + expand_limits(x = c(1,200), y = c(0, 0.03)) + scale_fill_manual(name = &quot;Texte&quot;, labels = c(&quot;Kohlhaas&quot;, &quot;Steppenwolf&quot;), values = c(kohltab_200 = &quot;darkred&quot;, steptab_200 = &quot;darkgreen&quot;)) + scale_y_continuous(labels = percent_format(accuracy = 1)) + scale_x_continuous(breaks = c(1,5,10,20,30,40,50,75,100,125,150,175,200)) + coord_cartesian(expand = FALSE, clip = &quot;off&quot;) + # don&#39;t expand margin + no clipping geom_vline(xintercept = step_mean$median_laenge, color = &quot;darkgreen&quot;, lty = 4, size = 1.3) + geom_vline(xintercept = kohl_mean$median_laenge, color = &quot;darkred&quot;, lty = 4, size = 1.3) + labs(x = &quot;Tokens (Wörter + Interpunktionszeichen)&quot;, y = &quot;Anteil&quot;, title = &quot;Äußerungslänge in _Kohlhaas_ und _Steppenwolf_&quot;, caption = &quot;red = **mean** +/- **st.deviation**; blue = **median**&quot;) + theme_light() + theme(legend.position = &quot;top&quot;, plot.margin = margin(10, 10, 5, 10), plot.title=element_markdown(size=18, color = &quot;darkred&quot;), plot.title.position = &quot;panel&quot;, plot.caption = element_markdown(color = &quot;darkgreen&quot;), axis.title.y=element_text(size = 12, vjust=+0.2), axis.title.x=element_text(size = 12, vjust=-0.2), axis.text.y=element_text(size = 10), axis.text.x=element_text(size = 10)#, # panel.grid.major = element_blank(), # panel.grid.minor = element_blank() ) ggsave(&quot;pictures/kohlhaas_steppenwolf_utterance_length.png&quot;) ggks Mit library(patchwork) alle Äußerungen. library(patchwork) p1 &lt;- ggk / ggs ggsave(&quot;pictures/patch1_utterances_kohlhaas_steppenwolf.png&quot;) p1 18.17 Text 3 zerlegen Wir erstellen ein Korpus, das aus einzelnen Äußerungen besteht, und zwar mit dem Namen spiegelcorp. spiegelcrp &lt;- corpus(ausland2, text_field = &quot;text&quot;) spiegelcorp &lt;- corpus_reshape(spiegelcrp, to = &quot;sentences&quot;) 18.18 Texttabelle erstellen Das Äußerungskorpus wird in eine Tabelle umgewandelt. spiegeltxt &lt;- spiegelcorp %&gt;% as_tibble(rownames = &quot;doc_id&quot;) %&gt;% rename(text = value) %&gt;% mutate(text = as.character(text) %&gt;% str_squish()) Die Äußerungstatistik erhält ebenfalls Tabellenform. spiegelstats &lt;- summary(spiegelcorp, n = 430041) %&gt;% as_tibble() %&gt;% rename(doc_id = Text) Nun können wir die beiden Tabellen vereinen, und zwar mit Hilfe der gemeinsamen Spalte doc_id, die wir vorher in beiden Einzeltabellen vorbereitet und entsprechend benannt haben. Außerdem filtern wir auch die leeren Zeilen (d.h. jene ohne Tokens) heraus. spiegeltab &lt;- spiegelstats %&gt;% full_join(spiegeltxt, by = &quot;doc_id&quot;) %&gt;% filter(Tokens &gt; 0) 18.19 Speichern der Tabelle # write_csv(spiegeltab, &quot;data/spiegel_ausland2_tabelle.csv&quot;) # write_rds(spiegeltab, &quot;data/spiegel_ausland2_tabelle.rds&quot;) 18.20 Auswahl nach Länge Die Tabelle wird zunächst mit der Funktion arrange() sortiert. Dann können wir die längsten Äußerungen auswählen und speichern. spiegeltab %&gt;% arrange(-Tokens) %&gt;% select(-Sentences) %&gt;% head(10) %&gt;% rmarkdown::paged_table() Die längsten Äußerungen in den Spiegel-Artikeln enthalten lange Aufzählungen von Personen, Orten und Titeln (möglicherweise in Tabellenform). Sie sind eigentlich nicht repräsentativ für den Schreibstil in Spiegel-Artikeln zum Thema Auslandspolitik. Die längste Äußerung enthält 589 Tokens (Interpunktionszeichen sind inbegriffen). Mit dem Befehl cat(spiegel_periode1) können wir sie uns ansehen. spiegel_periode1 &lt;- spiegeltab %&gt;% filter(doc_id == &quot;text930.21&quot;) %&gt;% pull(text) write_lines( spiegel_periode1, &quot;data/spiegel_periode1.txt&quot;) Suchen wir mal mittellange Äußerungen in den Spiegel-Artikeln heraus! Zu diesem Zweck verändern wir unsere Filtermethode. Wir wählen alle Äußerungen, die 50 bis 60 Tokens lang sind. Unser Programm hat mehr als 1000 Äußerungen von dieser Länge gefunden. spiegel_utterances_50_60 &lt;- spiegeltab %&gt;% filter(Tokens &gt; 49 &amp; Tokens &lt; 61) %&gt;% pull(text) write_lines( spiegel_utterances_50_60, &quot;data/spiegel_utterances_50_60.txt&quot;) cat(spiegel_utterances_50_60[25]) ## Nachdem der NDR im Zuge eines Sparprogramms das Ende der Literatursendung &quot;Bücherjournal&quot; bekannt gegeben hatte, hatte es Proteste von namhaften Schriftstellern und Verlegerinnen gegeben, Felicitas von Lovenberg gab etwa zu bedenken: &quot;Leser*in wird und bleibt nur, wer die passenden Bücher für sich findet. 18.21 Durchschnittslänge Wie lang sind die Äußerungen im Durchschnitt? - Etwa 19,71 Tokens pro Äußerung. Im Vergleich mit Hermann Hesses Steppenwolf (25,58 Tokens) und Kleists Kohlhaas (54,76 Tokens) deutlich weniger. spiegel_mean &lt;- spiegeltab %&gt;% summarise(median_laenge = median(Tokens) %&gt;% round(2), mittlere_laenge = mean(Tokens) %&gt;% round(2), sd_laenge = sd(Tokens) %&gt;% round(2)) spiegel_mean ## # A tibble: 1 x 3 ## median_laenge mittlere_laenge sd_laenge ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18 19.7 11.8 Das Histogramm zeigt die Spannbreite und welche Äußerungslängen für die Novelle charakteristisch sind. library(ggtext) ggsp &lt;- spiegeltab %&gt;% ggplot(aes(Tokens)) + geom_histogram(aes(y=..density..), binwidth = 20, fill = &quot;darkgreen&quot;, alpha = 0.8, color = &quot;black&quot;) + scale_x_continuous(breaks = c(0,20,40,60,80,100,250,500,600)) + # geom_freqpoly(binwidth = 20) + geom_density(fill = &quot;cyan&quot;, alpha = 0.4, size = 1) + geom_vline(xintercept = step_mean$median_laenge, color = &quot;blue&quot;, lty = 4, size = 1.3) + geom_vline(xintercept = step_mean$mittlere_laenge, color = &quot;red&quot;, lty = 2, size = 1) + geom_vline(xintercept = step_mean$mittlere_laenge + step_mean$sd_laenge, color = &quot;red&quot;, lty = 3, size = 1) + geom_vline(xintercept = step_mean$mittlere_laenge - step_mean$sd_laenge, color = &quot;red&quot;, lty = 3, size = 1) + labs(title = &quot;Äußerungslänge in _Spiegel_-Artikeln über _Auslandspolitik_&quot;, caption = &#39;&lt;span style=&quot;color:red;&quot;&gt;**-.-.-. MEAN**&lt;/span&gt; +/- &lt;span style=&quot;color:red;&quot;&gt;**..... st. deviation**&lt;/span&gt; ; &lt;span style=&quot;color:blue;&quot;&gt;**-.-.-. MEDIAN**&lt;/span&gt;&#39;) + theme_bw() + theme(plot.title=element_markdown(size=18, color = &quot;darkred&quot;), plot.caption = element_markdown(color = &quot;darkgreen&quot;), axis.title.y=element_text(size = 12, vjust=+0.2), axis.title.x=element_text(size = 12, vjust=-0.2), axis.text.y=element_text(size = 10), axis.text.x=element_text(size = 10), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) ggsp 18.22 Auswahl nach Konnektoren spiegel_utterances_aber1 &lt;- spiegeltab %&gt;% filter(str_detect(text, &quot;aber&quot;)) %&gt;% # filter(Tokens &gt; 29 &amp; Tokens &lt; 41) %&gt;% pull(text) write_lines( spiegel_utterances_aber1, &quot;data/spiegel_utterances_aber1.txt&quot;) cat(spiegel_utterances_aber1[1]) ## Für deutsche Bewerber in der Schweiz heißt das: Durchaus selbstbewusst, aber bitte nicht arrogant auftreten und nicht zu dick auftragen. 18.23 Welche Konnektoren? Die grammatische Analyse führen wir mit udpipe durch. Zuerst laden wir ein entsprechendes Sprachmodell (falls es noch nicht geladen ist). library(udpipe) destfile = &quot;german-gsd-ud-2.5-191206.udpipe&quot; if(!file.exists(destfile)){ sprachmodell &lt;- udpipe_download_model(language = &quot;german&quot;) udmodel_de &lt;- udpipe_load_model(sprachmodell$file_model) } else { file_model = destfile udmodel_de &lt;- udpipe_load_model(file_model) } Das Programm udpipe annotiert die Texte mit Hilfe des Sprachmodells. Da es sich um Abertausende von Artikeln handelt, dauert das auf handelsüblichen Computern im Jahr 2021 mehr als 10 Minuten. Falls man nicht so lange warten möchte, nimmt man nur eine Stichprobe. Zu Demonstrationszwecken sollen 100 Artikel mal genug sein. set.seed(2020) spiegel_sample &lt;- slice_sample(ausland2, n = 100) x &lt;- udpipe_annotate(udmodel_de, x = as.character(spiegel_sample$text), trace = FALSE) sp &lt;- as.data.frame(x) %&gt;% mutate(doc_id = &quot;spiegel_auslandspolitik&quot;) Nun sind wir in der Lage, Wortklassen zu zählen, die udpipe identifiziert hat. Dazu verwenden wir die Funktion count(). Gezählt werden die Kategorien in der Spalte upos. sp %&gt;% group_by(doc_id) %&gt;% count(upos, sort = TRUE) %&gt;% rmarkdown::paged_table() In der Spalte upos interessieren uns nur die Kategorien CCONJ (Junktoren) und SCONJ (Subjunktoren). Deshalb filtern wir alle anderen Kategorien heraus. sp %&gt;% filter(upos == &quot;CCONJ&quot; | upos == &quot;SCONJ&quot;) %&gt;% count(upos, sort = TRUE) %&gt;% mutate(pct = round(100*n/sum(n), 2)) ## upos n pct ## 1 CCONJ 1526 71.11 ## 2 SCONJ 620 28.89 Der Anteil der Junktoren beträgt fast drei Viertel, der der Subjunktoren mehr als ein Viertel. Der Anteil der Nebensätze in Kleists Novelle scheint höher zu sein in modernen Zeitungstexten. tabelle_cconj_sconj &lt;- sp %&gt;% filter(upos == &quot;CCONJ&quot; | upos == &quot;SCONJ&quot;) %&gt;% mutate(lemma = str_remove(lemma, &quot;[:PUNCT:]&quot;), lemma = str_to_lower(lemma), token = str_remove(token, &quot;[:PUNCT:]&quot;)) %&gt;% count(lemma, sort = TRUE) tabelle_cconj_sconj %&gt;% rmarkdown::paged_table() 18.24 Vergleich der Äußerungslängen Wir prüfen das noch mit einem parametrischen Test, einer Anova, und einem nicht-parametrischen Test, einem Kruskal-Wallis-Test, ob sich die durchschnittlichen Äußerungslängen voneinander signifikant unterscheiden. Zu diesem Zweck vereinen wir die drei Datensätze in den Datensatz utter_length. Eine Anova mit einer einzigen unabhängigen Variable (hier: doc_id) und einer numerischen abhängigen Variable (hier: Tokens) kann man in R mit Hilfe der Funktion oneway.test() durchführen, einen entsprechenden nicht-parametrischen Test, den Kruskal-Wallis-Test, mit der Funktion kruskal.test(). step_tokens &lt;- steptab[,3] %&gt;% mutate(doc_id = &quot;steppenwolf&quot;) kohl_tokens &lt;- kohltab[,3] %&gt;% mutate(doc_id = &quot;kohlhaas&quot;) spiegel_tokens &lt;- spiegeltab[,3] %&gt;% mutate(doc_id = &quot;spiegel&quot;) utter_length &lt;- rbind(kohl_tokens, step_tokens, spiegel_tokens) # parametric test (One-Way Anova) anova1 &lt;- oneway.test(Tokens ~ doc_id, data = utter_length) anova1 ## ## One-way analysis of means (not assuming equal variances) ## ## data: Tokens and doc_id ## F = 362.58, num df = 2.0, denom df = 1644.1, p-value &lt; 2.2e-16 # non-parametric test (Kruskal-Wallis-Test) kruskal1 &lt;- kruskal.test(Tokens ~ doc_id, data = utter_length) kruskal1 ## ## Kruskal-Wallis rank sum test ## ## data: Tokens by doc_id ## Kruskal-Wallis chi-squared = 808.87, df = 2, p-value &lt; 2.2e-16 Sowohl der parametrische als auch der nicht-parametrische Test bestätigen signifikante Unterschiede. Die Signifikanz der Unterschiede zwischen den Gruppen (Kohlhaas, Steppenwolf, Spiegel) lässt sich mit einem Post-hoc-Test bestätigen, hier mit den Funktionen aov() und TukeyHSD(). anova2 &lt;- aov(Tokens ~ doc_id, data = utter_length) TukeyHSD(anova2, which = &quot;doc_id&quot;) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Tokens ~ doc_id, data = utter_length) ## ## $doc_id ## diff lwr upr p adj ## spiegel-kohlhaas -35.051679 -36.066812 -34.036545 0 ## steppenwolf-kohlhaas -29.180311 -30.311580 -28.049042 0 ## steppenwolf-spiegel 5.871368 5.368433 6.374302 0 Mit dem Package ggstatsplot kann man gleichzeitig einen statistischen Test und eine graphische Darstellung ausgeben lassen. library(ggstatsplot) set.seed(2020) plot1 &lt;- utter_length %&gt;% ggbetweenstats(x = doc_id, y = Tokens, type = &quot;parametric&quot;, pairwise.comparisons = TRUE, p.adjust.method = &quot;holm&quot;, bf.message = FALSE, var.equal = FALSE) plot1 In der folgenden graphischen Darstellung werden nur Äußerungen mit weniger als 200 Tokens berücksichtigt. kohltab_200 &lt;- kohltab %&gt;% filter(Tokens &lt; 200) steptab_200 &lt;- steptab %&gt;% filter(Tokens &lt; 200) spiegeltab_200 &lt;- spiegeltab %&gt;% filter(Tokens &lt; 200) # Normal distribution with Spiegel data x &lt;- seq(from = 1, to = 100, length.out = 300) x &lt;- spiegeltab_200$Tokens dens &lt;- data.frame(x = x, y = dnorm(x, mean = 19.71, sd = 11.76)) mycaption &lt;- &#39;&lt;span style=&quot;color:black;&quot;&gt;**MEDIAN**:&lt;/span&gt; : &lt;span style=&quot;color:darkred;&quot;&gt;**-.-.- _Kohlhaas_**&lt;/span&gt; ; &lt;span style=&quot;color:darkgreen;&quot;&gt;**-.-.- _Steppenwolf_**&lt;/span&gt; ; &lt;span style=&quot;color:magenta;&quot;&gt;**-.-.- _Spiegel_**&lt;/span&gt;&#39; ggksp &lt;- ggplot() + geom_density(aes(Tokens, fill = &quot;kohltab_200&quot;), alpha = .4, data = kohltab_200) + geom_density(aes(Tokens, fill = &quot;steptab_200&quot;), alpha = .4, data = steptab_200) + geom_density(aes(Tokens, fill = &quot;spiegeltab_200&quot;), alpha = .2, data = spiegeltab_200) + # normal distribution with Spiegel mean and sd values geom_line(aes(x,y, color = &quot;red&quot;), size = 2, alpha = .6, data = dens) + expand_limits(x = c(1,200), y = c(0, 0.03)) + scale_color_manual(name = &quot;Normalverteilung (theoretisch)&quot;, labels = &quot;Spiegel&quot;, values = &quot;red&quot;) + scale_fill_manual(name = &quot;Texte&quot;, labels = c(&quot;Kohlhaas&quot;, &quot;Steppenwolf&quot;, &quot;Spiegel&quot;), values = c(kohltab_200 = &quot;darkred&quot;, steptab_200 = &quot;darkgreen&quot;, spiegeltab_200 = &quot;magenta&quot;)) + scale_y_continuous(labels = percent_format(accuracy = 1)) + scale_x_continuous(breaks = c(1,5,10,20,30,40,50,75,100,125,150,175,200)) + coord_cartesian(expand = FALSE, clip = &quot;off&quot;) + # don&#39;t expand margin + no clipping geom_vline(xintercept = kohl_mean$median_laenge, color = &quot;darkred&quot;, lty = 4, size = 1.3) + geom_vline(xintercept = step_mean$median_laenge, color = &quot;darkgreen&quot;, lty = 4, size = 1.3) + geom_vline(xintercept = spiegel_mean$median_laenge, color = &quot;magenta&quot;, lty = 4, size = 1.3) + labs(x = &quot;Tokens (Wörter + Interpunktionszeichen)&quot;, y = &quot;Anteil&quot;, title = &quot;Äußerungslänge in _Kohlhaas_, _Steppenwolf_ und _Spiegel_-Artikeln&quot;, subtitle = &quot;Normalverteilung mit Spiegel-Werten für _mean_ und _sd_ in roter Farbe&quot;, caption = mycaption ) + theme_light() + theme(legend.position = &quot;top&quot;, plot.margin = margin(10, 10, 5, 10), plot.title=element_markdown(size=18, color = &quot;darkred&quot;), plot.title.position = &quot;plot&quot;, plot.caption = element_markdown(color = &quot;darkgreen&quot;), axis.title.y=element_text(size = 12, vjust=+0.2), axis.title.x=element_text(size = 12, vjust=-0.2), axis.text.y=element_text(size = 10), axis.text.x=element_text(size = 10)#, # panel.grid.major = element_blank(), # panel.grid.minor = element_blank() ) ggsave(&quot;pictures/kohlhaas_steppenwolf_spiegel_utterance_length.png&quot;) ggksp Mit logarithmierter x-Achsenskala (bei Verwendung des Datensatzes utter_length). ggksp2 &lt;- utter_length %&gt;% filter(Tokens &lt; 200 &amp; Tokens &gt; 0) %&gt;% ggplot() + geom_density(aes(Tokens, fill = doc_id), alpha = .4) + scale_fill_manual(name = &quot;Texte&quot;, labels = c(&quot;Kohlhaas&quot;, &quot;Steppenwolf&quot;, &quot;Spiegel&quot;), values = c(&quot;darkred&quot;, &quot;darkgreen&quot;, &quot;magenta&quot;)) + scale_x_log10(breaks = breaks_log(n = 10, base = 10)) + # scale_y_continuous(labels = percent_format(accuracy = 1)) + # expand_limits(x = c(1,200), y = c(0, 0.03)) + coord_cartesian(expand = FALSE, clip = &quot;off&quot;) + # don&#39;t expand margin + no clipping geom_vline(xintercept = kohl_mean$median_laenge, color = &quot;darkred&quot;, lty = 4, size = 1.3) + geom_vline(xintercept = step_mean$median_laenge, color = &quot;darkgreen&quot;, lty = 4, size = 1.3) + geom_vline(xintercept = spiegel_mean$median_laenge, color = &quot;magenta&quot;, lty = 4, size = 1.3) + labs(x = &quot;Tokens (Wörter + Interpunktionszeichen)&quot;, y = &quot;Anteil&quot;, title = &quot;Äußerungslänge in _Kohlhaas_, _Steppenwolf_ und _Spiegel_-Artikeln&quot;, caption = mycaption) + theme_light() + theme(legend.position = &quot;top&quot;, plot.margin = margin(10, 10, 5, 10), plot.title=element_markdown(size=18, color = &quot;darkred&quot;), plot.title.position = &quot;plot&quot;, plot.caption = element_markdown(color = &quot;darkgreen&quot;), axis.title.y=element_text(size = 12, vjust=+0.2), axis.title.x=element_text(size = 12, vjust=-0.2), axis.text.y=element_text(size = 10), axis.text.x=element_text(size = 10)#, # panel.grid.major = element_blank(), # panel.grid.minor = element_blank() ) ggsave(&quot;pictures/kohlhaas_steppenwolf_spiegel_utterance_length2.png&quot;) ggksp2 Mit dem Package (patchwork) kann man eine Collage der graphischen Darstellungen zusammenstellen. library(patchwork) p2 &lt;- (ggk+labs(x = &quot;&quot;, caption = &quot;&quot;)) / (ggs+labs(x = &quot;&quot;, caption = &quot;&quot;)) / ggsp ggsave(&quot;pictures/patch1_utterances_kohlhaas_steppenwolf.png&quot;) p2 "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
